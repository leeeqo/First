{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a0d7b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import statistics\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae86b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REYX5_1</th>\n",
       "      <th>REYX5_2</th>\n",
       "      <th>REYX5_3</th>\n",
       "      <th>REYX5_4</th>\n",
       "      <th>REYX5_5</th>\n",
       "      <th>REYX5_6</th>\n",
       "      <th>REYX5_7</th>\n",
       "      <th>REYX5_8</th>\n",
       "      <th>REYX5_9</th>\n",
       "      <th>REYX5_10</th>\n",
       "      <th>...</th>\n",
       "      <th>H3_6</th>\n",
       "      <th>H3_7</th>\n",
       "      <th>H3_8</th>\n",
       "      <th>H3_9</th>\n",
       "      <th>H3_10</th>\n",
       "      <th>H3_11</th>\n",
       "      <th>H3_12</th>\n",
       "      <th>H3_13</th>\n",
       "      <th>H3_14</th>\n",
       "      <th>H3_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.021830</td>\n",
       "      <td>-0.022091</td>\n",
       "      <td>-0.023217</td>\n",
       "      <td>-0.024061</td>\n",
       "      <td>-0.024793</td>\n",
       "      <td>-0.025011</td>\n",
       "      <td>-0.024841</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>-0.024106</td>\n",
       "      <td>-0.023762</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.026549</td>\n",
       "      <td>-0.026260</td>\n",
       "      <td>-0.025624</td>\n",
       "      <td>-0.024608</td>\n",
       "      <td>-0.023581</td>\n",
       "      <td>-0.022701</td>\n",
       "      <td>-0.022512</td>\n",
       "      <td>-0.022666</td>\n",
       "      <td>-0.023107</td>\n",
       "      <td>-0.023523</td>\n",
       "      <td>...</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.026601</td>\n",
       "      <td>-0.026467</td>\n",
       "      <td>-0.026082</td>\n",
       "      <td>-0.025734</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>-0.027129</td>\n",
       "      <td>-0.027003</td>\n",
       "      <td>-0.026047</td>\n",
       "      <td>-0.025100</td>\n",
       "      <td>-0.024086</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    REYX5_1   REYX5_2   REYX5_3   REYX5_4   REYX5_5   REYX5_6   REYX5_7  \\\n",
       "0 -0.021830 -0.022091 -0.023217 -0.024061 -0.024793 -0.025011 -0.024841   \n",
       "1 -0.026549 -0.026260 -0.025624 -0.024608 -0.023581 -0.022701 -0.022512   \n",
       "2 -0.026601 -0.026467 -0.026082 -0.025734 -0.026490 -0.027129 -0.027003   \n",
       "\n",
       "    REYX5_8   REYX5_9  REYX5_10  ...  H3_6  H3_7  H3_8  H3_9  H3_10  H3_11  \\\n",
       "0 -0.024436 -0.024106 -0.023762  ...  2.28  2.28  2.26  2.20   2.20   2.20   \n",
       "1 -0.022666 -0.023107 -0.023523  ...  2.26  2.24  2.26  2.26   2.22   2.26   \n",
       "2 -0.026047 -0.025100 -0.024086  ...  2.28  2.26  2.26  2.28   2.24   2.22   \n",
       "\n",
       "   H3_12  H3_13  H3_14  H3_15  \n",
       "0   2.28   2.28   2.26   2.20  \n",
       "1   2.26   2.24   2.20   2.20  \n",
       "2   2.20   2.24   2.24   2.24  \n",
       "\n",
       "[3 rows x 169 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_path = 'E:\\учеба\\Диплом\\Data_1\\mtsgrvmgn_trn.csv'\n",
    "train_data = pd.read_csv(train_path)\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28a8f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "REYX5_1    float64\n",
       "REYX5_2    float64\n",
       "REYX5_3    float64\n",
       "REYX5_4    float64\n",
       "REYX5_5    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9cabd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REYX5_1</th>\n",
       "      <th>REYX5_2</th>\n",
       "      <th>REYX5_3</th>\n",
       "      <th>REYX5_4</th>\n",
       "      <th>REYX5_5</th>\n",
       "      <th>REYX5_6</th>\n",
       "      <th>REYX5_7</th>\n",
       "      <th>REYX5_8</th>\n",
       "      <th>REYX5_9</th>\n",
       "      <th>REYX5_10</th>\n",
       "      <th>...</th>\n",
       "      <th>H3_6</th>\n",
       "      <th>H3_7</th>\n",
       "      <th>H3_8</th>\n",
       "      <th>H3_9</th>\n",
       "      <th>H3_10</th>\n",
       "      <th>H3_11</th>\n",
       "      <th>H3_12</th>\n",
       "      <th>H3_13</th>\n",
       "      <th>H3_14</th>\n",
       "      <th>H3_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.021830</td>\n",
       "      <td>-0.022091</td>\n",
       "      <td>-0.023217</td>\n",
       "      <td>-0.024061</td>\n",
       "      <td>-0.024793</td>\n",
       "      <td>-0.025011</td>\n",
       "      <td>-0.024841</td>\n",
       "      <td>-0.024436</td>\n",
       "      <td>-0.024106</td>\n",
       "      <td>-0.023762</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.026549</td>\n",
       "      <td>-0.026260</td>\n",
       "      <td>-0.025624</td>\n",
       "      <td>-0.024608</td>\n",
       "      <td>-0.023581</td>\n",
       "      <td>-0.022701</td>\n",
       "      <td>-0.022512</td>\n",
       "      <td>-0.022666</td>\n",
       "      <td>-0.023107</td>\n",
       "      <td>-0.023523</td>\n",
       "      <td>...</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.026601</td>\n",
       "      <td>-0.026467</td>\n",
       "      <td>-0.026082</td>\n",
       "      <td>-0.025734</td>\n",
       "      <td>-0.026490</td>\n",
       "      <td>-0.027129</td>\n",
       "      <td>-0.027003</td>\n",
       "      <td>-0.026047</td>\n",
       "      <td>-0.025100</td>\n",
       "      <td>-0.024086</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.025399</td>\n",
       "      <td>-0.025498</td>\n",
       "      <td>-0.025850</td>\n",
       "      <td>-0.026132</td>\n",
       "      <td>-0.026361</td>\n",
       "      <td>-0.026477</td>\n",
       "      <td>-0.026468</td>\n",
       "      <td>-0.026424</td>\n",
       "      <td>-0.026492</td>\n",
       "      <td>-0.026233</td>\n",
       "      <td>...</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.025989</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>-0.025485</td>\n",
       "      <td>-0.025054</td>\n",
       "      <td>-0.025265</td>\n",
       "      <td>-0.025427</td>\n",
       "      <td>-0.024987</td>\n",
       "      <td>-0.024082</td>\n",
       "      <td>-0.023703</td>\n",
       "      <td>-0.023614</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>-0.022011</td>\n",
       "      <td>-0.022516</td>\n",
       "      <td>-0.023482</td>\n",
       "      <td>-0.024256</td>\n",
       "      <td>-0.024894</td>\n",
       "      <td>-0.025021</td>\n",
       "      <td>-0.024626</td>\n",
       "      <td>-0.023904</td>\n",
       "      <td>-0.023579</td>\n",
       "      <td>-0.023708</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>-0.022753</td>\n",
       "      <td>-0.023719</td>\n",
       "      <td>-0.025585</td>\n",
       "      <td>-0.026531</td>\n",
       "      <td>-0.026159</td>\n",
       "      <td>-0.025076</td>\n",
       "      <td>-0.025118</td>\n",
       "      <td>-0.025259</td>\n",
       "      <td>-0.025110</td>\n",
       "      <td>-0.024550</td>\n",
       "      <td>...</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>-0.023837</td>\n",
       "      <td>-0.023682</td>\n",
       "      <td>-0.023464</td>\n",
       "      <td>-0.023381</td>\n",
       "      <td>-0.023868</td>\n",
       "      <td>-0.024479</td>\n",
       "      <td>-0.025030</td>\n",
       "      <td>-0.025496</td>\n",
       "      <td>-0.025837</td>\n",
       "      <td>-0.025838</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>-0.022903</td>\n",
       "      <td>-0.023091</td>\n",
       "      <td>-0.023489</td>\n",
       "      <td>-0.024362</td>\n",
       "      <td>-0.025999</td>\n",
       "      <td>-0.026914</td>\n",
       "      <td>-0.026765</td>\n",
       "      <td>-0.026012</td>\n",
       "      <td>-0.026193</td>\n",
       "      <td>-0.026918</td>\n",
       "      <td>...</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>-0.021729</td>\n",
       "      <td>-0.021917</td>\n",
       "      <td>-0.022170</td>\n",
       "      <td>-0.022415</td>\n",
       "      <td>-0.022510</td>\n",
       "      <td>-0.022336</td>\n",
       "      <td>-0.022205</td>\n",
       "      <td>-0.022080</td>\n",
       "      <td>-0.022254</td>\n",
       "      <td>-0.022779</td>\n",
       "      <td>...</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.28</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        REYX5_1   REYX5_2   REYX5_3   REYX5_4   REYX5_5   REYX5_6   REYX5_7  \\\n",
       "0     -0.021830 -0.022091 -0.023217 -0.024061 -0.024793 -0.025011 -0.024841   \n",
       "1     -0.026549 -0.026260 -0.025624 -0.024608 -0.023581 -0.022701 -0.022512   \n",
       "2     -0.026601 -0.026467 -0.026082 -0.025734 -0.026490 -0.027129 -0.027003   \n",
       "3     -0.025399 -0.025498 -0.025850 -0.026132 -0.026361 -0.026477 -0.026468   \n",
       "4     -0.025989 -0.025878 -0.025485 -0.025054 -0.025265 -0.025427 -0.024987   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "20995 -0.022011 -0.022516 -0.023482 -0.024256 -0.024894 -0.025021 -0.024626   \n",
       "20996 -0.022753 -0.023719 -0.025585 -0.026531 -0.026159 -0.025076 -0.025118   \n",
       "20997 -0.023837 -0.023682 -0.023464 -0.023381 -0.023868 -0.024479 -0.025030   \n",
       "20998 -0.022903 -0.023091 -0.023489 -0.024362 -0.025999 -0.026914 -0.026765   \n",
       "20999 -0.021729 -0.021917 -0.022170 -0.022415 -0.022510 -0.022336 -0.022205   \n",
       "\n",
       "        REYX5_8   REYX5_9  REYX5_10  ...  H3_6  H3_7  H3_8  H3_9  H3_10  \\\n",
       "0     -0.024436 -0.024106 -0.023762  ...  2.28  2.28  2.26  2.20   2.20   \n",
       "1     -0.022666 -0.023107 -0.023523  ...  2.26  2.24  2.26  2.26   2.22   \n",
       "2     -0.026047 -0.025100 -0.024086  ...  2.28  2.26  2.26  2.28   2.24   \n",
       "3     -0.026424 -0.026492 -0.026233  ...  2.24  2.20  2.28  2.26   2.20   \n",
       "4     -0.024082 -0.023703 -0.023614  ...  2.28  2.28  2.22  2.20   2.24   \n",
       "...         ...       ...       ...  ...   ...   ...   ...   ...    ...   \n",
       "20995 -0.023904 -0.023579 -0.023708  ...  2.28  2.24  2.26  2.22   2.26   \n",
       "20996 -0.025259 -0.025110 -0.024550  ...  2.22  2.22  2.22  2.20   2.22   \n",
       "20997 -0.025496 -0.025837 -0.025838  ...  2.28  2.20  2.26  2.26   2.28   \n",
       "20998 -0.026012 -0.026193 -0.026918  ...  2.20  2.22  2.22  2.26   2.24   \n",
       "20999 -0.022080 -0.022254 -0.022779  ...  2.28  2.20  2.26  2.24   2.24   \n",
       "\n",
       "       H3_11  H3_12  H3_13  H3_14  H3_15  \n",
       "0       2.20   2.28   2.28   2.26   2.20  \n",
       "1       2.26   2.26   2.24   2.20   2.20  \n",
       "2       2.22   2.20   2.24   2.24   2.24  \n",
       "3       2.26   2.22   2.20   2.28   2.26  \n",
       "4       2.22   2.20   2.24   2.26   2.26  \n",
       "...      ...    ...    ...    ...    ...  \n",
       "20995   2.20   2.22   2.26   2.26   2.24  \n",
       "20996   2.24   2.22   2.22   2.28   2.24  \n",
       "20997   2.20   2.20   2.28   2.26   2.26  \n",
       "20998   2.28   2.26   2.28   2.24   2.20  \n",
       "20999   2.22   2.24   2.28   2.22   2.22  \n",
       "\n",
       "[21000 rows x 138 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "la = train_data.drop(train_data.columns[:62], axis=1)\n",
    "la = la.drop(la.columns[31:62], axis=1)\n",
    "la = la.drop(la.columns[31:46], axis=1)\n",
    "#la.columns[46:61]\n",
    "\n",
    "#pd.concat([train_data.iloc[:, 62:93], train_data.iloc[:, 124:]], axis=1).head(3)\n",
    "train_data.drop(train_data.iloc[:, 93:124], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bce2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({}, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.attrs, train_data.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d74bba93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_value_stats = train_data.isnull().sum(axis=0)\n",
    "null_value_stats[null_value_stats != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb5b5be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REYX5_1</th>\n",
       "      <th>REYX5_2</th>\n",
       "      <th>REYX5_3</th>\n",
       "      <th>REYX5_4</th>\n",
       "      <th>REYX5_5</th>\n",
       "      <th>REYX5_6</th>\n",
       "      <th>REYX5_7</th>\n",
       "      <th>REYX5_8</th>\n",
       "      <th>REYX5_9</th>\n",
       "      <th>REYX5_10</th>\n",
       "      <th>...</th>\n",
       "      <th>H3_6</th>\n",
       "      <th>H3_7</th>\n",
       "      <th>H3_8</th>\n",
       "      <th>H3_9</th>\n",
       "      <th>H3_10</th>\n",
       "      <th>H3_11</th>\n",
       "      <th>H3_12</th>\n",
       "      <th>H3_13</th>\n",
       "      <th>H3_14</th>\n",
       "      <th>H3_15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.023830</td>\n",
       "      <td>-0.023770</td>\n",
       "      <td>-0.023618</td>\n",
       "      <td>-0.02342</td>\n",
       "      <td>-0.023614</td>\n",
       "      <td>-0.023788</td>\n",
       "      <td>-0.024027</td>\n",
       "      <td>-0.024397</td>\n",
       "      <td>-0.025130</td>\n",
       "      <td>-0.025704</td>\n",
       "      <td>...</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.025463</td>\n",
       "      <td>-0.025015</td>\n",
       "      <td>-0.024392</td>\n",
       "      <td>-0.02384</td>\n",
       "      <td>-0.024010</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>-0.025185</td>\n",
       "      <td>-0.025812</td>\n",
       "      <td>-0.026263</td>\n",
       "      <td>-0.026392</td>\n",
       "      <td>...</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.24</td>\n",
       "      <td>2.26</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.20</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    REYX5_1   REYX5_2   REYX5_3  REYX5_4   REYX5_5   REYX5_6   REYX5_7  \\\n",
       "0 -0.023830 -0.023770 -0.023618 -0.02342 -0.023614 -0.023788 -0.024027   \n",
       "1 -0.025463 -0.025015 -0.024392 -0.02384 -0.024010 -0.024387 -0.025185   \n",
       "\n",
       "    REYX5_8   REYX5_9  REYX5_10  ...  H3_6  H3_7  H3_8  H3_9  H3_10  H3_11  \\\n",
       "0 -0.024397 -0.025130 -0.025704  ...  2.22  2.22  2.24  2.26   2.26   2.20   \n",
       "1 -0.025812 -0.026263 -0.026392  ...  2.26  2.22  2.20  2.24   2.26   2.22   \n",
       "\n",
       "   H3_12  H3_13  H3_14  H3_15  \n",
       "0   2.20    2.2   2.24   2.20  \n",
       "1   2.22    2.2   2.20   2.28  \n",
       "\n",
       "[2 rows x 169 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data\n",
    "\n",
    "test_path = 'E:\\учеба\\Диплом\\Data_1\\mtsgrvmgn_pro.csv'\n",
    "test_data = pd.read_csv(test_path)\n",
    "\n",
    "# Validation data\n",
    "\n",
    "validation_path = 'E:\\учеба\\Диплом\\Data_1\\mtsgrvmgn_tst.csv'\n",
    "validation_data = pd.read_csv(validation_path)\n",
    "validation_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649ff919",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc00baf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>G_76_61</th>\n",
       "      <th>G_76_62</th>\n",
       "      <th>G_76_63</th>\n",
       "      <th>G_76_64</th>\n",
       "      <th>G_76_65</th>\n",
       "      <th>G_76_66</th>\n",
       "      <th>G_76_67</th>\n",
       "      <th>G_76_68</th>\n",
       "      <th>G_76_69</th>\n",
       "      <th>G_76_70</th>\n",
       "      <th>...</th>\n",
       "      <th>G_76_82</th>\n",
       "      <th>G_76_83</th>\n",
       "      <th>G_76_84</th>\n",
       "      <th>G_76_85</th>\n",
       "      <th>G_76_86</th>\n",
       "      <th>G_76_87</th>\n",
       "      <th>G_76_88</th>\n",
       "      <th>G_76_89</th>\n",
       "      <th>G_76_90</th>\n",
       "      <th>G_76_91</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-7.140</td>\n",
       "      <td>-6.955</td>\n",
       "      <td>-4.864</td>\n",
       "      <td>-2.627</td>\n",
       "      <td>-1.304</td>\n",
       "      <td>0.206</td>\n",
       "      <td>1.498</td>\n",
       "      <td>2.695</td>\n",
       "      <td>3.349</td>\n",
       "      <td>3.748</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-1.712</td>\n",
       "      <td>-2.831</td>\n",
       "      <td>-0.902</td>\n",
       "      <td>1.472</td>\n",
       "      <td>2.744</td>\n",
       "      <td>3.021</td>\n",
       "      <td>1.289</td>\n",
       "      <td>-0.776</td>\n",
       "      <td>-1.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.320</td>\n",
       "      <td>3.111</td>\n",
       "      <td>2.407</td>\n",
       "      <td>1.667</td>\n",
       "      <td>1.053</td>\n",
       "      <td>0.985</td>\n",
       "      <td>2.448</td>\n",
       "      <td>3.964</td>\n",
       "      <td>4.550</td>\n",
       "      <td>4.532</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.035</td>\n",
       "      <td>-4.874</td>\n",
       "      <td>-6.407</td>\n",
       "      <td>-6.686</td>\n",
       "      <td>-6.509</td>\n",
       "      <td>-5.178</td>\n",
       "      <td>-4.072</td>\n",
       "      <td>-5.997</td>\n",
       "      <td>-8.151</td>\n",
       "      <td>-8.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.287</td>\n",
       "      <td>-3.367</td>\n",
       "      <td>-4.547</td>\n",
       "      <td>-5.774</td>\n",
       "      <td>-5.314</td>\n",
       "      <td>-4.888</td>\n",
       "      <td>-6.173</td>\n",
       "      <td>-7.422</td>\n",
       "      <td>-8.397</td>\n",
       "      <td>-9.129</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.951</td>\n",
       "      <td>-5.020</td>\n",
       "      <td>-6.300</td>\n",
       "      <td>-7.252</td>\n",
       "      <td>-7.575</td>\n",
       "      <td>-5.288</td>\n",
       "      <td>-2.712</td>\n",
       "      <td>-1.611</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.461</td>\n",
       "      <td>-0.907</td>\n",
       "      <td>0.353</td>\n",
       "      <td>1.661</td>\n",
       "      <td>2.392</td>\n",
       "      <td>2.640</td>\n",
       "      <td>1.988</td>\n",
       "      <td>0.748</td>\n",
       "      <td>-1.289</td>\n",
       "      <td>-3.286</td>\n",
       "      <td>...</td>\n",
       "      <td>4.137</td>\n",
       "      <td>3.878</td>\n",
       "      <td>2.958</td>\n",
       "      <td>2.167</td>\n",
       "      <td>1.872</td>\n",
       "      <td>3.042</td>\n",
       "      <td>4.659</td>\n",
       "      <td>5.871</td>\n",
       "      <td>6.897</td>\n",
       "      <td>7.541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.598</td>\n",
       "      <td>3.293</td>\n",
       "      <td>2.059</td>\n",
       "      <td>0.637</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.646</td>\n",
       "      <td>-1.510</td>\n",
       "      <td>-1.861</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>1.030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291</td>\n",
       "      <td>-2.144</td>\n",
       "      <td>-3.876</td>\n",
       "      <td>-4.368</td>\n",
       "      <td>-4.478</td>\n",
       "      <td>-3.719</td>\n",
       "      <td>-2.476</td>\n",
       "      <td>-1.236</td>\n",
       "      <td>-0.069</td>\n",
       "      <td>0.449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20995</th>\n",
       "      <td>-6.419</td>\n",
       "      <td>-6.552</td>\n",
       "      <td>-5.888</td>\n",
       "      <td>-5.176</td>\n",
       "      <td>-4.726</td>\n",
       "      <td>-4.248</td>\n",
       "      <td>-4.642</td>\n",
       "      <td>-4.871</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>-3.149</td>\n",
       "      <td>...</td>\n",
       "      <td>2.143</td>\n",
       "      <td>2.251</td>\n",
       "      <td>1.869</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-1.148</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>1.433</td>\n",
       "      <td>1.648</td>\n",
       "      <td>1.507</td>\n",
       "      <td>1.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20996</th>\n",
       "      <td>-7.922</td>\n",
       "      <td>-7.927</td>\n",
       "      <td>-6.190</td>\n",
       "      <td>-4.586</td>\n",
       "      <td>-6.531</td>\n",
       "      <td>-8.603</td>\n",
       "      <td>-8.231</td>\n",
       "      <td>-7.528</td>\n",
       "      <td>-6.942</td>\n",
       "      <td>-6.275</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.617</td>\n",
       "      <td>-5.283</td>\n",
       "      <td>-4.161</td>\n",
       "      <td>-4.952</td>\n",
       "      <td>-5.931</td>\n",
       "      <td>-6.043</td>\n",
       "      <td>-5.804</td>\n",
       "      <td>-4.256</td>\n",
       "      <td>-2.641</td>\n",
       "      <td>-2.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>2.537</td>\n",
       "      <td>2.551</td>\n",
       "      <td>2.271</td>\n",
       "      <td>2.118</td>\n",
       "      <td>2.798</td>\n",
       "      <td>3.535</td>\n",
       "      <td>3.745</td>\n",
       "      <td>3.657</td>\n",
       "      <td>3.248</td>\n",
       "      <td>2.847</td>\n",
       "      <td>...</td>\n",
       "      <td>4.745</td>\n",
       "      <td>4.185</td>\n",
       "      <td>3.268</td>\n",
       "      <td>2.182</td>\n",
       "      <td>1.636</td>\n",
       "      <td>2.653</td>\n",
       "      <td>3.442</td>\n",
       "      <td>2.536</td>\n",
       "      <td>1.264</td>\n",
       "      <td>0.570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>-6.465</td>\n",
       "      <td>-6.470</td>\n",
       "      <td>-6.691</td>\n",
       "      <td>-6.994</td>\n",
       "      <td>-6.223</td>\n",
       "      <td>-5.563</td>\n",
       "      <td>-6.981</td>\n",
       "      <td>-8.325</td>\n",
       "      <td>-7.884</td>\n",
       "      <td>-7.107</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.805</td>\n",
       "      <td>-3.663</td>\n",
       "      <td>-4.644</td>\n",
       "      <td>-4.737</td>\n",
       "      <td>-4.554</td>\n",
       "      <td>-3.813</td>\n",
       "      <td>-3.145</td>\n",
       "      <td>-3.414</td>\n",
       "      <td>-3.894</td>\n",
       "      <td>-4.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>-1.203</td>\n",
       "      <td>-1.340</td>\n",
       "      <td>-1.570</td>\n",
       "      <td>-1.933</td>\n",
       "      <td>-2.884</td>\n",
       "      <td>-3.821</td>\n",
       "      <td>-4.508</td>\n",
       "      <td>-5.083</td>\n",
       "      <td>-5.254</td>\n",
       "      <td>-5.331</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.235</td>\n",
       "      <td>-2.885</td>\n",
       "      <td>-3.168</td>\n",
       "      <td>-1.114</td>\n",
       "      <td>1.070</td>\n",
       "      <td>2.461</td>\n",
       "      <td>3.435</td>\n",
       "      <td>3.160</td>\n",
       "      <td>2.696</td>\n",
       "      <td>2.679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       G_76_61  G_76_62  G_76_63  G_76_64  G_76_65  G_76_66  G_76_67  G_76_68  \\\n",
       "0       -7.140   -6.955   -4.864   -2.627   -1.304    0.206    1.498    2.695   \n",
       "1        3.320    3.111    2.407    1.667    1.053    0.985    2.448    3.964   \n",
       "2       -3.287   -3.367   -4.547   -5.774   -5.314   -4.888   -6.173   -7.422   \n",
       "3       -1.461   -0.907    0.353    1.661    2.392    2.640    1.988    0.748   \n",
       "4        3.598    3.293    2.059    0.637   -0.090   -0.646   -1.510   -1.861   \n",
       "...        ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "20995   -6.419   -6.552   -5.888   -5.176   -4.726   -4.248   -4.642   -4.871   \n",
       "20996   -7.922   -7.927   -6.190   -4.586   -6.531   -8.603   -8.231   -7.528   \n",
       "20997    2.537    2.551    2.271    2.118    2.798    3.535    3.745    3.657   \n",
       "20998   -6.465   -6.470   -6.691   -6.994   -6.223   -5.563   -6.981   -8.325   \n",
       "20999   -1.203   -1.340   -1.570   -1.933   -2.884   -3.821   -4.508   -5.083   \n",
       "\n",
       "       G_76_69  G_76_70  ...  G_76_82  G_76_83  G_76_84  G_76_85  G_76_86  \\\n",
       "0        3.349    3.748  ...   -0.020   -1.712   -2.831   -0.902    1.472   \n",
       "1        4.550    4.532  ...   -3.035   -4.874   -6.407   -6.686   -6.509   \n",
       "2       -8.397   -9.129  ...   -3.951   -5.020   -6.300   -7.252   -7.575   \n",
       "3       -1.289   -3.286  ...    4.137    3.878    2.958    2.167    1.872   \n",
       "4       -0.519    1.030  ...   -0.291   -2.144   -3.876   -4.368   -4.478   \n",
       "...        ...      ...  ...      ...      ...      ...      ...      ...   \n",
       "20995   -4.068   -3.149  ...    2.143    2.251    1.869    0.179   -1.148   \n",
       "20996   -6.942   -6.275  ...   -6.617   -5.283   -4.161   -4.952   -5.931   \n",
       "20997    3.248    2.847  ...    4.745    4.185    3.268    2.182    1.636   \n",
       "20998   -7.884   -7.107  ...   -2.805   -3.663   -4.644   -4.737   -4.554   \n",
       "20999   -5.254   -5.331  ...   -2.235   -2.885   -3.168   -1.114    1.070   \n",
       "\n",
       "       G_76_87  G_76_88  G_76_89  G_76_90  G_76_91  \n",
       "0        2.744    3.021    1.289   -0.776   -1.867  \n",
       "1       -5.178   -4.072   -5.997   -8.151   -8.612  \n",
       "2       -5.288   -2.712   -1.611   -0.871   -0.480  \n",
       "3        3.042    4.659    5.871    6.897    7.541  \n",
       "4       -3.719   -2.476   -1.236   -0.069    0.449  \n",
       "...        ...      ...      ...      ...      ...  \n",
       "20995   -0.056    1.433    1.648    1.507    1.543  \n",
       "20996   -6.043   -5.804   -4.256   -2.641   -2.205  \n",
       "20997    2.653    3.442    2.536    1.264    0.570  \n",
       "20998   -3.813   -3.145   -3.414   -3.894   -4.162  \n",
       "20999    2.461    3.435    3.160    2.696    2.679  \n",
       "\n",
       "[21000 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PREPARATION\n",
    "\n",
    "import torch\n",
    "\n",
    "G_train = pd.concat([train_data.iloc[:, 62:93], train_data.iloc[:, 124:]], axis=1)\n",
    "G_test = pd.concat([test_data.iloc[:, 62:93], test_data.iloc[:, 124:]], axis=1)\n",
    "G_valid = pd.concat([validation_data.iloc[:, 62:93], validation_data.iloc[:, 124:]], axis=1)\n",
    "# G, H1, H2, H3\n",
    "\n",
    "M_train = train_data.iloc[:, 93:]\n",
    "M_test = test_data.iloc[:, 93:]\n",
    "M_valid = validation_data.iloc[:, 93:]\n",
    "# M, H1, H2, H3\n",
    "\n",
    "MT_train = pd.concat([train_data.iloc[:, :62], train_data.iloc[:, 124:]], axis=1)\n",
    "MT_test = pd.concat([test_data.iloc[:, :62], test_data.iloc[:, 124:]], axis=1)\n",
    "MT_valid = pd.concat([validation_data.iloc[:, :62], validation_data.iloc[:, 124:]], axis=1)\n",
    "# Re, Im, H1, H2, H3\n",
    "\n",
    "G_M_train = train_data.iloc[:, 62:]\n",
    "G_M_test = test_data.iloc[:, 62:]\n",
    "G_M_valid = validation_data.iloc[:, 62:]\n",
    "# G, M, H1, H2, H3\n",
    "\n",
    "G_MT_train = train_data.drop(train_data.iloc[:, 93:124], axis=1)\n",
    "G_MT_test = test_data.drop(test_data.iloc[:, 93:124], axis=1)\n",
    "G_MT_valid = validation_data.drop(validation_data.iloc[:, 93:124], axis=1)\n",
    "# Re, Im, G, H1, H2, H3\n",
    "\n",
    "M_MT_train = train_data.drop(train_data.iloc[:, 62:93], axis=1)\n",
    "M_MT_test = test_data.drop(test_data.iloc[:, 62:93], axis=1)\n",
    "M_MT_valid = validation_data.drop(validation_data.iloc[:, 62:93], axis=1)\n",
    "# Re, Im, M, H1, H2, H3\n",
    "\n",
    "G_M_MT_train = train_data\n",
    "G_M_MT_test = test_data\n",
    "G_M_MT_valid = validation_data\n",
    "\n",
    "G_MT_train.iloc[:, 62:93]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a40335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 93])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CHOSEN LAYER - H1\n",
    "\n",
    "X_G_train = G_train.iloc[:, :31]\n",
    "Y_train = G_train.iloc[:, 31:46]\n",
    "\n",
    "X_G_test = G_test.iloc[:, :31]\n",
    "Y_test = G_test.iloc[:, 31:46]\n",
    "\n",
    "X_G_valid = G_valid.iloc[:, :31]\n",
    "Y_valid = G_valid.iloc[:, 31:46]\n",
    "\n",
    "X_G_train, y_train = torch.tensor(X_G_train.values).type(torch.float), torch.tensor(Y_train.values).type(torch.float)\n",
    "X_G_test, y_test = torch.tensor(X_G_test.values).type(torch.float), torch.tensor(Y_test.values).type(torch.float)\n",
    "X_G_valid, y_valid = torch.tensor(X_G_valid.values).type(torch.float), torch.tensor(Y_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_M_train = M_train.iloc[:, :31]\n",
    "X_M_test = M_test.iloc[:, :31]\n",
    "X_M_valid = M_valid.iloc[:, :31]\n",
    "\n",
    "X_M_train = torch.tensor(X_M_train.values).type(torch.float)\n",
    "X_M_test = torch.tensor(X_M_test.values).type(torch.float)\n",
    "X_M_valid = torch.tensor(X_M_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_MT_train = MT_train.iloc[:, :62]\n",
    "X_MT_test = MT_test.iloc[:, :62]\n",
    "X_MT_valid = MT_valid.iloc[:, :62]\n",
    "\n",
    "X_MT_train = torch.tensor(X_MT_train.values).type(torch.float)\n",
    "X_MT_test = torch.tensor(X_MT_test.values).type(torch.float)\n",
    "X_MT_valid = torch.tensor(X_MT_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_G_M_train = G_M_train.iloc[:, :62]\n",
    "X_G_M_test = G_M_test.iloc[:, :62]\n",
    "X_G_M_valid = G_M_valid.iloc[:, :62]\n",
    "\n",
    "X_G_M_train = torch.tensor(X_G_M_train.values).type(torch.float)\n",
    "X_G_M_test = torch.tensor(X_G_M_test.values).type(torch.float)\n",
    "X_G_M_valid = torch.tensor(X_G_M_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_G_MT_train = G_MT_train.iloc[:, :93]\n",
    "X_G_MT_test = G_MT_test.iloc[:, :93]\n",
    "X_G_MT_valid = G_MT_valid.iloc[:, :93]\n",
    "\n",
    "X_G_MT_train = torch.tensor(X_G_MT_train.values).type(torch.float)\n",
    "X_G_MT_test = torch.tensor(X_G_MT_test.values).type(torch.float)\n",
    "X_G_MT_valid = torch.tensor(X_G_MT_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_M_MT_train = M_MT_train.iloc[:, :93]\n",
    "X_M_MT_test = M_MT_test.iloc[:, :93]\n",
    "X_M_MT_valid = M_MT_valid.iloc[:, :93]\n",
    "\n",
    "X_M_MT_train = torch.tensor(X_M_MT_train.values).type(torch.float)\n",
    "X_M_MT_test = torch.tensor(X_M_MT_test.values).type(torch.float)\n",
    "X_M_MT_valid = torch.tensor(X_M_MT_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_G_M_MT_train = G_M_MT_train.iloc[:, :124]\n",
    "X_G_M_MT_test = G_M_MT_test.iloc[:, :124]\n",
    "X_G_M_MT_valid = G_M_MT_valid.iloc[:, :124]\n",
    "\n",
    "X_G_M_MT_train = torch.tensor(X_G_M_MT_train.values).type(torch.float)\n",
    "X_G_M_MT_test = torch.tensor(X_G_M_MT_test.values).type(torch.float)\n",
    "X_G_M_MT_valid = torch.tensor(X_G_M_MT_valid.values).type(torch.float)\n",
    "\n",
    "\n",
    "X_G_MT_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2121cd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Initial_Model_V0(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=input_dim,\n",
    "                             out_features=hidden_units)\n",
    "        self.act1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(in_features=hidden_units,\n",
    "                             out_features=output_dim)\n",
    "        #self.act2 = nn.Linear(in_features=output_dim,\n",
    "        #                      out_features=output_dim,\n",
    "        #                      bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.act2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0044e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# НАПИСАТЬ НОРМ КОД, ОБЩИЙ ДЛЯ ВСЕХ КОМБИНАЦИЙ МЕТОДОВ\n",
    "\n",
    "class G_M_Stacking_Model_V0(nn.Module):\n",
    "    def __init__(self, init_input_dim, hidden_units, output_dim):\n",
    "        super().__init__()\n",
    "        self.initial_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=init_input_dim,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_dim)\n",
    "        )\n",
    "        self.meta_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=10,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_g, x_m):\n",
    "        res = []\n",
    "        for net in range(5):\n",
    "            res.append(self.initial_layers(x_g))\n",
    "        for net in range(5):\n",
    "            res.append(self.initial_layers(x_m))\n",
    "        x = self.meta_layers(torch.cat(res, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278834c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_MT_Stacking_Model_V0(nn.Module):\n",
    "    def __init__(self, init_input_dim, hidden_units, output_dim):\n",
    "        super().__init__()\n",
    "        self.initial_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=init_input_dim,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_dim)\n",
    "            \n",
    "        )\n",
    "        self.meta_layers = nn.Sequential(\n",
    "            nn.Linear(in_features=10,\n",
    "                      out_features=hidden_units),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=hidden_units,\n",
    "                      out_features=output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_g, x_m):\n",
    "        res = []\n",
    "        for net in range(5):\n",
    "            res.append(self.initial_layers(x_g))\n",
    "        for net in range(5):\n",
    "            res.append(self.initial_layers(x_m))\n",
    "        x = self.meta_layers(torch.cat(res, 1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcec885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_V1():\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, prev_loss, next_loss):\n",
    "        if (next_loss - prev_loss) >= min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c209dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, validation_loss):\n",
    "        if (abs(validation_loss - train_loss)) < self.min_delta:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.patience:  \n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90335793",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "988a8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc273888",
   "metadata": {},
   "source": [
    "# Initial - G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "568ab18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "input_dim = 31\n",
    "hidden_units = 31\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "G_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    G_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "19029d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.1400, -6.9550, -4.8640,  ...,  1.2890, -0.7760, -1.8670],\n",
       "        [ 3.3200,  3.1110,  2.4070,  ..., -5.9970, -8.1510, -8.6120],\n",
       "        [-3.2870, -3.3670, -4.5470,  ..., -1.6110, -0.8710, -0.4800],\n",
       "        ...,\n",
       "        [ 2.5370,  2.5510,  2.2710,  ...,  2.5360,  1.2640,  0.5700],\n",
       "        [-6.4650, -6.4700, -6.6910,  ..., -3.4140, -3.8940, -4.1620],\n",
       "        [-1.2030, -1.3400, -1.5700,  ...,  3.1600,  2.6960,  2.6790]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_G_train, y_train = X_G_train.to(device), y_train.to(device)\n",
    "X_G_test, y_test = X_G_test.to(device), y_test.to(device)\n",
    "X_G_valid, y_valid = X_G_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_G_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "75ae349c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 4.22463 | Test Loss: 3.67559 | Delta: 3.64584\n",
      "Epoch: 200 | Loss: 0.00411 | Test Loss: 0.00412 | Delta: 0.00407\n",
      "Epoch: 400 | Loss: 0.00297 | Test Loss: 0.00297 | Delta: 0.00294\n",
      "Epoch: 600 | Loss: 0.00264 | Test Loss: 0.00264 | Delta: 0.00263\n",
      "Epoch: 800 | Loss: 0.00247 | Test Loss: 0.00249 | Delta: 0.00247\n",
      "Epoch: 1000 | Loss: 0.00234 | Test Loss: 0.00237 | Delta: 0.00234\n",
      "Epoch: 1200 | Loss: 0.00223 | Test Loss: 0.00226 | Delta: 0.00223\n",
      "Epoch: 1400 | Loss: 0.00212 | Test Loss: 0.00216 | Delta: 0.00211\n",
      "Epoch: 1600 | Loss: 0.00199 | Test Loss: 0.00205 | Delta: 0.00199\n",
      "Epoch: 1800 | Loss: 0.00185 | Test Loss: 0.00189 | Delta: 0.00184\n",
      "Epoch: 2000 | Loss: 0.00172 | Test Loss: 0.00176 | Delta: 0.00173\n",
      "Epoch: 2200 | Loss: 0.00164 | Test Loss: 0.00169 | Delta: 0.00165\n",
      "Epoch: 2400 | Loss: 0.00156 | Test Loss: 0.00162 | Delta: 0.00158\n",
      "Epoch: 2600 | Loss: 0.00150 | Test Loss: 0.00155 | Delta: 0.00152\n",
      "Epoch: 2800 | Loss: 0.00145 | Test Loss: 0.00150 | Delta: 0.00147\n",
      "Epoch: 3000 | Loss: 0.00141 | Test Loss: 0.00145 | Delta: 0.00143\n",
      "Epoch: 3200 | Loss: 0.00138 | Test Loss: 0.00141 | Delta: 0.00139\n",
      "Epoch: 3400 | Loss: 0.00136 | Test Loss: 0.00139 | Delta: 0.00137\n",
      "Epoch: 3600 | Loss: 0.00133 | Test Loss: 0.00135 | Delta: 0.00134\n",
      "Epoch: 3800 | Loss: 0.00130 | Test Loss: 0.00133 | Delta: 0.00131\n",
      "Epoch: 4000 | Loss: 0.00128 | Test Loss: 0.00131 | Delta: 0.00129\n",
      "Epoch: 4200 | Loss: 0.00126 | Test Loss: 0.00129 | Delta: 0.00127\n",
      "Epoch: 4400 | Loss: 0.00126 | Test Loss: 0.00127 | Delta: 0.00126\n",
      "Epoch: 4600 | Loss: 0.00123 | Test Loss: 0.00126 | Delta: 0.00124\n",
      "Epoch: 4800 | Loss: 0.00122 | Test Loss: 0.00124 | Delta: 0.00123\n",
      "Early stopping at epoch: 4841\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.67138 | Test Loss: 1.33743 | Delta: 1.31865\n",
      "Epoch: 200 | Loss: 0.00279 | Test Loss: 0.00271 | Delta: 0.00278\n",
      "Epoch: 400 | Loss: 0.00174 | Test Loss: 0.00171 | Delta: 0.00173\n",
      "Epoch: 600 | Loss: 0.00149 | Test Loss: 0.00150 | Delta: 0.00149\n",
      "Epoch: 800 | Loss: 0.00142 | Test Loss: 0.00143 | Delta: 0.00142\n",
      "Epoch: 1000 | Loss: 0.00139 | Test Loss: 0.00140 | Delta: 0.00139\n",
      "Epoch: 1200 | Loss: 0.00137 | Test Loss: 0.00138 | Delta: 0.00138\n",
      "Epoch: 1400 | Loss: 0.00136 | Test Loss: 0.00137 | Delta: 0.00136\n",
      "Epoch: 1600 | Loss: 0.00134 | Test Loss: 0.00136 | Delta: 0.00135\n",
      "Epoch: 1800 | Loss: 0.00133 | Test Loss: 0.00135 | Delta: 0.00134\n",
      "Epoch: 2000 | Loss: 0.00132 | Test Loss: 0.00134 | Delta: 0.00133\n",
      "Epoch: 2200 | Loss: 0.00131 | Test Loss: 0.00133 | Delta: 0.00132\n",
      "Epoch: 2400 | Loss: 0.00130 | Test Loss: 0.00132 | Delta: 0.00131\n",
      "Epoch: 2600 | Loss: 0.00129 | Test Loss: 0.00131 | Delta: 0.00130\n",
      "Epoch: 2800 | Loss: 0.00128 | Test Loss: 0.00130 | Delta: 0.00129\n",
      "Epoch: 3000 | Loss: 0.00127 | Test Loss: 0.00128 | Delta: 0.00128\n",
      "Epoch: 3200 | Loss: 0.00125 | Test Loss: 0.00127 | Delta: 0.00127\n",
      "Epoch: 3400 | Loss: 0.00124 | Test Loss: 0.00126 | Delta: 0.00125\n",
      "Epoch: 3600 | Loss: 0.00125 | Test Loss: 0.00125 | Delta: 0.00124\n",
      "Epoch: 3800 | Loss: 0.00131 | Test Loss: 0.00130 | Delta: 0.00131\n",
      "Epoch: 4000 | Loss: 0.00121 | Test Loss: 0.00123 | Delta: 0.00124\n",
      "Epoch: 4200 | Loss: 0.00119 | Test Loss: 0.00121 | Delta: 0.00121\n",
      "Epoch: 4400 | Loss: 0.00118 | Test Loss: 0.00120 | Delta: 0.00120\n",
      "Epoch: 4600 | Loss: 0.00117 | Test Loss: 0.00119 | Delta: 0.00119\n",
      "Epoch: 4800 | Loss: 0.00116 | Test Loss: 0.00118 | Delta: 0.00118\n",
      "Epoch: 5000 | Loss: 0.00115 | Test Loss: 0.00117 | Delta: 0.00117\n",
      "Epoch: 5200 | Loss: 0.00114 | Test Loss: 0.00116 | Delta: 0.00116\n",
      "Epoch: 5400 | Loss: 0.00113 | Test Loss: 0.00116 | Delta: 0.00115\n",
      "Epoch: 5600 | Loss: 0.00112 | Test Loss: 0.00115 | Delta: 0.00114\n",
      "Epoch: 5800 | Loss: 0.00112 | Test Loss: 0.00114 | Delta: 0.00114\n",
      "Epoch: 6000 | Loss: 0.00218 | Test Loss: 0.00183 | Delta: 0.00180\n",
      "Epoch: 6200 | Loss: 0.00110 | Test Loss: 0.00112 | Delta: 0.00112\n",
      "Early stopping at epoch: 6351\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 0.76537 | Test Loss: 0.56569 | Delta: 0.56310\n",
      "Epoch: 200 | Loss: 0.00184 | Test Loss: 0.00181 | Delta: 0.00179\n",
      "Epoch: 400 | Loss: 0.00139 | Test Loss: 0.00135 | Delta: 0.00136\n",
      "Epoch: 600 | Loss: 0.00129 | Test Loss: 0.00125 | Delta: 0.00126\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00120 | Delta: 0.00122\n",
      "Epoch: 1000 | Loss: 0.00120 | Test Loss: 0.00117 | Delta: 0.00119\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00115 | Delta: 0.00116\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00113 | Delta: 0.00114\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00111 | Delta: 0.00112\n",
      "Epoch: 1800 | Loss: 0.00110 | Test Loss: 0.00109 | Delta: 0.00110\n",
      "Epoch: 2000 | Loss: 0.00108 | Test Loss: 0.00106 | Delta: 0.00107\n",
      "Epoch: 2200 | Loss: 0.00105 | Test Loss: 0.00104 | Delta: 0.00105\n",
      "Epoch: 2400 | Loss: 0.00103 | Test Loss: 0.00101 | Delta: 0.00103\n",
      "Epoch: 2600 | Loss: 0.00116 | Test Loss: 0.00099 | Delta: 0.00101\n",
      "Epoch: 2800 | Loss: 0.00152 | Test Loss: 0.00131 | Delta: 0.00132\n",
      "Epoch: 3000 | Loss: 0.00098 | Test Loss: 0.00096 | Delta: 0.00098\n",
      "Epoch: 3200 | Loss: 0.00097 | Test Loss: 0.00095 | Delta: 0.00097\n",
      "Epoch: 3400 | Loss: 0.00097 | Test Loss: 0.00094 | Delta: 0.00097\n",
      "Epoch: 3600 | Loss: 0.00094 | Test Loss: 0.00093 | Delta: 0.00094\n",
      "Epoch: 3800 | Loss: 0.00094 | Test Loss: 0.00092 | Delta: 0.00094\n",
      "Epoch: 4000 | Loss: 0.00137 | Test Loss: 0.00182 | Delta: 0.00188\n",
      "Epoch: 4200 | Loss: 0.00092 | Test Loss: 0.00090 | Delta: 0.00092\n",
      "Epoch: 4400 | Loss: 0.00092 | Test Loss: 0.00090 | Delta: 0.00092\n",
      "Epoch: 4600 | Loss: 0.00090 | Test Loss: 0.00089 | Delta: 0.00091\n",
      "Early stopping at epoch: 4749\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 2.18368 | Test Loss: 1.76616 | Delta: 1.74884\n",
      "Epoch: 200 | Loss: 0.00254 | Test Loss: 0.00259 | Delta: 0.00258\n",
      "Epoch: 400 | Loss: 0.00165 | Test Loss: 0.00169 | Delta: 0.00168\n",
      "Epoch: 600 | Loss: 0.00142 | Test Loss: 0.00145 | Delta: 0.00143\n",
      "Epoch: 800 | Loss: 0.00131 | Test Loss: 0.00134 | Delta: 0.00132\n",
      "Epoch: 1000 | Loss: 0.00126 | Test Loss: 0.00128 | Delta: 0.00127\n",
      "Epoch: 1200 | Loss: 0.00122 | Test Loss: 0.00124 | Delta: 0.00123\n",
      "Epoch: 1400 | Loss: 0.00119 | Test Loss: 0.00121 | Delta: 0.00120\n",
      "Epoch: 1600 | Loss: 0.00117 | Test Loss: 0.00119 | Delta: 0.00118\n",
      "Epoch: 1800 | Loss: 0.00115 | Test Loss: 0.00117 | Delta: 0.00116\n",
      "Epoch: 2000 | Loss: 0.00114 | Test Loss: 0.00115 | Delta: 0.00114\n",
      "Epoch: 2200 | Loss: 0.00112 | Test Loss: 0.00114 | Delta: 0.00112\n",
      "Epoch: 2400 | Loss: 0.00110 | Test Loss: 0.00112 | Delta: 0.00111\n",
      "Epoch: 2600 | Loss: 0.00108 | Test Loss: 0.00110 | Delta: 0.00109\n",
      "Epoch: 2800 | Loss: 0.00106 | Test Loss: 0.00108 | Delta: 0.00107\n",
      "Epoch: 3000 | Loss: 0.00104 | Test Loss: 0.00106 | Delta: 0.00105\n",
      "Epoch: 3200 | Loss: 0.00102 | Test Loss: 0.00103 | Delta: 0.00102\n",
      "Epoch: 3400 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00099\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00098 | Delta: 0.00097\n",
      "Epoch: 3800 | Loss: 0.00097 | Test Loss: 0.00097 | Delta: 0.00095\n",
      "Epoch: 4000 | Loss: 0.00091 | Test Loss: 0.00092 | Delta: 0.00092\n",
      "Epoch: 4200 | Loss: 0.00089 | Test Loss: 0.00090 | Delta: 0.00090\n",
      "Epoch: 4400 | Loss: 0.00087 | Test Loss: 0.00087 | Delta: 0.00088\n",
      "Epoch: 4600 | Loss: 0.00085 | Test Loss: 0.00086 | Delta: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00084 | Test Loss: 0.00084 | Delta: 0.00085\n",
      "Epoch: 5000 | Loss: 0.00082 | Test Loss: 0.00083 | Delta: 0.00083\n",
      "Epoch: 5200 | Loss: 0.00081 | Test Loss: 0.00081 | Delta: 0.00082\n",
      "Epoch: 5400 | Loss: 0.00080 | Test Loss: 0.00081 | Delta: 0.00082\n",
      "Epoch: 5600 | Loss: 0.00079 | Test Loss: 0.00079 | Delta: 0.00080\n",
      "Early stopping at epoch: 5687\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 0.81488 | Test Loss: 0.59355 | Delta: 0.59877\n",
      "Epoch: 200 | Loss: 0.00205 | Test Loss: 0.00204 | Delta: 0.00203\n",
      "Epoch: 400 | Loss: 0.00143 | Test Loss: 0.00149 | Delta: 0.00146\n",
      "Epoch: 600 | Loss: 0.00130 | Test Loss: 0.00136 | Delta: 0.00132\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00130 | Delta: 0.00126\n",
      "Epoch: 1000 | Loss: 0.00120 | Test Loss: 0.00126 | Delta: 0.00122\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00123 | Delta: 0.00120\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00121 | Delta: 0.00118\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00119 | Delta: 0.00116\n",
      "Epoch: 1800 | Loss: 0.00112 | Test Loss: 0.00117 | Delta: 0.00114\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00115 | Delta: 0.00112\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00113 | Delta: 0.00111\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00111 | Delta: 0.00109\n",
      "Epoch: 2600 | Loss: 0.00104 | Test Loss: 0.00109 | Delta: 0.00106\n",
      "Epoch: 2800 | Loss: 0.00101 | Test Loss: 0.00106 | Delta: 0.00104\n",
      "Epoch: 3000 | Loss: 0.00098 | Test Loss: 0.00103 | Delta: 0.00101\n",
      "Epoch: 3200 | Loss: 0.00096 | Test Loss: 0.00100 | Delta: 0.00098\n",
      "Epoch: 3400 | Loss: 0.00094 | Test Loss: 0.00098 | Delta: 0.00096\n",
      "Epoch: 3600 | Loss: 0.00091 | Test Loss: 0.00095 | Delta: 0.00094\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00093 | Delta: 0.00091\n",
      "Epoch: 4000 | Loss: 0.00099 | Test Loss: 0.00096 | Delta: 0.00093\n",
      "Epoch: 4200 | Loss: 0.00110 | Test Loss: 0.00142 | Delta: 0.00135\n",
      "Epoch: 4400 | Loss: 0.00084 | Test Loss: 0.00087 | Delta: 0.00086\n",
      "Epoch: 4600 | Loss: 0.00083 | Test Loss: 0.00086 | Delta: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00083 | Test Loss: 0.00086 | Delta: 0.00088\n",
      "Early stopping at epoch: 4802\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 3.62345 | Test Loss: 3.19497 | Delta: 3.15334\n",
      "Epoch: 200 | Loss: 0.00281 | Test Loss: 0.00280 | Delta: 0.00277\n",
      "Epoch: 400 | Loss: 0.00174 | Test Loss: 0.00170 | Delta: 0.00172\n",
      "Epoch: 600 | Loss: 0.00144 | Test Loss: 0.00141 | Delta: 0.00144\n",
      "Epoch: 800 | Loss: 0.00131 | Test Loss: 0.00130 | Delta: 0.00132\n",
      "Epoch: 1000 | Loss: 0.00124 | Test Loss: 0.00124 | Delta: 0.00124\n",
      "Epoch: 1200 | Loss: 0.00120 | Test Loss: 0.00121 | Delta: 0.00120\n",
      "Epoch: 1400 | Loss: 0.00118 | Test Loss: 0.00119 | Delta: 0.00118\n",
      "Epoch: 1600 | Loss: 0.00116 | Test Loss: 0.00117 | Delta: 0.00116\n",
      "Epoch: 1800 | Loss: 0.00114 | Test Loss: 0.00116 | Delta: 0.00114\n",
      "Epoch: 2000 | Loss: 0.00112 | Test Loss: 0.00114 | Delta: 0.00112\n",
      "Epoch: 2200 | Loss: 0.00111 | Test Loss: 0.00112 | Delta: 0.00110\n",
      "Epoch: 2400 | Loss: 0.00109 | Test Loss: 0.00111 | Delta: 0.00108\n",
      "Epoch: 2600 | Loss: 0.00107 | Test Loss: 0.00109 | Delta: 0.00107\n",
      "Epoch: 2800 | Loss: 0.00105 | Test Loss: 0.00107 | Delta: 0.00105\n",
      "Epoch: 3000 | Loss: 0.00103 | Test Loss: 0.00105 | Delta: 0.00103\n",
      "Epoch: 3200 | Loss: 0.00101 | Test Loss: 0.00103 | Delta: 0.00100\n",
      "Epoch: 3400 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00098\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00099 | Delta: 0.00096\n",
      "Epoch: 3800 | Loss: 0.00094 | Test Loss: 0.00097 | Delta: 0.00094\n",
      "Epoch: 4000 | Loss: 0.00092 | Test Loss: 0.00094 | Delta: 0.00092\n",
      "Epoch: 4200 | Loss: 0.00102 | Test Loss: 0.00093 | Delta: 0.00090\n",
      "Epoch: 4400 | Loss: 0.00095 | Test Loss: 0.00094 | Delta: 0.00092\n",
      "Epoch: 4600 | Loss: 0.00087 | Test Loss: 0.00090 | Delta: 0.00088\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00087 | Delta: 0.00085\n",
      "Epoch: 5000 | Loss: 0.00084 | Test Loss: 0.00086 | Delta: 0.00083\n",
      "Epoch: 5200 | Loss: 0.00083 | Test Loss: 0.00085 | Delta: 0.00082\n",
      "Epoch: 5400 | Loss: 0.00082 | Test Loss: 0.00083 | Delta: 0.00081\n",
      "Early stopping at epoch: 5525\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 1.44735 | Test Loss: 1.12217 | Delta: 1.09592\n",
      "Epoch: 200 | Loss: 0.00225 | Test Loss: 0.00227 | Delta: 0.00218\n",
      "Epoch: 400 | Loss: 0.00150 | Test Loss: 0.00153 | Delta: 0.00145\n",
      "Epoch: 600 | Loss: 0.00135 | Test Loss: 0.00138 | Delta: 0.00131\n",
      "Epoch: 800 | Loss: 0.00129 | Test Loss: 0.00131 | Delta: 0.00125\n",
      "Epoch: 1000 | Loss: 0.00125 | Test Loss: 0.00127 | Delta: 0.00121\n",
      "Epoch: 1200 | Loss: 0.00122 | Test Loss: 0.00124 | Delta: 0.00118\n",
      "Epoch: 1400 | Loss: 0.00119 | Test Loss: 0.00121 | Delta: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00117 | Test Loss: 0.00119 | Delta: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00114 | Test Loss: 0.00117 | Delta: 0.00111\n",
      "Epoch: 2000 | Loss: 0.00112 | Test Loss: 0.00115 | Delta: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00110 | Test Loss: 0.00113 | Delta: 0.00107\n",
      "Epoch: 2400 | Loss: 0.00107 | Test Loss: 0.00111 | Delta: 0.00105\n",
      "Epoch: 2600 | Loss: 0.00105 | Test Loss: 0.00109 | Delta: 0.00102\n",
      "Epoch: 2800 | Loss: 0.00102 | Test Loss: 0.00107 | Delta: 0.00100\n",
      "Epoch: 3000 | Loss: 0.00100 | Test Loss: 0.00105 | Delta: 0.00097\n",
      "Epoch: 3200 | Loss: 0.00103 | Test Loss: 0.00105 | Delta: 0.00098\n",
      "Epoch: 3400 | Loss: 0.00095 | Test Loss: 0.00100 | Delta: 0.00092\n",
      "Epoch: 3600 | Loss: 0.00092 | Test Loss: 0.00098 | Delta: 0.00090\n",
      "Epoch: 3800 | Loss: 0.00090 | Test Loss: 0.00095 | Delta: 0.00088\n",
      "Epoch: 4000 | Loss: 0.00088 | Test Loss: 0.00093 | Delta: 0.00086\n",
      "Epoch: 4200 | Loss: 0.00086 | Test Loss: 0.00091 | Delta: 0.00084\n",
      "Epoch: 4400 | Loss: 0.00085 | Test Loss: 0.00090 | Delta: 0.00082\n",
      "Epoch: 4600 | Loss: 0.00085 | Test Loss: 0.00089 | Delta: 0.00082\n",
      "Epoch: 4800 | Loss: 0.00082 | Test Loss: 0.00087 | Delta: 0.00079\n",
      "Epoch: 5000 | Loss: 0.00081 | Test Loss: 0.00086 | Delta: 0.00079\n",
      "Epoch: 5200 | Loss: 0.00079 | Test Loss: 0.00084 | Delta: 0.00077\n",
      "Epoch: 5400 | Loss: 0.00078 | Test Loss: 0.00084 | Delta: 0.00076\n",
      "Epoch: 5600 | Loss: 0.00079 | Test Loss: 0.00086 | Delta: 0.00079\n",
      "Early stopping at epoch: 5667\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.51605 | Test Loss: 2.14342 | Delta: 2.12741\n",
      "Epoch: 200 | Loss: 0.00265 | Test Loss: 0.00258 | Delta: 0.00265\n",
      "Epoch: 400 | Loss: 0.00161 | Test Loss: 0.00161 | Delta: 0.00163\n",
      "Epoch: 600 | Loss: 0.00137 | Test Loss: 0.00139 | Delta: 0.00139\n",
      "Epoch: 800 | Loss: 0.00129 | Test Loss: 0.00131 | Delta: 0.00130\n",
      "Epoch: 1000 | Loss: 0.00124 | Test Loss: 0.00126 | Delta: 0.00125\n",
      "Epoch: 1200 | Loss: 0.00121 | Test Loss: 0.00123 | Delta: 0.00122\n",
      "Epoch: 1400 | Loss: 0.00118 | Test Loss: 0.00120 | Delta: 0.00120\n",
      "Epoch: 1600 | Loss: 0.00116 | Test Loss: 0.00118 | Delta: 0.00118\n",
      "Epoch: 1800 | Loss: 0.00115 | Test Loss: 0.00117 | Delta: 0.00116\n",
      "Epoch: 2000 | Loss: 0.00113 | Test Loss: 0.00115 | Delta: 0.00114\n",
      "Epoch: 2200 | Loss: 0.00112 | Test Loss: 0.00114 | Delta: 0.00113\n",
      "Epoch: 2400 | Loss: 0.00110 | Test Loss: 0.00112 | Delta: 0.00111\n",
      "Epoch: 2600 | Loss: 0.00108 | Test Loss: 0.00111 | Delta: 0.00109\n",
      "Epoch: 2800 | Loss: 0.00107 | Test Loss: 0.00109 | Delta: 0.00108\n",
      "Epoch: 3000 | Loss: 0.00105 | Test Loss: 0.00107 | Delta: 0.00106\n",
      "Epoch: 3200 | Loss: 0.00103 | Test Loss: 0.00105 | Delta: 0.00104\n",
      "Epoch: 3400 | Loss: 0.00101 | Test Loss: 0.00103 | Delta: 0.00102\n",
      "Epoch: 3600 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00100\n",
      "Epoch: 3800 | Loss: 0.00097 | Test Loss: 0.00101 | Delta: 0.00099\n",
      "Epoch: 4000 | Loss: 0.00094 | Test Loss: 0.00096 | Delta: 0.00094\n",
      "Epoch: 4200 | Loss: 0.00092 | Test Loss: 0.00094 | Delta: 0.00092\n",
      "Epoch: 4400 | Loss: 0.00090 | Test Loss: 0.00092 | Delta: 0.00090\n",
      "Epoch: 4600 | Loss: 0.00089 | Test Loss: 0.00091 | Delta: 0.00089\n",
      "Epoch: 4800 | Loss: 0.00088 | Test Loss: 0.00091 | Delta: 0.00088\n",
      "Epoch: 5000 | Loss: 0.00086 | Test Loss: 0.00088 | Delta: 0.00086\n",
      "Epoch: 5200 | Loss: 0.00085 | Test Loss: 0.00087 | Delta: 0.00085\n",
      "Epoch: 5400 | Loss: 0.00085 | Test Loss: 0.00087 | Delta: 0.00085\n",
      "Epoch: 5600 | Loss: 0.00083 | Test Loss: 0.00084 | Delta: 0.00082\n",
      "Epoch: 5800 | Loss: 0.00082 | Test Loss: 0.00084 | Delta: 0.00082\n",
      "Epoch: 6000 | Loss: 0.00083 | Test Loss: 0.00087 | Delta: 0.00085\n",
      "Epoch: 6200 | Loss: 0.00080 | Test Loss: 0.00082 | Delta: 0.00080\n",
      "Early stopping at epoch: 6325\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 0.11836 | Test Loss: 0.07053 | Delta: 0.06983\n",
      "Epoch: 200 | Loss: 0.00140 | Test Loss: 0.00139 | Delta: 0.00146\n",
      "Epoch: 400 | Loss: 0.00123 | Test Loss: 0.00123 | Delta: 0.00129\n",
      "Epoch: 600 | Loss: 0.00116 | Test Loss: 0.00117 | Delta: 0.00122\n",
      "Epoch: 800 | Loss: 0.00112 | Test Loss: 0.00113 | Delta: 0.00117\n",
      "Epoch: 1000 | Loss: 0.00110 | Test Loss: 0.00119 | Delta: 0.00120\n",
      "Epoch: 1200 | Loss: 0.00115 | Test Loss: 0.00156 | Delta: 0.00156\n",
      "Epoch: 1400 | Loss: 0.00103 | Test Loss: 0.00104 | Delta: 0.00106\n",
      "Epoch: 1600 | Loss: 0.00100 | Test Loss: 0.00102 | Delta: 0.00104\n",
      "Epoch: 1800 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00102\n",
      "Epoch: 2000 | Loss: 0.00097 | Test Loss: 0.00101 | Delta: 0.00101\n",
      "Epoch: 2200 | Loss: 0.00095 | Test Loss: 0.00097 | Delta: 0.00098\n",
      "Epoch: 2400 | Loss: 0.00095 | Test Loss: 0.00096 | Delta: 0.00098\n",
      "Epoch: 2600 | Loss: 0.00091 | Test Loss: 0.00093 | Delta: 0.00094\n",
      "Epoch: 2800 | Loss: 0.00091 | Test Loss: 0.00093 | Delta: 0.00094\n",
      "Epoch: 3000 | Loss: 0.00088 | Test Loss: 0.00091 | Delta: 0.00091\n",
      "Epoch: 3200 | Loss: 0.00088 | Test Loss: 0.00090 | Delta: 0.00091\n",
      "Epoch: 3400 | Loss: 0.00086 | Test Loss: 0.00088 | Delta: 0.00089\n",
      "Epoch: 3600 | Loss: 0.00086 | Test Loss: 0.00088 | Delta: 0.00089\n",
      "Epoch: 3800 | Loss: 0.00084 | Test Loss: 0.00086 | Delta: 0.00087\n",
      "Epoch: 4000 | Loss: 0.00084 | Test Loss: 0.00086 | Delta: 0.00087\n",
      "Epoch: 4200 | Loss: 0.00095 | Test Loss: 0.00100 | Delta: 0.00103\n",
      "Epoch: 4400 | Loss: 0.00082 | Test Loss: 0.00084 | Delta: 0.00085\n",
      "Epoch: 4600 | Loss: 0.00083 | Test Loss: 0.00085 | Delta: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00080 | Test Loss: 0.00083 | Delta: 0.00083\n",
      "Early stopping at epoch: 4928\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 2.24708 | Test Loss: 1.87404 | Delta: 1.86974\n",
      "Epoch: 200 | Loss: 0.00212 | Test Loss: 0.00209 | Delta: 0.00218\n",
      "Epoch: 400 | Loss: 0.00140 | Test Loss: 0.00142 | Delta: 0.00145\n",
      "Epoch: 600 | Loss: 0.00129 | Test Loss: 0.00131 | Delta: 0.00132\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00125 | Delta: 0.00125\n",
      "Epoch: 1000 | Loss: 0.00119 | Test Loss: 0.00121 | Delta: 0.00121\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00119 | Delta: 0.00119\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00116 | Delta: 0.00117\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00114 | Delta: 0.00115\n",
      "Epoch: 1800 | Loss: 0.00111 | Test Loss: 0.00112 | Delta: 0.00113\n",
      "Epoch: 2000 | Loss: 0.00109 | Test Loss: 0.00110 | Delta: 0.00111\n",
      "Epoch: 2200 | Loss: 0.00107 | Test Loss: 0.00108 | Delta: 0.00109\n",
      "Epoch: 2400 | Loss: 0.00105 | Test Loss: 0.00107 | Delta: 0.00107\n",
      "Epoch: 2600 | Loss: 0.00103 | Test Loss: 0.00105 | Delta: 0.00105\n",
      "Epoch: 2800 | Loss: 0.00101 | Test Loss: 0.00103 | Delta: 0.00104\n",
      "Epoch: 3000 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00102\n",
      "Epoch: 3200 | Loss: 0.00097 | Test Loss: 0.00099 | Delta: 0.00101\n",
      "Epoch: 3400 | Loss: 0.00094 | Test Loss: 0.00096 | Delta: 0.00098\n",
      "Epoch: 3600 | Loss: 0.00092 | Test Loss: 0.00094 | Delta: 0.00095\n",
      "Epoch: 3800 | Loss: 0.00104 | Test Loss: 0.00107 | Delta: 0.00110\n",
      "Epoch: 4000 | Loss: 0.00089 | Test Loss: 0.00091 | Delta: 0.00092\n",
      "Epoch: 4200 | Loss: 0.00087 | Test Loss: 0.00088 | Delta: 0.00090\n",
      "Epoch: 4400 | Loss: 0.00085 | Test Loss: 0.00087 | Delta: 0.00089\n",
      "Epoch: 4600 | Loss: 0.00084 | Test Loss: 0.00085 | Delta: 0.00087\n",
      "Epoch: 4800 | Loss: 0.00083 | Test Loss: 0.00087 | Delta: 0.00089\n",
      "Early stopping at epoch: 4977\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 2.46974 | Test Loss: 2.02770 | Delta: 2.02953\n",
      "Epoch: 200 | Loss: 0.00212 | Test Loss: 0.00213 | Delta: 0.00218\n",
      "Epoch: 400 | Loss: 0.00150 | Test Loss: 0.00152 | Delta: 0.00159\n",
      "Epoch: 600 | Loss: 0.00133 | Test Loss: 0.00136 | Delta: 0.00142\n",
      "Epoch: 800 | Loss: 0.00126 | Test Loss: 0.00128 | Delta: 0.00133\n",
      "Epoch: 1000 | Loss: 0.00121 | Test Loss: 0.00124 | Delta: 0.00129\n",
      "Epoch: 1200 | Loss: 0.00118 | Test Loss: 0.00121 | Delta: 0.00126\n",
      "Epoch: 1400 | Loss: 0.00116 | Test Loss: 0.00119 | Delta: 0.00123\n",
      "Epoch: 1600 | Loss: 0.00115 | Test Loss: 0.00117 | Delta: 0.00122\n",
      "Epoch: 1800 | Loss: 0.00114 | Test Loss: 0.00116 | Delta: 0.00120\n",
      "Epoch: 2000 | Loss: 0.00112 | Test Loss: 0.00115 | Delta: 0.00119\n",
      "Epoch: 2200 | Loss: 0.00111 | Test Loss: 0.00113 | Delta: 0.00118\n",
      "Epoch: 2400 | Loss: 0.00110 | Test Loss: 0.00112 | Delta: 0.00116\n",
      "Epoch: 2600 | Loss: 0.00109 | Test Loss: 0.00111 | Delta: 0.00115\n",
      "Epoch: 2800 | Loss: 0.00107 | Test Loss: 0.00110 | Delta: 0.00113\n",
      "Epoch: 3000 | Loss: 0.00106 | Test Loss: 0.00108 | Delta: 0.00112\n",
      "Epoch: 3200 | Loss: 0.00105 | Test Loss: 0.00107 | Delta: 0.00110\n",
      "Epoch: 3400 | Loss: 0.00103 | Test Loss: 0.00105 | Delta: 0.00108\n",
      "Epoch: 3600 | Loss: 0.00101 | Test Loss: 0.00103 | Delta: 0.00106\n",
      "Epoch: 3800 | Loss: 0.00099 | Test Loss: 0.00101 | Delta: 0.00104\n",
      "Epoch: 4000 | Loss: 0.00097 | Test Loss: 0.00099 | Delta: 0.00102\n",
      "Epoch: 4200 | Loss: 0.00095 | Test Loss: 0.00098 | Delta: 0.00101\n",
      "Epoch: 4400 | Loss: 0.00093 | Test Loss: 0.00095 | Delta: 0.00097\n",
      "Epoch: 4600 | Loss: 0.00091 | Test Loss: 0.00093 | Delta: 0.00095\n",
      "Epoch: 4800 | Loss: 0.00090 | Test Loss: 0.00091 | Delta: 0.00093\n",
      "Epoch: 5000 | Loss: 0.00089 | Test Loss: 0.00091 | Delta: 0.00092\n",
      "Epoch: 5200 | Loss: 0.00086 | Test Loss: 0.00087 | Delta: 0.00089\n",
      "Epoch: 5400 | Loss: 0.00085 | Test Loss: 0.00086 | Delta: 0.00088\n",
      "Epoch: 5600 | Loss: 0.00084 | Test Loss: 0.00086 | Delta: 0.00087\n",
      "Epoch: 5800 | Loss: 0.00109 | Test Loss: 0.00089 | Delta: 0.00090\n",
      "Epoch: 6000 | Loss: 0.00081 | Test Loss: 0.00083 | Delta: 0.00084\n",
      "Epoch: 6200 | Loss: 0.00081 | Test Loss: 0.00082 | Delta: 0.00083\n",
      "Epoch: 6400 | Loss: 0.00086 | Test Loss: 0.00096 | Delta: 0.00098\n",
      "Epoch: 6600 | Loss: 0.00079 | Test Loss: 0.00080 | Delta: 0.00081\n",
      "Epoch: 6800 | Loss: 0.00081 | Test Loss: 0.00081 | Delta: 0.00082\n",
      "Epoch: 7000 | Loss: 0.00077 | Test Loss: 0.00079 | Delta: 0.00079\n",
      "Epoch: 7200 | Loss: 0.00078 | Test Loss: 0.00080 | Delta: 0.00080\n",
      "Epoch: 7400 | Loss: 0.00076 | Test Loss: 0.00078 | Delta: 0.00078\n",
      "Early stopping at epoch: 7570\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 2.20472 | Test Loss: 1.82087 | Delta: 1.81069\n",
      "Epoch: 200 | Loss: 0.00264 | Test Loss: 0.00266 | Delta: 0.00266\n",
      "Epoch: 400 | Loss: 0.00160 | Test Loss: 0.00165 | Delta: 0.00163\n",
      "Epoch: 600 | Loss: 0.00138 | Test Loss: 0.00142 | Delta: 0.00141\n",
      "Epoch: 800 | Loss: 0.00130 | Test Loss: 0.00132 | Delta: 0.00132\n",
      "Epoch: 1000 | Loss: 0.00125 | Test Loss: 0.00127 | Delta: 0.00127\n",
      "Epoch: 1200 | Loss: 0.00122 | Test Loss: 0.00123 | Delta: 0.00124\n",
      "Epoch: 1400 | Loss: 0.00119 | Test Loss: 0.00120 | Delta: 0.00121\n",
      "Epoch: 1600 | Loss: 0.00117 | Test Loss: 0.00118 | Delta: 0.00119\n",
      "Epoch: 1800 | Loss: 0.00115 | Test Loss: 0.00116 | Delta: 0.00117\n",
      "Epoch: 2000 | Loss: 0.00113 | Test Loss: 0.00114 | Delta: 0.00115\n",
      "Epoch: 2200 | Loss: 0.00111 | Test Loss: 0.00112 | Delta: 0.00113\n",
      "Epoch: 2400 | Loss: 0.00109 | Test Loss: 0.00110 | Delta: 0.00111\n",
      "Epoch: 2600 | Loss: 0.00107 | Test Loss: 0.00108 | Delta: 0.00109\n",
      "Epoch: 2800 | Loss: 0.00105 | Test Loss: 0.00106 | Delta: 0.00107\n",
      "Epoch: 3000 | Loss: 0.00103 | Test Loss: 0.00104 | Delta: 0.00105\n",
      "Epoch: 3200 | Loss: 0.00100 | Test Loss: 0.00101 | Delta: 0.00102\n",
      "Epoch: 3400 | Loss: 0.00098 | Test Loss: 0.00099 | Delta: 0.00100\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00097 | Delta: 0.00098\n",
      "Epoch: 3800 | Loss: 0.00094 | Test Loss: 0.00095 | Delta: 0.00095\n",
      "Epoch: 4000 | Loss: 0.00092 | Test Loss: 0.00092 | Delta: 0.00093\n",
      "Epoch: 4200 | Loss: 0.00093 | Test Loss: 0.00092 | Delta: 0.00093\n",
      "Epoch: 4400 | Loss: 0.00090 | Test Loss: 0.00099 | Delta: 0.00098\n",
      "Epoch: 4600 | Loss: 0.00086 | Test Loss: 0.00087 | Delta: 0.00088\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00086 | Delta: 0.00086\n",
      "Epoch: 5000 | Loss: 0.00084 | Test Loss: 0.00085 | Delta: 0.00085\n",
      "Epoch: 5200 | Loss: 0.00086 | Test Loss: 0.00092 | Delta: 0.00092\n",
      "Epoch: 5400 | Loss: 0.00081 | Test Loss: 0.00082 | Delta: 0.00082\n",
      "Epoch: 5600 | Loss: 0.00081 | Test Loss: 0.00081 | Delta: 0.00082\n",
      "Epoch: 5800 | Loss: 0.00153 | Test Loss: 0.00088 | Delta: 0.00089\n",
      "Epoch: 6000 | Loss: 0.00079 | Test Loss: 0.00079 | Delta: 0.00080\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00081 | Delta: 0.00081\n",
      "Early stopping at epoch: 6232\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 2.77198 | Test Loss: 2.32706 | Delta: 2.32045\n",
      "Epoch: 200 | Loss: 0.00292 | Test Loss: 0.00289 | Delta: 0.00295\n",
      "Epoch: 400 | Loss: 0.00159 | Test Loss: 0.00158 | Delta: 0.00158\n",
      "Epoch: 600 | Loss: 0.00136 | Test Loss: 0.00136 | Delta: 0.00136\n",
      "Epoch: 800 | Loss: 0.00128 | Test Loss: 0.00128 | Delta: 0.00128\n",
      "Epoch: 1000 | Loss: 0.00124 | Test Loss: 0.00124 | Delta: 0.00124\n",
      "Epoch: 1200 | Loss: 0.00121 | Test Loss: 0.00121 | Delta: 0.00121\n",
      "Epoch: 1400 | Loss: 0.00118 | Test Loss: 0.00119 | Delta: 0.00119\n",
      "Epoch: 1600 | Loss: 0.00116 | Test Loss: 0.00117 | Delta: 0.00117\n",
      "Epoch: 1800 | Loss: 0.00115 | Test Loss: 0.00116 | Delta: 0.00115\n",
      "Epoch: 2000 | Loss: 0.00113 | Test Loss: 0.00114 | Delta: 0.00113\n",
      "Epoch: 2200 | Loss: 0.00112 | Test Loss: 0.00113 | Delta: 0.00112\n",
      "Epoch: 2400 | Loss: 0.00111 | Test Loss: 0.00111 | Delta: 0.00111\n",
      "Epoch: 2600 | Loss: 0.00109 | Test Loss: 0.00110 | Delta: 0.00110\n",
      "Epoch: 2800 | Loss: 0.00108 | Test Loss: 0.00109 | Delta: 0.00108\n",
      "Epoch: 3000 | Loss: 0.00107 | Test Loss: 0.00107 | Delta: 0.00107\n",
      "Epoch: 3200 | Loss: 0.00105 | Test Loss: 0.00106 | Delta: 0.00105\n",
      "Epoch: 3400 | Loss: 0.00103 | Test Loss: 0.00104 | Delta: 0.00104\n",
      "Epoch: 3600 | Loss: 0.00102 | Test Loss: 0.00102 | Delta: 0.00102\n",
      "Epoch: 3800 | Loss: 0.00100 | Test Loss: 0.00099 | Delta: 0.00100\n",
      "Epoch: 4000 | Loss: 0.00099 | Test Loss: 0.00098 | Delta: 0.00100\n",
      "Epoch: 4200 | Loss: 0.00096 | Test Loss: 0.00095 | Delta: 0.00096\n",
      "Epoch: 4400 | Loss: 0.00094 | Test Loss: 0.00094 | Delta: 0.00094\n",
      "Epoch: 4600 | Loss: 0.00093 | Test Loss: 0.00092 | Delta: 0.00093\n",
      "Epoch: 4800 | Loss: 0.00092 | Test Loss: 0.00090 | Delta: 0.00092\n",
      "Epoch: 5000 | Loss: 0.00090 | Test Loss: 0.00089 | Delta: 0.00090\n",
      "Epoch: 5200 | Loss: 0.00089 | Test Loss: 0.00088 | Delta: 0.00089\n",
      "Early stopping at epoch: 5226\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.16294 | Test Loss: 1.74539 | Delta: 1.74925\n",
      "Epoch: 200 | Loss: 0.00299 | Test Loss: 0.00301 | Delta: 0.00291\n",
      "Epoch: 400 | Loss: 0.00174 | Test Loss: 0.00169 | Delta: 0.00169\n",
      "Epoch: 600 | Loss: 0.00153 | Test Loss: 0.00149 | Delta: 0.00149\n",
      "Epoch: 800 | Loss: 0.00144 | Test Loss: 0.00142 | Delta: 0.00141\n",
      "Epoch: 1000 | Loss: 0.00140 | Test Loss: 0.00138 | Delta: 0.00136\n",
      "Epoch: 1200 | Loss: 0.00137 | Test Loss: 0.00135 | Delta: 0.00133\n",
      "Epoch: 1400 | Loss: 0.00134 | Test Loss: 0.00133 | Delta: 0.00131\n",
      "Epoch: 1600 | Loss: 0.00132 | Test Loss: 0.00132 | Delta: 0.00128\n",
      "Epoch: 1800 | Loss: 0.00131 | Test Loss: 0.00131 | Delta: 0.00127\n",
      "Epoch: 2000 | Loss: 0.00129 | Test Loss: 0.00129 | Delta: 0.00125\n",
      "Epoch: 2200 | Loss: 0.00128 | Test Loss: 0.00128 | Delta: 0.00123\n",
      "Epoch: 2400 | Loss: 0.00126 | Test Loss: 0.00126 | Delta: 0.00122\n",
      "Epoch: 2600 | Loss: 0.00125 | Test Loss: 0.00125 | Delta: 0.00120\n",
      "Epoch: 2800 | Loss: 0.00123 | Test Loss: 0.00123 | Delta: 0.00119\n",
      "Epoch: 3000 | Loss: 0.00122 | Test Loss: 0.00121 | Delta: 0.00117\n",
      "Epoch: 3200 | Loss: 0.00120 | Test Loss: 0.00121 | Delta: 0.00116\n",
      "Epoch: 3400 | Loss: 0.00129 | Test Loss: 0.00127 | Delta: 0.00126\n",
      "Epoch: 3600 | Loss: 0.00118 | Test Loss: 0.00118 | Delta: 0.00113\n",
      "Epoch: 3800 | Loss: 0.00116 | Test Loss: 0.00117 | Delta: 0.00112\n",
      "Epoch: 4000 | Loss: 0.00116 | Test Loss: 0.00117 | Delta: 0.00112\n",
      "Epoch: 4200 | Loss: 0.00114 | Test Loss: 0.00116 | Delta: 0.00110\n",
      "Epoch: 4400 | Loss: 0.00113 | Test Loss: 0.00115 | Delta: 0.00110\n",
      "Epoch: 4600 | Loss: 0.00113 | Test Loss: 0.00115 | Delta: 0.00111\n",
      "Epoch: 4800 | Loss: 0.00112 | Test Loss: 0.00114 | Delta: 0.00108\n",
      "Epoch: 5000 | Loss: 0.00111 | Test Loss: 0.00113 | Delta: 0.00108\n",
      "Epoch: 5200 | Loss: 0.00110 | Test Loss: 0.00112 | Delta: 0.00107\n",
      "Early stopping at epoch: 5373\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 0.51454 | Test Loss: 0.39028 | Delta: 0.38159\n",
      "Epoch: 200 | Loss: 0.00274 | Test Loss: 0.00276 | Delta: 0.00271\n",
      "Epoch: 400 | Loss: 0.00232 | Test Loss: 0.00235 | Delta: 0.00231\n",
      "Epoch: 600 | Loss: 0.00204 | Test Loss: 0.00206 | Delta: 0.00204\n",
      "Epoch: 800 | Loss: 0.00180 | Test Loss: 0.00182 | Delta: 0.00180\n",
      "Epoch: 1000 | Loss: 0.00164 | Test Loss: 0.00166 | Delta: 0.00164\n",
      "Epoch: 1200 | Loss: 0.00155 | Test Loss: 0.00157 | Delta: 0.00155\n",
      "Epoch: 1400 | Loss: 0.00150 | Test Loss: 0.00156 | Delta: 0.00159\n",
      "Epoch: 1600 | Loss: 0.00146 | Test Loss: 0.00154 | Delta: 0.00150\n",
      "Epoch: 1800 | Loss: 0.00144 | Test Loss: 0.00144 | Delta: 0.00146\n",
      "Epoch: 2000 | Loss: 0.00257 | Test Loss: 0.00181 | Delta: 0.00173\n",
      "Epoch: 2200 | Loss: 0.00133 | Test Loss: 0.00134 | Delta: 0.00133\n",
      "Epoch: 2400 | Loss: 0.00131 | Test Loss: 0.00131 | Delta: 0.00131\n",
      "Epoch: 2600 | Loss: 0.00129 | Test Loss: 0.00129 | Delta: 0.00129\n",
      "Epoch: 2800 | Loss: 0.00129 | Test Loss: 0.00129 | Delta: 0.00129\n",
      "Epoch: 3000 | Loss: 0.00125 | Test Loss: 0.00126 | Delta: 0.00125\n",
      "Epoch: 3200 | Loss: 0.00124 | Test Loss: 0.00124 | Delta: 0.00123\n",
      "Epoch: 3400 | Loss: 0.00123 | Test Loss: 0.00124 | Delta: 0.00123\n",
      "Epoch: 3600 | Loss: 0.00121 | Test Loss: 0.00121 | Delta: 0.00120\n",
      "Epoch: 3800 | Loss: 0.00120 | Test Loss: 0.00120 | Delta: 0.00119\n",
      "Epoch: 4000 | Loss: 0.00119 | Test Loss: 0.00120 | Delta: 0.00119\n",
      "Epoch: 4200 | Loss: 0.00117 | Test Loss: 0.00119 | Delta: 0.00116\n",
      "Early stopping at epoch: 4216\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 10000 # for sgd > 25000\n",
    "learning_rate = 0.003 # for sgd 0.003\n",
    "momentum = 0.9\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum,\n",
    "    #                            weight_decay=1e-5)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate,\n",
    "                                 weight_decay=1e-5)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience,\n",
    "                                      min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Delta: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f4bb583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "1686566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num = 35\n",
    "\n",
    "plt.plot(epochs_list[1:num], train_losses[1:num], label=\"Train loss\")\n",
    "plt.plot(epochs_list[1:num], test_losses[1:num], label=\"Test loss\")\n",
    "plt.plot(epochs_list[1:num], valid_losses[1:num], label=\"Valid_loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a72b5b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 6.8410 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_rel_error = statistics.mean(rel_errors)\n",
    "\n",
    "print(f\"Related error: {(G_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "21468941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.06615702788738048]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_rel_errors = []\n",
    "G_rel_errors.append(G_rel_error)\n",
    "\n",
    "G_rel_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "81a05e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.06615702788738048], 0.0)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_st_dev = statistics.pstdev(G_rel_errors)\n",
    "\n",
    "G_rel_errors, G_st_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88b6d1",
   "metadata": {},
   "source": [
    "# Initial - M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9f31806",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 31\n",
    "hidden_units = 31\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "M_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    M_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0cd9b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_M_train, y_train = X_M_train.to(device), y_train.to(device)\n",
    "X_M_test, y_test = X_M_test.to(device), y_test.to(device)\n",
    "X_M_valid, y_valid = X_M_valid.to(device), y_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b35abd45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.30028 | Test Loss: 1.15618 | Delta: 1.14039\n",
      "Epoch: 200 | Loss: 0.00773 | Test Loss: 0.00798 | Delta: 0.00768\n",
      "Epoch: 400 | Loss: 0.00540 | Test Loss: 0.00550 | Delta: 0.00540\n",
      "Epoch: 600 | Loss: 0.00508 | Test Loss: 0.00513 | Delta: 0.00509\n",
      "Epoch: 800 | Loss: 0.00487 | Test Loss: 0.00491 | Delta: 0.00489\n",
      "Epoch: 1000 | Loss: 0.00469 | Test Loss: 0.00472 | Delta: 0.00470\n",
      "Epoch: 1200 | Loss: 0.00451 | Test Loss: 0.00455 | Delta: 0.00452\n",
      "Epoch: 1400 | Loss: 0.00435 | Test Loss: 0.00438 | Delta: 0.00435\n",
      "Epoch: 1600 | Loss: 0.00418 | Test Loss: 0.00421 | Delta: 0.00418\n",
      "Epoch: 1800 | Loss: 0.00402 | Test Loss: 0.00404 | Delta: 0.00400\n",
      "Epoch: 2000 | Loss: 0.00385 | Test Loss: 0.00387 | Delta: 0.00383\n",
      "Epoch: 2200 | Loss: 0.00369 | Test Loss: 0.00370 | Delta: 0.00365\n",
      "Epoch: 2400 | Loss: 0.00352 | Test Loss: 0.00352 | Delta: 0.00348\n",
      "Epoch: 2600 | Loss: 0.00335 | Test Loss: 0.00335 | Delta: 0.00330\n",
      "Epoch: 2800 | Loss: 0.00318 | Test Loss: 0.00318 | Delta: 0.00312\n",
      "Epoch: 3000 | Loss: 0.00302 | Test Loss: 0.00302 | Delta: 0.00295\n",
      "Epoch: 3200 | Loss: 0.00286 | Test Loss: 0.00286 | Delta: 0.00279\n",
      "Epoch: 3400 | Loss: 0.00272 | Test Loss: 0.00272 | Delta: 0.00264\n",
      "Epoch: 3600 | Loss: 0.00258 | Test Loss: 0.00260 | Delta: 0.00251\n",
      "Epoch: 3800 | Loss: 0.00246 | Test Loss: 0.00248 | Delta: 0.00238\n",
      "Epoch: 4000 | Loss: 0.00235 | Test Loss: 0.00238 | Delta: 0.00227\n",
      "Epoch: 4200 | Loss: 0.00225 | Test Loss: 0.00229 | Delta: 0.00217\n",
      "Epoch: 4400 | Loss: 0.00216 | Test Loss: 0.00221 | Delta: 0.00208\n",
      "Epoch: 4600 | Loss: 0.00208 | Test Loss: 0.00213 | Delta: 0.00200\n",
      "Epoch: 4800 | Loss: 0.00201 | Test Loss: 0.00207 | Delta: 0.00193\n",
      "Epoch: 5000 | Loss: 0.00195 | Test Loss: 0.00201 | Delta: 0.00188\n",
      "Epoch: 5200 | Loss: 0.00191 | Test Loss: 0.00198 | Delta: 0.00183\n",
      "Epoch: 5400 | Loss: 0.00186 | Test Loss: 0.00192 | Delta: 0.00179\n",
      "Epoch: 5600 | Loss: 0.00182 | Test Loss: 0.00188 | Delta: 0.00176\n",
      "Epoch: 5800 | Loss: 0.00179 | Test Loss: 0.00185 | Delta: 0.00172\n",
      "Epoch: 6000 | Loss: 0.00180 | Test Loss: 0.00188 | Delta: 0.00173\n",
      "Epoch: 6200 | Loss: 0.00174 | Test Loss: 0.00179 | Delta: 0.00167\n",
      "Epoch: 6400 | Loss: 0.00171 | Test Loss: 0.00178 | Delta: 0.00165\n",
      "Early stopping at epoch: 6424\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.63041 | Test Loss: 1.47808 | Delta: 1.45647\n",
      "Epoch: 200 | Loss: 0.00843 | Test Loss: 0.00878 | Delta: 0.00805\n",
      "Epoch: 400 | Loss: 0.00588 | Test Loss: 0.00609 | Delta: 0.00566\n",
      "Epoch: 600 | Loss: 0.00543 | Test Loss: 0.00553 | Delta: 0.00524\n",
      "Epoch: 800 | Loss: 0.00520 | Test Loss: 0.00527 | Delta: 0.00503\n",
      "Epoch: 1000 | Loss: 0.00498 | Test Loss: 0.00503 | Delta: 0.00482\n",
      "Epoch: 1200 | Loss: 0.00475 | Test Loss: 0.00479 | Delta: 0.00461\n",
      "Epoch: 1400 | Loss: 0.00452 | Test Loss: 0.00456 | Delta: 0.00439\n",
      "Epoch: 1600 | Loss: 0.00430 | Test Loss: 0.00433 | Delta: 0.00419\n",
      "Epoch: 1800 | Loss: 0.00410 | Test Loss: 0.00412 | Delta: 0.00400\n",
      "Epoch: 2000 | Loss: 0.00391 | Test Loss: 0.00392 | Delta: 0.00382\n",
      "Epoch: 2200 | Loss: 0.00374 | Test Loss: 0.00375 | Delta: 0.00367\n",
      "Epoch: 2400 | Loss: 0.00358 | Test Loss: 0.00359 | Delta: 0.00352\n",
      "Epoch: 2600 | Loss: 0.00344 | Test Loss: 0.00345 | Delta: 0.00339\n",
      "Epoch: 2800 | Loss: 0.00330 | Test Loss: 0.00332 | Delta: 0.00326\n",
      "Epoch: 3000 | Loss: 0.00318 | Test Loss: 0.00320 | Delta: 0.00315\n",
      "Epoch: 3200 | Loss: 0.00306 | Test Loss: 0.00308 | Delta: 0.00303\n",
      "Epoch: 3400 | Loss: 0.00295 | Test Loss: 0.00298 | Delta: 0.00293\n",
      "Epoch: 3600 | Loss: 0.00285 | Test Loss: 0.00288 | Delta: 0.00283\n",
      "Epoch: 3800 | Loss: 0.00276 | Test Loss: 0.00279 | Delta: 0.00273\n",
      "Epoch: 4000 | Loss: 0.00267 | Test Loss: 0.00270 | Delta: 0.00264\n",
      "Epoch: 4200 | Loss: 0.00258 | Test Loss: 0.00261 | Delta: 0.00256\n",
      "Epoch: 4400 | Loss: 0.00250 | Test Loss: 0.00252 | Delta: 0.00247\n",
      "Epoch: 4600 | Loss: 0.00241 | Test Loss: 0.00244 | Delta: 0.00239\n",
      "Epoch: 4800 | Loss: 0.00234 | Test Loss: 0.00236 | Delta: 0.00232\n",
      "Epoch: 5000 | Loss: 0.00227 | Test Loss: 0.00230 | Delta: 0.00226\n",
      "Epoch: 5200 | Loss: 0.00221 | Test Loss: 0.00224 | Delta: 0.00219\n",
      "Epoch: 5400 | Loss: 0.00215 | Test Loss: 0.00218 | Delta: 0.00213\n",
      "Epoch: 5600 | Loss: 0.00210 | Test Loss: 0.00212 | Delta: 0.00208\n",
      "Epoch: 5800 | Loss: 0.00204 | Test Loss: 0.00206 | Delta: 0.00202\n",
      "Epoch: 6000 | Loss: 0.00198 | Test Loss: 0.00201 | Delta: 0.00196\n",
      "Epoch: 6200 | Loss: 0.00194 | Test Loss: 0.00197 | Delta: 0.00191\n",
      "Epoch: 6400 | Loss: 0.00190 | Test Loss: 0.00193 | Delta: 0.00187\n",
      "Epoch: 6600 | Loss: 0.00192 | Test Loss: 0.00193 | Delta: 0.00185\n",
      "Epoch: 6800 | Loss: 0.00180 | Test Loss: 0.00183 | Delta: 0.00178\n",
      "Epoch: 7000 | Loss: 0.00177 | Test Loss: 0.00179 | Delta: 0.00174\n",
      "Epoch: 7200 | Loss: 0.00173 | Test Loss: 0.00176 | Delta: 0.00171\n",
      "Epoch: 7400 | Loss: 0.00170 | Test Loss: 0.00173 | Delta: 0.00168\n",
      "Epoch: 7600 | Loss: 0.00167 | Test Loss: 0.00170 | Delta: 0.00165\n",
      "Epoch: 7800 | Loss: 0.00166 | Test Loss: 0.00170 | Delta: 0.00164\n",
      "Early stopping at epoch: 7816\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 2.54348 | Test Loss: 2.33553 | Delta: 2.31611\n",
      "Epoch: 200 | Loss: 0.00917 | Test Loss: 0.00914 | Delta: 0.00875\n",
      "Epoch: 400 | Loss: 0.00588 | Test Loss: 0.00586 | Delta: 0.00573\n",
      "Epoch: 600 | Loss: 0.00521 | Test Loss: 0.00511 | Delta: 0.00511\n",
      "Epoch: 800 | Loss: 0.00496 | Test Loss: 0.00482 | Delta: 0.00487\n",
      "Epoch: 1000 | Loss: 0.00475 | Test Loss: 0.00460 | Delta: 0.00467\n",
      "Epoch: 1200 | Loss: 0.00453 | Test Loss: 0.00439 | Delta: 0.00445\n",
      "Epoch: 1400 | Loss: 0.00429 | Test Loss: 0.00415 | Delta: 0.00422\n",
      "Epoch: 1600 | Loss: 0.00404 | Test Loss: 0.00391 | Delta: 0.00398\n",
      "Epoch: 1800 | Loss: 0.00379 | Test Loss: 0.00367 | Delta: 0.00374\n",
      "Epoch: 2000 | Loss: 0.00356 | Test Loss: 0.00344 | Delta: 0.00351\n",
      "Epoch: 2200 | Loss: 0.00334 | Test Loss: 0.00324 | Delta: 0.00331\n",
      "Epoch: 2400 | Loss: 0.00315 | Test Loss: 0.00306 | Delta: 0.00312\n",
      "Epoch: 2600 | Loss: 0.00298 | Test Loss: 0.00290 | Delta: 0.00295\n",
      "Epoch: 2800 | Loss: 0.00283 | Test Loss: 0.00276 | Delta: 0.00280\n",
      "Epoch: 3000 | Loss: 0.00269 | Test Loss: 0.00264 | Delta: 0.00267\n",
      "Epoch: 3200 | Loss: 0.00256 | Test Loss: 0.00253 | Delta: 0.00254\n",
      "Epoch: 3400 | Loss: 0.00245 | Test Loss: 0.00243 | Delta: 0.00243\n",
      "Epoch: 3600 | Loss: 0.00236 | Test Loss: 0.00234 | Delta: 0.00233\n",
      "Epoch: 3800 | Loss: 0.00226 | Test Loss: 0.00226 | Delta: 0.00224\n",
      "Epoch: 4000 | Loss: 0.00219 | Test Loss: 0.00220 | Delta: 0.00216\n",
      "Epoch: 4200 | Loss: 0.00213 | Test Loss: 0.00215 | Delta: 0.00210\n",
      "Epoch: 4400 | Loss: 0.00208 | Test Loss: 0.00211 | Delta: 0.00206\n",
      "Epoch: 4600 | Loss: 0.00205 | Test Loss: 0.00208 | Delta: 0.00202\n",
      "Epoch: 4800 | Loss: 0.00201 | Test Loss: 0.00204 | Delta: 0.00199\n",
      "Epoch: 5000 | Loss: 0.00198 | Test Loss: 0.00201 | Delta: 0.00196\n",
      "Epoch: 5200 | Loss: 0.00195 | Test Loss: 0.00199 | Delta: 0.00192\n",
      "Epoch: 5400 | Loss: 0.00192 | Test Loss: 0.00196 | Delta: 0.00189\n",
      "Epoch: 5600 | Loss: 0.00189 | Test Loss: 0.00193 | Delta: 0.00186\n",
      "Epoch: 5800 | Loss: 0.00186 | Test Loss: 0.00189 | Delta: 0.00183\n",
      "Epoch: 6000 | Loss: 0.00182 | Test Loss: 0.00186 | Delta: 0.00180\n",
      "Epoch: 6200 | Loss: 0.00179 | Test Loss: 0.00183 | Delta: 0.00176\n",
      "Epoch: 6400 | Loss: 0.00176 | Test Loss: 0.00179 | Delta: 0.00173\n",
      "Epoch: 6600 | Loss: 0.00173 | Test Loss: 0.00176 | Delta: 0.00170\n",
      "Epoch: 6800 | Loss: 0.00169 | Test Loss: 0.00173 | Delta: 0.00167\n",
      "Epoch: 7000 | Loss: 0.00166 | Test Loss: 0.00170 | Delta: 0.00164\n",
      "Epoch: 7200 | Loss: 0.00163 | Test Loss: 0.00167 | Delta: 0.00161\n",
      "Epoch: 7400 | Loss: 0.00160 | Test Loss: 0.00163 | Delta: 0.00158\n",
      "Epoch: 7600 | Loss: 0.00158 | Test Loss: 0.00162 | Delta: 0.00156\n",
      "Epoch: 7800 | Loss: 0.00157 | Test Loss: 0.00159 | Delta: 0.00155\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 2.89456 | Test Loss: 2.69773 | Delta: 2.68746\n",
      "Epoch: 200 | Loss: 0.00855 | Test Loss: 0.00860 | Delta: 0.00852\n",
      "Epoch: 400 | Loss: 0.00560 | Test Loss: 0.00563 | Delta: 0.00573\n",
      "Epoch: 600 | Loss: 0.00507 | Test Loss: 0.00505 | Delta: 0.00521\n",
      "Epoch: 800 | Loss: 0.00488 | Test Loss: 0.00484 | Delta: 0.00502\n",
      "Epoch: 1000 | Loss: 0.00476 | Test Loss: 0.00470 | Delta: 0.00489\n",
      "Epoch: 1200 | Loss: 0.00463 | Test Loss: 0.00458 | Delta: 0.00476\n",
      "Epoch: 1400 | Loss: 0.00450 | Test Loss: 0.00444 | Delta: 0.00461\n",
      "Epoch: 1600 | Loss: 0.00434 | Test Loss: 0.00429 | Delta: 0.00445\n",
      "Epoch: 1800 | Loss: 0.00418 | Test Loss: 0.00412 | Delta: 0.00428\n",
      "Epoch: 2000 | Loss: 0.00400 | Test Loss: 0.00394 | Delta: 0.00409\n",
      "Epoch: 2200 | Loss: 0.00380 | Test Loss: 0.00375 | Delta: 0.00389\n",
      "Epoch: 2400 | Loss: 0.00360 | Test Loss: 0.00355 | Delta: 0.00368\n",
      "Epoch: 2600 | Loss: 0.00339 | Test Loss: 0.00334 | Delta: 0.00346\n",
      "Epoch: 2800 | Loss: 0.00317 | Test Loss: 0.00313 | Delta: 0.00323\n",
      "Epoch: 3000 | Loss: 0.00296 | Test Loss: 0.00292 | Delta: 0.00301\n",
      "Epoch: 3200 | Loss: 0.00276 | Test Loss: 0.00272 | Delta: 0.00280\n",
      "Epoch: 3400 | Loss: 0.00256 | Test Loss: 0.00254 | Delta: 0.00260\n",
      "Epoch: 3600 | Loss: 0.00239 | Test Loss: 0.00237 | Delta: 0.00242\n",
      "Epoch: 3800 | Loss: 0.00224 | Test Loss: 0.00222 | Delta: 0.00226\n",
      "Epoch: 4000 | Loss: 0.00211 | Test Loss: 0.00209 | Delta: 0.00213\n",
      "Epoch: 4200 | Loss: 0.00201 | Test Loss: 0.00199 | Delta: 0.00203\n",
      "Epoch: 4400 | Loss: 0.00193 | Test Loss: 0.00191 | Delta: 0.00195\n",
      "Epoch: 4600 | Loss: 0.00186 | Test Loss: 0.00185 | Delta: 0.00188\n",
      "Epoch: 4800 | Loss: 0.00181 | Test Loss: 0.00180 | Delta: 0.00183\n",
      "Epoch: 5000 | Loss: 0.00177 | Test Loss: 0.00176 | Delta: 0.00179\n",
      "Epoch: 5200 | Loss: 0.00174 | Test Loss: 0.00173 | Delta: 0.00176\n",
      "Epoch: 5400 | Loss: 0.00171 | Test Loss: 0.00170 | Delta: 0.00173\n",
      "Epoch: 5600 | Loss: 0.00168 | Test Loss: 0.00168 | Delta: 0.00170\n",
      "Epoch: 5800 | Loss: 0.00166 | Test Loss: 0.00165 | Delta: 0.00167\n",
      "Epoch: 6000 | Loss: 0.00163 | Test Loss: 0.00163 | Delta: 0.00165\n",
      "Epoch: 6200 | Loss: 0.00161 | Test Loss: 0.00161 | Delta: 0.00162\n",
      "Epoch: 6400 | Loss: 0.00159 | Test Loss: 0.00159 | Delta: 0.00160\n",
      "Epoch: 6600 | Loss: 0.00156 | Test Loss: 0.00156 | Delta: 0.00157\n",
      "Epoch: 6800 | Loss: 0.00154 | Test Loss: 0.00154 | Delta: 0.00155\n",
      "Epoch: 7000 | Loss: 0.00152 | Test Loss: 0.00152 | Delta: 0.00153\n",
      "Epoch: 7200 | Loss: 0.00150 | Test Loss: 0.00150 | Delta: 0.00150\n",
      "Epoch: 7400 | Loss: 0.00148 | Test Loss: 0.00147 | Delta: 0.00148\n",
      "Epoch: 7600 | Loss: 0.00146 | Test Loss: 0.00145 | Delta: 0.00146\n",
      "Epoch: 7800 | Loss: 0.00144 | Test Loss: 0.00143 | Delta: 0.00144\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 3.31697 | Test Loss: 3.09206 | Delta: 3.09586\n",
      "Epoch: 200 | Loss: 0.00864 | Test Loss: 0.00873 | Delta: 0.00874\n",
      "Epoch: 400 | Loss: 0.00531 | Test Loss: 0.00531 | Delta: 0.00545\n",
      "Epoch: 600 | Loss: 0.00478 | Test Loss: 0.00480 | Delta: 0.00498\n",
      "Epoch: 800 | Loss: 0.00462 | Test Loss: 0.00463 | Delta: 0.00482\n",
      "Epoch: 1000 | Loss: 0.00450 | Test Loss: 0.00451 | Delta: 0.00470\n",
      "Epoch: 1200 | Loss: 0.00437 | Test Loss: 0.00438 | Delta: 0.00457\n",
      "Epoch: 1400 | Loss: 0.00423 | Test Loss: 0.00424 | Delta: 0.00443\n",
      "Epoch: 1600 | Loss: 0.00408 | Test Loss: 0.00409 | Delta: 0.00427\n",
      "Epoch: 1800 | Loss: 0.00391 | Test Loss: 0.00393 | Delta: 0.00410\n",
      "Epoch: 2000 | Loss: 0.00373 | Test Loss: 0.00375 | Delta: 0.00391\n",
      "Epoch: 2200 | Loss: 0.00354 | Test Loss: 0.00356 | Delta: 0.00372\n",
      "Epoch: 2400 | Loss: 0.00335 | Test Loss: 0.00336 | Delta: 0.00351\n",
      "Epoch: 2600 | Loss: 0.00316 | Test Loss: 0.00317 | Delta: 0.00331\n",
      "Epoch: 2800 | Loss: 0.00297 | Test Loss: 0.00298 | Delta: 0.00311\n",
      "Epoch: 3000 | Loss: 0.00278 | Test Loss: 0.00279 | Delta: 0.00291\n",
      "Epoch: 3200 | Loss: 0.00260 | Test Loss: 0.00260 | Delta: 0.00271\n",
      "Epoch: 3400 | Loss: 0.00242 | Test Loss: 0.00241 | Delta: 0.00252\n",
      "Epoch: 3600 | Loss: 0.00225 | Test Loss: 0.00224 | Delta: 0.00234\n",
      "Epoch: 3800 | Loss: 0.00210 | Test Loss: 0.00208 | Delta: 0.00218\n",
      "Epoch: 4000 | Loss: 0.00197 | Test Loss: 0.00195 | Delta: 0.00204\n",
      "Epoch: 4200 | Loss: 0.00185 | Test Loss: 0.00183 | Delta: 0.00192\n",
      "Epoch: 4400 | Loss: 0.00176 | Test Loss: 0.00173 | Delta: 0.00181\n",
      "Epoch: 4600 | Loss: 0.00168 | Test Loss: 0.00165 | Delta: 0.00173\n",
      "Epoch: 4800 | Loss: 0.00162 | Test Loss: 0.00159 | Delta: 0.00167\n",
      "Epoch: 5000 | Loss: 0.00158 | Test Loss: 0.00154 | Delta: 0.00162\n",
      "Epoch: 5200 | Loss: 0.00154 | Test Loss: 0.00151 | Delta: 0.00158\n",
      "Epoch: 5400 | Loss: 0.00151 | Test Loss: 0.00148 | Delta: 0.00154\n",
      "Epoch: 5600 | Loss: 0.00148 | Test Loss: 0.00145 | Delta: 0.00151\n",
      "Epoch: 5800 | Loss: 0.00145 | Test Loss: 0.00142 | Delta: 0.00149\n",
      "Epoch: 6000 | Loss: 0.00142 | Test Loss: 0.00139 | Delta: 0.00146\n",
      "Epoch: 6200 | Loss: 0.00140 | Test Loss: 0.00137 | Delta: 0.00144\n",
      "Epoch: 6400 | Loss: 0.00138 | Test Loss: 0.00135 | Delta: 0.00142\n",
      "Epoch: 6600 | Loss: 0.00136 | Test Loss: 0.00133 | Delta: 0.00140\n",
      "Epoch: 6800 | Loss: 0.00134 | Test Loss: 0.00131 | Delta: 0.00138\n",
      "Epoch: 7000 | Loss: 0.00132 | Test Loss: 0.00130 | Delta: 0.00136\n",
      "Epoch: 7200 | Loss: 0.00131 | Test Loss: 0.00128 | Delta: 0.00135\n",
      "Epoch: 7400 | Loss: 0.00129 | Test Loss: 0.00126 | Delta: 0.00133\n",
      "Epoch: 7600 | Loss: 0.00127 | Test Loss: 0.00125 | Delta: 0.00131\n",
      "Epoch: 7800 | Loss: 0.00126 | Test Loss: 0.00124 | Delta: 0.00129\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 0.42548 | Test Loss: 0.34458 | Delta: 0.34533\n",
      "Epoch: 200 | Loss: 0.00535 | Test Loss: 0.00511 | Delta: 0.00533\n",
      "Epoch: 400 | Loss: 0.00501 | Test Loss: 0.00478 | Delta: 0.00501\n",
      "Epoch: 600 | Loss: 0.00468 | Test Loss: 0.00447 | Delta: 0.00469\n",
      "Epoch: 800 | Loss: 0.00436 | Test Loss: 0.00418 | Delta: 0.00437\n",
      "Epoch: 1000 | Loss: 0.00405 | Test Loss: 0.00389 | Delta: 0.00407\n",
      "Epoch: 1200 | Loss: 0.00375 | Test Loss: 0.00361 | Delta: 0.00377\n",
      "Epoch: 1400 | Loss: 0.00344 | Test Loss: 0.00333 | Delta: 0.00348\n",
      "Epoch: 1600 | Loss: 0.00315 | Test Loss: 0.00306 | Delta: 0.00318\n",
      "Epoch: 1800 | Loss: 0.00286 | Test Loss: 0.00280 | Delta: 0.00290\n",
      "Epoch: 2000 | Loss: 0.00260 | Test Loss: 0.00257 | Delta: 0.00265\n",
      "Epoch: 2200 | Loss: 0.00239 | Test Loss: 0.00237 | Delta: 0.00243\n",
      "Epoch: 2400 | Loss: 0.00221 | Test Loss: 0.00220 | Delta: 0.00225\n",
      "Epoch: 2600 | Loss: 0.00205 | Test Loss: 0.00205 | Delta: 0.00209\n",
      "Epoch: 2800 | Loss: 0.00191 | Test Loss: 0.00192 | Delta: 0.00194\n",
      "Epoch: 3000 | Loss: 0.00179 | Test Loss: 0.00179 | Delta: 0.00181\n",
      "Epoch: 3200 | Loss: 0.00167 | Test Loss: 0.00167 | Delta: 0.00170\n",
      "Epoch: 3400 | Loss: 0.00157 | Test Loss: 0.00157 | Delta: 0.00159\n",
      "Epoch: 3600 | Loss: 0.00147 | Test Loss: 0.00147 | Delta: 0.00150\n",
      "Epoch: 3800 | Loss: 0.00140 | Test Loss: 0.00140 | Delta: 0.00142\n",
      "Epoch: 4000 | Loss: 0.00134 | Test Loss: 0.00134 | Delta: 0.00137\n",
      "Epoch: 4200 | Loss: 0.00130 | Test Loss: 0.00129 | Delta: 0.00132\n",
      "Epoch: 4400 | Loss: 0.00125 | Test Loss: 0.00126 | Delta: 0.00128\n",
      "Epoch: 4600 | Loss: 0.00123 | Test Loss: 0.00122 | Delta: 0.00125\n",
      "Epoch: 4800 | Loss: 0.00121 | Test Loss: 0.00120 | Delta: 0.00123\n",
      "Epoch: 5000 | Loss: 0.00119 | Test Loss: 0.00119 | Delta: 0.00121\n",
      "Epoch: 5200 | Loss: 0.00118 | Test Loss: 0.00118 | Delta: 0.00120\n",
      "Epoch: 5400 | Loss: 0.00127 | Test Loss: 0.00120 | Delta: 0.00121\n",
      "Epoch: 5600 | Loss: 0.00115 | Test Loss: 0.00114 | Delta: 0.00117\n",
      "Epoch: 5800 | Loss: 0.00114 | Test Loss: 0.00113 | Delta: 0.00116\n",
      "Early stopping at epoch: 5883\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 1.77450 | Test Loss: 1.62844 | Delta: 1.61867\n",
      "Epoch: 200 | Loss: 0.00535 | Test Loss: 0.00553 | Delta: 0.00523\n",
      "Epoch: 400 | Loss: 0.00454 | Test Loss: 0.00469 | Delta: 0.00443\n",
      "Epoch: 600 | Loss: 0.00441 | Test Loss: 0.00454 | Delta: 0.00430\n",
      "Epoch: 800 | Loss: 0.00430 | Test Loss: 0.00443 | Delta: 0.00419\n",
      "Epoch: 1000 | Loss: 0.00419 | Test Loss: 0.00432 | Delta: 0.00408\n",
      "Epoch: 1200 | Loss: 0.00406 | Test Loss: 0.00419 | Delta: 0.00396\n",
      "Epoch: 1400 | Loss: 0.00392 | Test Loss: 0.00405 | Delta: 0.00382\n",
      "Epoch: 1600 | Loss: 0.00376 | Test Loss: 0.00389 | Delta: 0.00367\n",
      "Epoch: 1800 | Loss: 0.00358 | Test Loss: 0.00372 | Delta: 0.00351\n",
      "Epoch: 2000 | Loss: 0.00340 | Test Loss: 0.00353 | Delta: 0.00333\n",
      "Epoch: 2200 | Loss: 0.00321 | Test Loss: 0.00334 | Delta: 0.00315\n",
      "Epoch: 2400 | Loss: 0.00302 | Test Loss: 0.00316 | Delta: 0.00297\n",
      "Epoch: 2600 | Loss: 0.00284 | Test Loss: 0.00297 | Delta: 0.00279\n",
      "Epoch: 2800 | Loss: 0.00266 | Test Loss: 0.00279 | Delta: 0.00262\n",
      "Epoch: 3000 | Loss: 0.00250 | Test Loss: 0.00262 | Delta: 0.00246\n",
      "Epoch: 3200 | Loss: 0.00235 | Test Loss: 0.00246 | Delta: 0.00231\n",
      "Epoch: 3400 | Loss: 0.00222 | Test Loss: 0.00231 | Delta: 0.00217\n",
      "Epoch: 3600 | Loss: 0.00209 | Test Loss: 0.00218 | Delta: 0.00205\n",
      "Epoch: 3800 | Loss: 0.00198 | Test Loss: 0.00206 | Delta: 0.00193\n",
      "Epoch: 4000 | Loss: 0.00186 | Test Loss: 0.00194 | Delta: 0.00182\n",
      "Epoch: 4200 | Loss: 0.00175 | Test Loss: 0.00182 | Delta: 0.00171\n",
      "Epoch: 4400 | Loss: 0.00164 | Test Loss: 0.00170 | Delta: 0.00160\n",
      "Epoch: 4600 | Loss: 0.00153 | Test Loss: 0.00158 | Delta: 0.00150\n",
      "Epoch: 4800 | Loss: 0.00143 | Test Loss: 0.00147 | Delta: 0.00140\n",
      "Epoch: 5000 | Loss: 0.00134 | Test Loss: 0.00138 | Delta: 0.00131\n",
      "Epoch: 5200 | Loss: 0.00126 | Test Loss: 0.00129 | Delta: 0.00124\n",
      "Epoch: 5400 | Loss: 0.00120 | Test Loss: 0.00123 | Delta: 0.00118\n",
      "Epoch: 5600 | Loss: 0.00115 | Test Loss: 0.00118 | Delta: 0.00114\n",
      "Epoch: 5800 | Loss: 0.00112 | Test Loss: 0.00115 | Delta: 0.00111\n",
      "Epoch: 6000 | Loss: 0.00110 | Test Loss: 0.00113 | Delta: 0.00109\n",
      "Epoch: 6200 | Loss: 0.00108 | Test Loss: 0.00111 | Delta: 0.00108\n",
      "Epoch: 6400 | Loss: 0.00107 | Test Loss: 0.00111 | Delta: 0.00107\n",
      "Epoch: 6600 | Loss: 0.00106 | Test Loss: 0.00108 | Delta: 0.00106\n",
      "Epoch: 6800 | Loss: 0.00105 | Test Loss: 0.00107 | Delta: 0.00105\n",
      "Epoch: 7000 | Loss: 0.00104 | Test Loss: 0.00107 | Delta: 0.00104\n",
      "Early stopping at epoch: 7196\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 1.30638 | Test Loss: 1.17301 | Delta: 1.16393\n",
      "Epoch: 200 | Loss: 0.00477 | Test Loss: 0.00486 | Delta: 0.00484\n",
      "Epoch: 400 | Loss: 0.00449 | Test Loss: 0.00457 | Delta: 0.00458\n",
      "Epoch: 600 | Loss: 0.00435 | Test Loss: 0.00444 | Delta: 0.00445\n",
      "Epoch: 800 | Loss: 0.00420 | Test Loss: 0.00428 | Delta: 0.00429\n",
      "Epoch: 1000 | Loss: 0.00404 | Test Loss: 0.00412 | Delta: 0.00412\n",
      "Epoch: 1200 | Loss: 0.00386 | Test Loss: 0.00394 | Delta: 0.00394\n",
      "Epoch: 1400 | Loss: 0.00367 | Test Loss: 0.00375 | Delta: 0.00375\n",
      "Epoch: 1600 | Loss: 0.00348 | Test Loss: 0.00355 | Delta: 0.00355\n",
      "Epoch: 1800 | Loss: 0.00328 | Test Loss: 0.00335 | Delta: 0.00335\n",
      "Epoch: 2000 | Loss: 0.00309 | Test Loss: 0.00315 | Delta: 0.00315\n",
      "Epoch: 2200 | Loss: 0.00289 | Test Loss: 0.00295 | Delta: 0.00295\n",
      "Epoch: 2400 | Loss: 0.00271 | Test Loss: 0.00277 | Delta: 0.00276\n",
      "Epoch: 2600 | Loss: 0.00254 | Test Loss: 0.00259 | Delta: 0.00259\n",
      "Epoch: 2800 | Loss: 0.00238 | Test Loss: 0.00243 | Delta: 0.00243\n",
      "Epoch: 3000 | Loss: 0.00223 | Test Loss: 0.00228 | Delta: 0.00227\n",
      "Epoch: 3200 | Loss: 0.00208 | Test Loss: 0.00213 | Delta: 0.00212\n",
      "Epoch: 3400 | Loss: 0.00194 | Test Loss: 0.00198 | Delta: 0.00198\n",
      "Epoch: 3600 | Loss: 0.00180 | Test Loss: 0.00185 | Delta: 0.00184\n",
      "Epoch: 3800 | Loss: 0.00166 | Test Loss: 0.00171 | Delta: 0.00169\n",
      "Epoch: 4000 | Loss: 0.00152 | Test Loss: 0.00157 | Delta: 0.00154\n",
      "Epoch: 4200 | Loss: 0.00138 | Test Loss: 0.00142 | Delta: 0.00140\n",
      "Epoch: 4400 | Loss: 0.00126 | Test Loss: 0.00130 | Delta: 0.00127\n",
      "Epoch: 4600 | Loss: 0.00117 | Test Loss: 0.00121 | Delta: 0.00118\n",
      "Epoch: 4800 | Loss: 0.00111 | Test Loss: 0.00115 | Delta: 0.00111\n",
      "Epoch: 5000 | Loss: 0.00107 | Test Loss: 0.00110 | Delta: 0.00107\n",
      "Epoch: 5200 | Loss: 0.00104 | Test Loss: 0.00108 | Delta: 0.00105\n",
      "Epoch: 5400 | Loss: 0.00102 | Test Loss: 0.00106 | Delta: 0.00103\n",
      "Epoch: 5600 | Loss: 0.00101 | Test Loss: 0.00104 | Delta: 0.00101\n",
      "Epoch: 5800 | Loss: 0.00100 | Test Loss: 0.00103 | Delta: 0.00100\n",
      "Epoch: 6000 | Loss: 0.00099 | Test Loss: 0.00102 | Delta: 0.00099\n",
      "Epoch: 6200 | Loss: 0.00098 | Test Loss: 0.00101 | Delta: 0.00098\n",
      "Epoch: 6400 | Loss: 0.00097 | Test Loss: 0.00101 | Delta: 0.00098\n",
      "Epoch: 6600 | Loss: 0.00097 | Test Loss: 0.00100 | Delta: 0.00097\n",
      "Epoch: 6800 | Loss: 0.00096 | Test Loss: 0.00100 | Delta: 0.00097\n",
      "Epoch: 7000 | Loss: 0.00097 | Test Loss: 0.00101 | Delta: 0.00099\n",
      "Epoch: 7200 | Loss: 0.00095 | Test Loss: 0.00099 | Delta: 0.00096\n",
      "Epoch: 7400 | Loss: 0.00095 | Test Loss: 0.00099 | Delta: 0.00095\n",
      "Epoch: 7600 | Loss: 0.00095 | Test Loss: 0.00098 | Delta: 0.00095\n",
      "Early stopping at epoch: 7605\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.80185 | Test Loss: 1.65317 | Delta: 1.65046\n",
      "Epoch: 200 | Loss: 0.00619 | Test Loss: 0.00610 | Delta: 0.00621\n",
      "Epoch: 400 | Loss: 0.00458 | Test Loss: 0.00455 | Delta: 0.00468\n",
      "Epoch: 600 | Loss: 0.00444 | Test Loss: 0.00442 | Delta: 0.00456\n",
      "Epoch: 800 | Loss: 0.00434 | Test Loss: 0.00433 | Delta: 0.00446\n",
      "Epoch: 1000 | Loss: 0.00423 | Test Loss: 0.00423 | Delta: 0.00435\n",
      "Epoch: 1200 | Loss: 0.00410 | Test Loss: 0.00411 | Delta: 0.00422\n",
      "Epoch: 1400 | Loss: 0.00396 | Test Loss: 0.00398 | Delta: 0.00407\n",
      "Epoch: 1600 | Loss: 0.00381 | Test Loss: 0.00384 | Delta: 0.00391\n",
      "Epoch: 1800 | Loss: 0.00364 | Test Loss: 0.00369 | Delta: 0.00375\n",
      "Epoch: 2000 | Loss: 0.00347 | Test Loss: 0.00353 | Delta: 0.00357\n",
      "Epoch: 2200 | Loss: 0.00329 | Test Loss: 0.00336 | Delta: 0.00338\n",
      "Epoch: 2400 | Loss: 0.00311 | Test Loss: 0.00319 | Delta: 0.00320\n",
      "Epoch: 2600 | Loss: 0.00294 | Test Loss: 0.00303 | Delta: 0.00301\n",
      "Epoch: 2800 | Loss: 0.00277 | Test Loss: 0.00287 | Delta: 0.00284\n",
      "Epoch: 3000 | Loss: 0.00262 | Test Loss: 0.00272 | Delta: 0.00268\n",
      "Epoch: 3200 | Loss: 0.00249 | Test Loss: 0.00259 | Delta: 0.00254\n",
      "Epoch: 3400 | Loss: 0.00237 | Test Loss: 0.00247 | Delta: 0.00242\n",
      "Epoch: 3600 | Loss: 0.00226 | Test Loss: 0.00237 | Delta: 0.00231\n",
      "Epoch: 3800 | Loss: 0.00217 | Test Loss: 0.00227 | Delta: 0.00221\n",
      "Epoch: 4000 | Loss: 0.00207 | Test Loss: 0.00217 | Delta: 0.00211\n",
      "Epoch: 4200 | Loss: 0.00197 | Test Loss: 0.00207 | Delta: 0.00201\n",
      "Epoch: 4400 | Loss: 0.00188 | Test Loss: 0.00197 | Delta: 0.00190\n",
      "Epoch: 4600 | Loss: 0.00178 | Test Loss: 0.00187 | Delta: 0.00180\n",
      "Epoch: 4800 | Loss: 0.00169 | Test Loss: 0.00178 | Delta: 0.00171\n",
      "Epoch: 5000 | Loss: 0.00161 | Test Loss: 0.00168 | Delta: 0.00162\n",
      "Epoch: 5200 | Loss: 0.00152 | Test Loss: 0.00159 | Delta: 0.00152\n",
      "Epoch: 5400 | Loss: 0.00143 | Test Loss: 0.00149 | Delta: 0.00143\n",
      "Epoch: 5600 | Loss: 0.00135 | Test Loss: 0.00140 | Delta: 0.00135\n",
      "Epoch: 5800 | Loss: 0.00127 | Test Loss: 0.00132 | Delta: 0.00127\n",
      "Epoch: 6000 | Loss: 0.00120 | Test Loss: 0.00125 | Delta: 0.00120\n",
      "Epoch: 6200 | Loss: 0.00117 | Test Loss: 0.00121 | Delta: 0.00118\n",
      "Epoch: 6400 | Loss: 0.00112 | Test Loss: 0.00115 | Delta: 0.00112\n",
      "Epoch: 6600 | Loss: 0.00109 | Test Loss: 0.00112 | Delta: 0.00109\n",
      "Epoch: 6800 | Loss: 0.00107 | Test Loss: 0.00110 | Delta: 0.00107\n",
      "Epoch: 7000 | Loss: 0.00107 | Test Loss: 0.00108 | Delta: 0.00106\n",
      "Epoch: 7200 | Loss: 0.00104 | Test Loss: 0.00107 | Delta: 0.00105\n",
      "Epoch: 7400 | Loss: 0.00103 | Test Loss: 0.00106 | Delta: 0.00104\n",
      "Epoch: 7600 | Loss: 0.00102 | Test Loss: 0.00105 | Delta: 0.00103\n",
      "Epoch: 7800 | Loss: 0.00101 | Test Loss: 0.00104 | Delta: 0.00102\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.69885 | Test Loss: 1.53368 | Delta: 1.54272\n",
      "Epoch: 200 | Loss: 0.00542 | Test Loss: 0.00523 | Delta: 0.00555\n",
      "Epoch: 400 | Loss: 0.00468 | Test Loss: 0.00453 | Delta: 0.00484\n",
      "Epoch: 600 | Loss: 0.00454 | Test Loss: 0.00441 | Delta: 0.00469\n",
      "Epoch: 800 | Loss: 0.00442 | Test Loss: 0.00431 | Delta: 0.00457\n",
      "Epoch: 1000 | Loss: 0.00430 | Test Loss: 0.00420 | Delta: 0.00444\n",
      "Epoch: 1200 | Loss: 0.00416 | Test Loss: 0.00407 | Delta: 0.00430\n",
      "Epoch: 1400 | Loss: 0.00402 | Test Loss: 0.00393 | Delta: 0.00415\n",
      "Epoch: 1600 | Loss: 0.00387 | Test Loss: 0.00379 | Delta: 0.00400\n",
      "Epoch: 1800 | Loss: 0.00371 | Test Loss: 0.00364 | Delta: 0.00383\n",
      "Epoch: 2000 | Loss: 0.00355 | Test Loss: 0.00349 | Delta: 0.00366\n",
      "Epoch: 2200 | Loss: 0.00338 | Test Loss: 0.00333 | Delta: 0.00349\n",
      "Epoch: 2400 | Loss: 0.00321 | Test Loss: 0.00317 | Delta: 0.00331\n",
      "Epoch: 2600 | Loss: 0.00304 | Test Loss: 0.00301 | Delta: 0.00313\n",
      "Epoch: 2800 | Loss: 0.00286 | Test Loss: 0.00285 | Delta: 0.00295\n",
      "Epoch: 3000 | Loss: 0.00269 | Test Loss: 0.00270 | Delta: 0.00278\n",
      "Epoch: 3200 | Loss: 0.00252 | Test Loss: 0.00254 | Delta: 0.00260\n",
      "Epoch: 3400 | Loss: 0.00235 | Test Loss: 0.00238 | Delta: 0.00243\n",
      "Epoch: 3600 | Loss: 0.00219 | Test Loss: 0.00223 | Delta: 0.00226\n",
      "Epoch: 3800 | Loss: 0.00205 | Test Loss: 0.00209 | Delta: 0.00211\n",
      "Epoch: 4000 | Loss: 0.00191 | Test Loss: 0.00195 | Delta: 0.00196\n",
      "Epoch: 4200 | Loss: 0.00178 | Test Loss: 0.00182 | Delta: 0.00182\n",
      "Epoch: 4400 | Loss: 0.00167 | Test Loss: 0.00170 | Delta: 0.00170\n",
      "Epoch: 4600 | Loss: 0.00156 | Test Loss: 0.00159 | Delta: 0.00158\n",
      "Epoch: 4800 | Loss: 0.00147 | Test Loss: 0.00150 | Delta: 0.00149\n",
      "Epoch: 5000 | Loss: 0.00140 | Test Loss: 0.00142 | Delta: 0.00141\n",
      "Epoch: 5200 | Loss: 0.00133 | Test Loss: 0.00135 | Delta: 0.00134\n",
      "Epoch: 5400 | Loss: 0.00128 | Test Loss: 0.00130 | Delta: 0.00128\n",
      "Epoch: 5600 | Loss: 0.00124 | Test Loss: 0.00125 | Delta: 0.00123\n",
      "Epoch: 5800 | Loss: 0.00120 | Test Loss: 0.00121 | Delta: 0.00120\n",
      "Epoch: 6000 | Loss: 0.00117 | Test Loss: 0.00118 | Delta: 0.00117\n",
      "Epoch: 6200 | Loss: 0.00115 | Test Loss: 0.00116 | Delta: 0.00115\n",
      "Epoch: 6400 | Loss: 0.00113 | Test Loss: 0.00114 | Delta: 0.00113\n",
      "Epoch: 6600 | Loss: 0.00112 | Test Loss: 0.00112 | Delta: 0.00112\n",
      "Epoch: 6800 | Loss: 0.00110 | Test Loss: 0.00111 | Delta: 0.00111\n",
      "Epoch: 7000 | Loss: 0.00109 | Test Loss: 0.00110 | Delta: 0.00110\n",
      "Epoch: 7200 | Loss: 0.00108 | Test Loss: 0.00109 | Delta: 0.00109\n",
      "Epoch: 7400 | Loss: 0.00107 | Test Loss: 0.00108 | Delta: 0.00108\n",
      "Epoch: 7600 | Loss: 0.00107 | Test Loss: 0.00107 | Delta: 0.00108\n",
      "Early stopping at epoch: 7678\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 1.64326 | Test Loss: 1.46006 | Delta: 1.48070\n",
      "Epoch: 200 | Loss: 0.00527 | Test Loss: 0.00524 | Delta: 0.00543\n",
      "Epoch: 400 | Loss: 0.00455 | Test Loss: 0.00456 | Delta: 0.00477\n",
      "Epoch: 600 | Loss: 0.00441 | Test Loss: 0.00444 | Delta: 0.00462\n",
      "Epoch: 800 | Loss: 0.00428 | Test Loss: 0.00433 | Delta: 0.00450\n",
      "Epoch: 1000 | Loss: 0.00415 | Test Loss: 0.00420 | Delta: 0.00436\n",
      "Epoch: 1200 | Loss: 0.00401 | Test Loss: 0.00407 | Delta: 0.00421\n",
      "Epoch: 1400 | Loss: 0.00387 | Test Loss: 0.00392 | Delta: 0.00405\n",
      "Epoch: 1600 | Loss: 0.00371 | Test Loss: 0.00377 | Delta: 0.00389\n",
      "Epoch: 1800 | Loss: 0.00355 | Test Loss: 0.00361 | Delta: 0.00372\n",
      "Epoch: 2000 | Loss: 0.00339 | Test Loss: 0.00344 | Delta: 0.00355\n",
      "Epoch: 2200 | Loss: 0.00322 | Test Loss: 0.00327 | Delta: 0.00336\n",
      "Epoch: 2400 | Loss: 0.00304 | Test Loss: 0.00309 | Delta: 0.00318\n",
      "Epoch: 2600 | Loss: 0.00287 | Test Loss: 0.00291 | Delta: 0.00299\n",
      "Epoch: 2800 | Loss: 0.00269 | Test Loss: 0.00273 | Delta: 0.00280\n",
      "Epoch: 3000 | Loss: 0.00251 | Test Loss: 0.00255 | Delta: 0.00261\n",
      "Epoch: 3200 | Loss: 0.00234 | Test Loss: 0.00238 | Delta: 0.00242\n",
      "Epoch: 3400 | Loss: 0.00218 | Test Loss: 0.00221 | Delta: 0.00224\n",
      "Epoch: 3600 | Loss: 0.00203 | Test Loss: 0.00206 | Delta: 0.00208\n",
      "Epoch: 3800 | Loss: 0.00189 | Test Loss: 0.00192 | Delta: 0.00193\n",
      "Epoch: 4000 | Loss: 0.00178 | Test Loss: 0.00181 | Delta: 0.00181\n",
      "Epoch: 4200 | Loss: 0.00169 | Test Loss: 0.00171 | Delta: 0.00171\n",
      "Epoch: 4400 | Loss: 0.00162 | Test Loss: 0.00164 | Delta: 0.00163\n",
      "Epoch: 4600 | Loss: 0.00156 | Test Loss: 0.00158 | Delta: 0.00156\n",
      "Epoch: 4800 | Loss: 0.00151 | Test Loss: 0.00154 | Delta: 0.00151\n",
      "Epoch: 5000 | Loss: 0.00147 | Test Loss: 0.00149 | Delta: 0.00147\n",
      "Epoch: 5200 | Loss: 0.00143 | Test Loss: 0.00146 | Delta: 0.00143\n",
      "Epoch: 5400 | Loss: 0.00140 | Test Loss: 0.00143 | Delta: 0.00140\n",
      "Epoch: 5600 | Loss: 0.00137 | Test Loss: 0.00140 | Delta: 0.00137\n",
      "Epoch: 5800 | Loss: 0.00134 | Test Loss: 0.00137 | Delta: 0.00134\n",
      "Epoch: 6000 | Loss: 0.00132 | Test Loss: 0.00135 | Delta: 0.00132\n",
      "Epoch: 6200 | Loss: 0.00130 | Test Loss: 0.00133 | Delta: 0.00130\n",
      "Epoch: 6400 | Loss: 0.00128 | Test Loss: 0.00131 | Delta: 0.00128\n",
      "Epoch: 6600 | Loss: 0.00126 | Test Loss: 0.00129 | Delta: 0.00127\n",
      "Epoch: 6800 | Loss: 0.00125 | Test Loss: 0.00128 | Delta: 0.00125\n",
      "Epoch: 7000 | Loss: 0.00124 | Test Loss: 0.00126 | Delta: 0.00124\n",
      "Epoch: 7200 | Loss: 0.00122 | Test Loss: 0.00125 | Delta: 0.00124\n",
      "Epoch: 7400 | Loss: 0.00134 | Test Loss: 0.00138 | Delta: 0.00138\n",
      "Epoch: 7600 | Loss: 0.00119 | Test Loss: 0.00123 | Delta: 0.00121\n",
      "Epoch: 7800 | Loss: 0.00118 | Test Loss: 0.00122 | Delta: 0.00120\n",
      "Early stopping at epoch: 7861\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 2.42303 | Test Loss: 2.23524 | Delta: 2.24865\n",
      "Epoch: 200 | Loss: 0.00863 | Test Loss: 0.00853 | Delta: 0.00848\n",
      "Epoch: 400 | Loss: 0.00558 | Test Loss: 0.00553 | Delta: 0.00558\n",
      "Epoch: 600 | Loss: 0.00498 | Test Loss: 0.00502 | Delta: 0.00502\n",
      "Epoch: 800 | Loss: 0.00478 | Test Loss: 0.00487 | Delta: 0.00484\n",
      "Epoch: 1000 | Loss: 0.00467 | Test Loss: 0.00478 | Delta: 0.00473\n",
      "Epoch: 1200 | Loss: 0.00457 | Test Loss: 0.00468 | Delta: 0.00462\n",
      "Epoch: 1400 | Loss: 0.00445 | Test Loss: 0.00456 | Delta: 0.00451\n",
      "Epoch: 1600 | Loss: 0.00432 | Test Loss: 0.00442 | Delta: 0.00437\n",
      "Epoch: 1800 | Loss: 0.00417 | Test Loss: 0.00427 | Delta: 0.00422\n",
      "Epoch: 2000 | Loss: 0.00401 | Test Loss: 0.00411 | Delta: 0.00406\n",
      "Epoch: 2200 | Loss: 0.00384 | Test Loss: 0.00393 | Delta: 0.00388\n",
      "Epoch: 2400 | Loss: 0.00365 | Test Loss: 0.00374 | Delta: 0.00369\n",
      "Epoch: 2600 | Loss: 0.00345 | Test Loss: 0.00353 | Delta: 0.00349\n",
      "Epoch: 2800 | Loss: 0.00324 | Test Loss: 0.00331 | Delta: 0.00328\n",
      "Epoch: 3000 | Loss: 0.00302 | Test Loss: 0.00308 | Delta: 0.00305\n",
      "Epoch: 3200 | Loss: 0.00280 | Test Loss: 0.00286 | Delta: 0.00283\n",
      "Epoch: 3400 | Loss: 0.00259 | Test Loss: 0.00263 | Delta: 0.00262\n",
      "Epoch: 3600 | Loss: 0.00239 | Test Loss: 0.00243 | Delta: 0.00242\n",
      "Epoch: 3800 | Loss: 0.00222 | Test Loss: 0.00225 | Delta: 0.00225\n",
      "Epoch: 4000 | Loss: 0.00207 | Test Loss: 0.00210 | Delta: 0.00210\n",
      "Epoch: 4200 | Loss: 0.00195 | Test Loss: 0.00198 | Delta: 0.00199\n",
      "Epoch: 4400 | Loss: 0.00186 | Test Loss: 0.00188 | Delta: 0.00189\n",
      "Epoch: 4600 | Loss: 0.00179 | Test Loss: 0.00181 | Delta: 0.00182\n",
      "Epoch: 4800 | Loss: 0.00174 | Test Loss: 0.00175 | Delta: 0.00177\n",
      "Epoch: 5000 | Loss: 0.00170 | Test Loss: 0.00170 | Delta: 0.00172\n",
      "Epoch: 5200 | Loss: 0.00166 | Test Loss: 0.00167 | Delta: 0.00169\n",
      "Epoch: 5400 | Loss: 0.00163 | Test Loss: 0.00163 | Delta: 0.00166\n",
      "Epoch: 5600 | Loss: 0.00161 | Test Loss: 0.00161 | Delta: 0.00163\n",
      "Epoch: 5800 | Loss: 0.00158 | Test Loss: 0.00158 | Delta: 0.00161\n",
      "Epoch: 6000 | Loss: 0.00156 | Test Loss: 0.00155 | Delta: 0.00158\n",
      "Epoch: 6200 | Loss: 0.00153 | Test Loss: 0.00152 | Delta: 0.00155\n",
      "Epoch: 6400 | Loss: 0.00150 | Test Loss: 0.00149 | Delta: 0.00153\n",
      "Epoch: 6600 | Loss: 0.00149 | Test Loss: 0.00149 | Delta: 0.00151\n",
      "Epoch: 6800 | Loss: 0.00145 | Test Loss: 0.00144 | Delta: 0.00148\n",
      "Epoch: 7000 | Loss: 0.00144 | Test Loss: 0.00144 | Delta: 0.00147\n",
      "Epoch: 7200 | Loss: 0.00141 | Test Loss: 0.00140 | Delta: 0.00145\n",
      "Epoch: 7400 | Loss: 0.00140 | Test Loss: 0.00138 | Delta: 0.00142\n",
      "Epoch: 7600 | Loss: 0.00145 | Test Loss: 0.00142 | Delta: 0.00144\n",
      "Epoch: 7800 | Loss: 0.00137 | Test Loss: 0.00137 | Delta: 0.00140\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 2.90559 | Test Loss: 2.69072 | Delta: 2.70670\n",
      "Epoch: 200 | Loss: 0.00914 | Test Loss: 0.00892 | Delta: 0.00925\n",
      "Epoch: 400 | Loss: 0.00630 | Test Loss: 0.00618 | Delta: 0.00644\n",
      "Epoch: 600 | Loss: 0.00548 | Test Loss: 0.00542 | Delta: 0.00563\n",
      "Epoch: 800 | Loss: 0.00514 | Test Loss: 0.00513 | Delta: 0.00529\n",
      "Epoch: 1000 | Loss: 0.00497 | Test Loss: 0.00499 | Delta: 0.00511\n",
      "Epoch: 1200 | Loss: 0.00481 | Test Loss: 0.00484 | Delta: 0.00495\n",
      "Epoch: 1400 | Loss: 0.00464 | Test Loss: 0.00467 | Delta: 0.00478\n",
      "Epoch: 1600 | Loss: 0.00445 | Test Loss: 0.00448 | Delta: 0.00458\n",
      "Epoch: 1800 | Loss: 0.00425 | Test Loss: 0.00427 | Delta: 0.00437\n",
      "Epoch: 2000 | Loss: 0.00403 | Test Loss: 0.00404 | Delta: 0.00413\n",
      "Epoch: 2200 | Loss: 0.00380 | Test Loss: 0.00381 | Delta: 0.00390\n",
      "Epoch: 2400 | Loss: 0.00356 | Test Loss: 0.00357 | Delta: 0.00365\n",
      "Epoch: 2600 | Loss: 0.00334 | Test Loss: 0.00334 | Delta: 0.00342\n",
      "Epoch: 2800 | Loss: 0.00313 | Test Loss: 0.00312 | Delta: 0.00321\n",
      "Epoch: 3000 | Loss: 0.00294 | Test Loss: 0.00292 | Delta: 0.00301\n",
      "Epoch: 3200 | Loss: 0.00276 | Test Loss: 0.00273 | Delta: 0.00283\n",
      "Epoch: 3400 | Loss: 0.00260 | Test Loss: 0.00257 | Delta: 0.00267\n",
      "Epoch: 3600 | Loss: 0.00245 | Test Loss: 0.00242 | Delta: 0.00252\n",
      "Epoch: 3800 | Loss: 0.00232 | Test Loss: 0.00229 | Delta: 0.00239\n",
      "Epoch: 4000 | Loss: 0.00221 | Test Loss: 0.00217 | Delta: 0.00228\n",
      "Epoch: 4200 | Loss: 0.00212 | Test Loss: 0.00208 | Delta: 0.00219\n",
      "Epoch: 4400 | Loss: 0.00204 | Test Loss: 0.00200 | Delta: 0.00211\n",
      "Epoch: 4600 | Loss: 0.00198 | Test Loss: 0.00193 | Delta: 0.00205\n",
      "Epoch: 4800 | Loss: 0.00193 | Test Loss: 0.00188 | Delta: 0.00200\n",
      "Epoch: 5000 | Loss: 0.00189 | Test Loss: 0.00184 | Delta: 0.00195\n",
      "Epoch: 5200 | Loss: 0.00185 | Test Loss: 0.00181 | Delta: 0.00192\n",
      "Epoch: 5400 | Loss: 0.00182 | Test Loss: 0.00178 | Delta: 0.00189\n",
      "Epoch: 5600 | Loss: 0.00180 | Test Loss: 0.00175 | Delta: 0.00186\n",
      "Epoch: 5800 | Loss: 0.00177 | Test Loss: 0.00173 | Delta: 0.00183\n",
      "Epoch: 6000 | Loss: 0.00175 | Test Loss: 0.00170 | Delta: 0.00181\n",
      "Epoch: 6200 | Loss: 0.00172 | Test Loss: 0.00167 | Delta: 0.00178\n",
      "Epoch: 6400 | Loss: 0.00169 | Test Loss: 0.00165 | Delta: 0.00175\n",
      "Epoch: 6600 | Loss: 0.00166 | Test Loss: 0.00163 | Delta: 0.00173\n",
      "Epoch: 6800 | Loss: 0.00164 | Test Loss: 0.00160 | Delta: 0.00170\n",
      "Epoch: 7000 | Loss: 0.00161 | Test Loss: 0.00158 | Delta: 0.00167\n",
      "Epoch: 7200 | Loss: 0.00158 | Test Loss: 0.00155 | Delta: 0.00165\n",
      "Epoch: 7400 | Loss: 0.00156 | Test Loss: 0.00153 | Delta: 0.00163\n",
      "Epoch: 7600 | Loss: 0.00156 | Test Loss: 0.00153 | Delta: 0.00161\n",
      "Epoch: 7800 | Loss: 0.00151 | Test Loss: 0.00149 | Delta: 0.00159\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 0.91085 | Test Loss: 0.79201 | Delta: 0.78738\n",
      "Epoch: 200 | Loss: 0.00636 | Test Loss: 0.00593 | Delta: 0.00655\n",
      "Epoch: 400 | Loss: 0.00554 | Test Loss: 0.00525 | Delta: 0.00569\n",
      "Epoch: 600 | Loss: 0.00527 | Test Loss: 0.00502 | Delta: 0.00540\n",
      "Epoch: 800 | Loss: 0.00501 | Test Loss: 0.00478 | Delta: 0.00512\n",
      "Epoch: 1000 | Loss: 0.00474 | Test Loss: 0.00453 | Delta: 0.00483\n",
      "Epoch: 1200 | Loss: 0.00447 | Test Loss: 0.00428 | Delta: 0.00454\n",
      "Epoch: 1400 | Loss: 0.00421 | Test Loss: 0.00403 | Delta: 0.00426\n",
      "Epoch: 1600 | Loss: 0.00396 | Test Loss: 0.00379 | Delta: 0.00400\n",
      "Epoch: 1800 | Loss: 0.00373 | Test Loss: 0.00358 | Delta: 0.00375\n",
      "Epoch: 2000 | Loss: 0.00351 | Test Loss: 0.00338 | Delta: 0.00353\n",
      "Epoch: 2200 | Loss: 0.00332 | Test Loss: 0.00320 | Delta: 0.00333\n",
      "Epoch: 2400 | Loss: 0.00314 | Test Loss: 0.00304 | Delta: 0.00315\n",
      "Epoch: 2600 | Loss: 0.00298 | Test Loss: 0.00289 | Delta: 0.00299\n",
      "Epoch: 2800 | Loss: 0.00284 | Test Loss: 0.00277 | Delta: 0.00285\n",
      "Epoch: 3000 | Loss: 0.00272 | Test Loss: 0.00266 | Delta: 0.00273\n",
      "Epoch: 3200 | Loss: 0.00260 | Test Loss: 0.00256 | Delta: 0.00261\n",
      "Epoch: 3400 | Loss: 0.00247 | Test Loss: 0.00244 | Delta: 0.00247\n",
      "Epoch: 3600 | Loss: 0.00236 | Test Loss: 0.00234 | Delta: 0.00236\n",
      "Epoch: 3800 | Loss: 0.00227 | Test Loss: 0.00226 | Delta: 0.00228\n",
      "Epoch: 4000 | Loss: 0.00220 | Test Loss: 0.00219 | Delta: 0.00220\n",
      "Epoch: 4200 | Loss: 0.00214 | Test Loss: 0.00213 | Delta: 0.00214\n",
      "Epoch: 4400 | Loss: 0.00208 | Test Loss: 0.00207 | Delta: 0.00208\n",
      "Epoch: 4600 | Loss: 0.00202 | Test Loss: 0.00201 | Delta: 0.00202\n",
      "Epoch: 4800 | Loss: 0.00197 | Test Loss: 0.00196 | Delta: 0.00197\n",
      "Epoch: 5000 | Loss: 0.00192 | Test Loss: 0.00191 | Delta: 0.00192\n",
      "Epoch: 5200 | Loss: 0.00187 | Test Loss: 0.00186 | Delta: 0.00187\n",
      "Epoch: 5400 | Loss: 0.00182 | Test Loss: 0.00182 | Delta: 0.00182\n",
      "Epoch: 5600 | Loss: 0.00178 | Test Loss: 0.00178 | Delta: 0.00178\n",
      "Epoch: 5800 | Loss: 0.00174 | Test Loss: 0.00174 | Delta: 0.00174\n",
      "Epoch: 6000 | Loss: 0.00174 | Test Loss: 0.00175 | Delta: 0.00176\n",
      "Epoch: 6200 | Loss: 0.00183 | Test Loss: 0.00180 | Delta: 0.00181\n",
      "Epoch: 6400 | Loss: 0.00164 | Test Loss: 0.00165 | Delta: 0.00165\n",
      "Epoch: 6600 | Loss: 0.00161 | Test Loss: 0.00163 | Delta: 0.00162\n",
      "Epoch: 6800 | Loss: 0.00159 | Test Loss: 0.00160 | Delta: 0.00159\n",
      "Epoch: 7000 | Loss: 0.00156 | Test Loss: 0.00158 | Delta: 0.00157\n",
      "Epoch: 7200 | Loss: 0.00154 | Test Loss: 0.00155 | Delta: 0.00155\n",
      "Early stopping at epoch: 7212\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.43539 | Test Loss: 1.30670 | Delta: 1.29084\n",
      "Epoch: 200 | Loss: 0.00849 | Test Loss: 0.00807 | Delta: 0.00829\n",
      "Epoch: 400 | Loss: 0.00498 | Test Loss: 0.00483 | Delta: 0.00490\n",
      "Epoch: 600 | Loss: 0.00454 | Test Loss: 0.00443 | Delta: 0.00447\n",
      "Epoch: 800 | Loss: 0.00436 | Test Loss: 0.00426 | Delta: 0.00430\n",
      "Epoch: 1000 | Loss: 0.00421 | Test Loss: 0.00411 | Delta: 0.00415\n",
      "Epoch: 1200 | Loss: 0.00406 | Test Loss: 0.00397 | Delta: 0.00401\n",
      "Epoch: 1400 | Loss: 0.00392 | Test Loss: 0.00383 | Delta: 0.00387\n",
      "Epoch: 1600 | Loss: 0.00378 | Test Loss: 0.00370 | Delta: 0.00374\n",
      "Epoch: 1800 | Loss: 0.00364 | Test Loss: 0.00356 | Delta: 0.00360\n",
      "Epoch: 2000 | Loss: 0.00349 | Test Loss: 0.00342 | Delta: 0.00346\n",
      "Epoch: 2200 | Loss: 0.00333 | Test Loss: 0.00327 | Delta: 0.00331\n",
      "Epoch: 2400 | Loss: 0.00318 | Test Loss: 0.00312 | Delta: 0.00315\n",
      "Epoch: 2600 | Loss: 0.00301 | Test Loss: 0.00297 | Delta: 0.00299\n",
      "Epoch: 2800 | Loss: 0.00285 | Test Loss: 0.00281 | Delta: 0.00284\n",
      "Epoch: 3000 | Loss: 0.00269 | Test Loss: 0.00267 | Delta: 0.00268\n",
      "Epoch: 3200 | Loss: 0.00254 | Test Loss: 0.00253 | Delta: 0.00253\n",
      "Epoch: 3400 | Loss: 0.00241 | Test Loss: 0.00241 | Delta: 0.00240\n",
      "Epoch: 3600 | Loss: 0.00229 | Test Loss: 0.00230 | Delta: 0.00228\n",
      "Epoch: 3800 | Loss: 0.00219 | Test Loss: 0.00220 | Delta: 0.00218\n",
      "Epoch: 4000 | Loss: 0.00210 | Test Loss: 0.00212 | Delta: 0.00209\n",
      "Epoch: 4200 | Loss: 0.00203 | Test Loss: 0.00205 | Delta: 0.00202\n",
      "Epoch: 4400 | Loss: 0.00196 | Test Loss: 0.00198 | Delta: 0.00194\n",
      "Epoch: 4600 | Loss: 0.00189 | Test Loss: 0.00192 | Delta: 0.00188\n",
      "Epoch: 4800 | Loss: 0.00184 | Test Loss: 0.00187 | Delta: 0.00182\n",
      "Epoch: 5000 | Loss: 0.00179 | Test Loss: 0.00182 | Delta: 0.00177\n",
      "Epoch: 5200 | Loss: 0.00174 | Test Loss: 0.00178 | Delta: 0.00173\n",
      "Epoch: 5400 | Loss: 0.00171 | Test Loss: 0.00175 | Delta: 0.00169\n",
      "Epoch: 5600 | Loss: 0.00168 | Test Loss: 0.00174 | Delta: 0.00167\n",
      "Epoch: 5800 | Loss: 0.00164 | Test Loss: 0.00169 | Delta: 0.00163\n",
      "Epoch: 6000 | Loss: 0.00161 | Test Loss: 0.00166 | Delta: 0.00160\n",
      "Epoch: 6200 | Loss: 0.00159 | Test Loss: 0.00164 | Delta: 0.00157\n",
      "Epoch: 6400 | Loss: 0.00156 | Test Loss: 0.00161 | Delta: 0.00155\n",
      "Epoch: 6600 | Loss: 0.00154 | Test Loss: 0.00159 | Delta: 0.00153\n",
      "Epoch: 6800 | Loss: 0.00152 | Test Loss: 0.00157 | Delta: 0.00151\n",
      "Early stopping at epoch: 6926\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 8000\n",
    "learning_rate = 0.002\n",
    "momentum = 1.2\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_M_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_M_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_M_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Delta: {(valid_loss.item()):.5f}\")\n",
    "    \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "384a9de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 7.9051 %\n"
     ]
    }
   ],
   "source": [
    "# mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_M_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "M_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(M_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4155d441",
   "metadata": {},
   "source": [
    "# Initial - MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e1f80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "input_dim = 62\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "MT_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    MT_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e7b5b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0218, -0.0221, -0.0232,  ..., -0.0733, -0.0740, -0.0746],\n",
       "        [-0.0265, -0.0263, -0.0256,  ..., -0.0713, -0.0710, -0.0708],\n",
       "        [-0.0266, -0.0265, -0.0261,  ..., -0.0726, -0.0725, -0.0724],\n",
       "        ...,\n",
       "        [-0.0238, -0.0237, -0.0235,  ..., -0.0745, -0.0749, -0.0752],\n",
       "        [-0.0229, -0.0231, -0.0235,  ..., -0.0720, -0.0722, -0.0723],\n",
       "        [-0.0217, -0.0219, -0.0222,  ..., -0.0722, -0.0720, -0.0718]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_MT_train, y_train = X_MT_train.to(device), y_train.to(device)\n",
    "X_MT_test, y_test = X_MT_test.to(device), y_test.to(device)\n",
    "X_MT_valid, y_valid = X_MT_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_MT_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cf938784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0509, -0.0515, -0.0541,  ..., -0.1709, -0.1725, -0.1738],\n",
       "        [-0.0620, -0.0613, -0.0598,  ..., -0.1664, -0.1658, -0.1654],\n",
       "        [-0.0625, -0.0622, -0.0613,  ..., -0.1708, -0.1705, -0.1703],\n",
       "        ...,\n",
       "        [-0.0548, -0.0544, -0.0539,  ..., -0.1712, -0.1721, -0.1729],\n",
       "        [-0.0541, -0.0545, -0.0555,  ..., -0.1701, -0.1704, -0.1707],\n",
       "        [-0.0516, -0.0521, -0.0527,  ..., -0.1715, -0.1710, -0.1706]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_MT_train_n = nn.functional.normalize(X_MT_train)\n",
    "X_MT_test_n = nn.functional.normalize(X_MT_test)\n",
    "X_MT_valid_n = nn.functional.normalize(X_MT_valid)\n",
    "\n",
    "X_MT_train_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "90b099db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.55496 | Test Loss: 1.51819 | Valid loss: 1.49661\n",
      "Epoch: 200 | Loss: 0.01857 | Test Loss: 0.01887 | Valid loss: 0.01857\n",
      "Epoch: 400 | Loss: 0.01857 | Test Loss: 0.01887 | Valid loss: 0.01857\n",
      "Epoch: 600 | Loss: 0.01856 | Test Loss: 0.01887 | Valid loss: 0.01856\n",
      "Epoch: 800 | Loss: 0.01856 | Test Loss: 0.01886 | Valid loss: 0.01856\n",
      "Epoch: 1000 | Loss: 0.01855 | Test Loss: 0.01885 | Valid loss: 0.01855\n",
      "Epoch: 1200 | Loss: 0.01855 | Test Loss: 0.01885 | Valid loss: 0.01854\n",
      "Epoch: 1400 | Loss: 0.01854 | Test Loss: 0.01884 | Valid loss: 0.01854\n",
      "Epoch: 1600 | Loss: 0.01853 | Test Loss: 0.01883 | Valid loss: 0.01853\n",
      "Epoch: 1800 | Loss: 0.01852 | Test Loss: 0.01882 | Valid loss: 0.01852\n",
      "Epoch: 2000 | Loss: 0.01851 | Test Loss: 0.01881 | Valid loss: 0.01851\n",
      "Epoch: 2200 | Loss: 0.01850 | Test Loss: 0.01880 | Valid loss: 0.01850\n",
      "Epoch: 2400 | Loss: 0.01848 | Test Loss: 0.01878 | Valid loss: 0.01848\n",
      "Epoch: 2600 | Loss: 0.01847 | Test Loss: 0.01877 | Valid loss: 0.01847\n",
      "Epoch: 2800 | Loss: 0.01845 | Test Loss: 0.01875 | Valid loss: 0.01845\n",
      "Epoch: 3000 | Loss: 0.01843 | Test Loss: 0.01873 | Valid loss: 0.01843\n",
      "Epoch: 3200 | Loss: 0.01841 | Test Loss: 0.01871 | Valid loss: 0.01841\n",
      "Epoch: 3400 | Loss: 0.01839 | Test Loss: 0.01868 | Valid loss: 0.01838\n",
      "Epoch: 3600 | Loss: 0.01836 | Test Loss: 0.01866 | Valid loss: 0.01836\n",
      "Epoch: 3800 | Loss: 0.01833 | Test Loss: 0.01862 | Valid loss: 0.01833\n",
      "Epoch: 4000 | Loss: 0.01830 | Test Loss: 0.01859 | Valid loss: 0.01830\n",
      "Epoch: 4200 | Loss: 0.01826 | Test Loss: 0.01855 | Valid loss: 0.01826\n",
      "Epoch: 4400 | Loss: 0.01822 | Test Loss: 0.01851 | Valid loss: 0.01822\n",
      "Epoch: 4600 | Loss: 0.01818 | Test Loss: 0.01846 | Valid loss: 0.01817\n",
      "Epoch: 4800 | Loss: 0.01812 | Test Loss: 0.01841 | Valid loss: 0.01812\n",
      "Epoch: 5000 | Loss: 0.01806 | Test Loss: 0.01835 | Valid loss: 0.01806\n",
      "Epoch: 5200 | Loss: 0.01799 | Test Loss: 0.01827 | Valid loss: 0.01799\n",
      "Epoch: 5400 | Loss: 0.01791 | Test Loss: 0.01819 | Valid loss: 0.01791\n",
      "Epoch: 5600 | Loss: 0.01782 | Test Loss: 0.01809 | Valid loss: 0.01781\n",
      "Epoch: 5800 | Loss: 0.01770 | Test Loss: 0.01797 | Valid loss: 0.01770\n",
      "Epoch: 6000 | Loss: 0.01756 | Test Loss: 0.01783 | Valid loss: 0.01756\n",
      "Epoch: 6200 | Loss: 0.01739 | Test Loss: 0.01765 | Valid loss: 0.01738\n",
      "Epoch: 6400 | Loss: 0.01717 | Test Loss: 0.01742 | Valid loss: 0.01717\n",
      "Epoch: 6600 | Loss: 0.01689 | Test Loss: 0.01714 | Valid loss: 0.01689\n",
      "Epoch: 6800 | Loss: 0.01654 | Test Loss: 0.01677 | Valid loss: 0.01654\n",
      "Epoch: 7000 | Loss: 0.01610 | Test Loss: 0.01631 | Valid loss: 0.01610\n",
      "Epoch: 7200 | Loss: 0.01554 | Test Loss: 0.01573 | Valid loss: 0.01553\n",
      "Epoch: 7400 | Loss: 0.01484 | Test Loss: 0.01501 | Valid loss: 0.01483\n",
      "Epoch: 7600 | Loss: 0.01399 | Test Loss: 0.01414 | Valid loss: 0.01399\n",
      "Epoch: 7800 | Loss: 0.01300 | Test Loss: 0.01311 | Valid loss: 0.01299\n",
      "Epoch: 8000 | Loss: 0.01188 | Test Loss: 0.01197 | Valid loss: 0.01188\n",
      "Epoch: 8200 | Loss: 0.01070 | Test Loss: 0.01076 | Valid loss: 0.01070\n",
      "Epoch: 8400 | Loss: 0.00951 | Test Loss: 0.00955 | Valid loss: 0.00951\n",
      "Epoch: 8600 | Loss: 0.00838 | Test Loss: 0.00841 | Valid loss: 0.00840\n",
      "Epoch: 8800 | Loss: 0.00740 | Test Loss: 0.00743 | Valid loss: 0.00742\n",
      "Epoch: 9000 | Loss: 0.00662 | Test Loss: 0.00665 | Valid loss: 0.00664\n",
      "Epoch: 9200 | Loss: 0.00602 | Test Loss: 0.00606 | Valid loss: 0.00605\n",
      "Epoch: 9400 | Loss: 0.00557 | Test Loss: 0.00562 | Valid loss: 0.00560\n",
      "Epoch: 9600 | Loss: 0.00524 | Test Loss: 0.00530 | Valid loss: 0.00527\n",
      "Epoch: 9800 | Loss: 0.00499 | Test Loss: 0.00506 | Valid loss: 0.00502\n",
      "Epoch: 10000 | Loss: 0.00479 | Test Loss: 0.00488 | Valid loss: 0.00482\n",
      "Epoch: 10200 | Loss: 0.00464 | Test Loss: 0.00473 | Valid loss: 0.00467\n",
      "Epoch: 10400 | Loss: 0.00451 | Test Loss: 0.00460 | Valid loss: 0.00454\n",
      "Epoch: 10600 | Loss: 0.00441 | Test Loss: 0.00450 | Valid loss: 0.00443\n",
      "Epoch: 10800 | Loss: 0.00431 | Test Loss: 0.00441 | Valid loss: 0.00434\n",
      "Epoch: 11000 | Loss: 0.00424 | Test Loss: 0.00434 | Valid loss: 0.00426\n",
      "Epoch: 11200 | Loss: 0.00417 | Test Loss: 0.00427 | Valid loss: 0.00419\n",
      "Epoch: 11400 | Loss: 0.00410 | Test Loss: 0.00421 | Valid loss: 0.00413\n",
      "Epoch: 11600 | Loss: 0.00405 | Test Loss: 0.00415 | Valid loss: 0.00407\n",
      "Epoch: 11800 | Loss: 0.00399 | Test Loss: 0.00410 | Valid loss: 0.00402\n",
      "Epoch: 12000 | Loss: 0.00394 | Test Loss: 0.00406 | Valid loss: 0.00397\n",
      "Epoch: 12200 | Loss: 0.00390 | Test Loss: 0.00401 | Valid loss: 0.00393\n",
      "Epoch: 12400 | Loss: 0.00386 | Test Loss: 0.00397 | Valid loss: 0.00388\n",
      "Epoch: 12600 | Loss: 0.00382 | Test Loss: 0.00393 | Valid loss: 0.00385\n",
      "Early stopping at epoch: 12797\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.83893 | Test Loss: 1.79083 | Valid loss: 1.76810\n",
      "Epoch: 200 | Loss: 0.01545 | Test Loss: 0.01604 | Valid loss: 0.01528\n",
      "Epoch: 400 | Loss: 0.01545 | Test Loss: 0.01604 | Valid loss: 0.01528\n",
      "Epoch: 600 | Loss: 0.01544 | Test Loss: 0.01604 | Valid loss: 0.01528\n",
      "Epoch: 800 | Loss: 0.01544 | Test Loss: 0.01603 | Valid loss: 0.01527\n",
      "Epoch: 1000 | Loss: 0.01543 | Test Loss: 0.01603 | Valid loss: 0.01527\n",
      "Epoch: 1200 | Loss: 0.01543 | Test Loss: 0.01602 | Valid loss: 0.01526\n",
      "Epoch: 1400 | Loss: 0.01542 | Test Loss: 0.01601 | Valid loss: 0.01526\n",
      "Epoch: 1600 | Loss: 0.01542 | Test Loss: 0.01601 | Valid loss: 0.01525\n",
      "Epoch: 1800 | Loss: 0.01541 | Test Loss: 0.01600 | Valid loss: 0.01524\n",
      "Epoch: 2000 | Loss: 0.01540 | Test Loss: 0.01599 | Valid loss: 0.01523\n",
      "Epoch: 2200 | Loss: 0.01539 | Test Loss: 0.01598 | Valid loss: 0.01522\n",
      "Epoch: 2400 | Loss: 0.01538 | Test Loss: 0.01597 | Valid loss: 0.01521\n",
      "Epoch: 2600 | Loss: 0.01536 | Test Loss: 0.01595 | Valid loss: 0.01520\n",
      "Epoch: 2800 | Loss: 0.01535 | Test Loss: 0.01594 | Valid loss: 0.01518\n",
      "Epoch: 3000 | Loss: 0.01533 | Test Loss: 0.01592 | Valid loss: 0.01517\n",
      "Epoch: 3200 | Loss: 0.01531 | Test Loss: 0.01590 | Valid loss: 0.01515\n",
      "Epoch: 3400 | Loss: 0.01529 | Test Loss: 0.01588 | Valid loss: 0.01513\n",
      "Epoch: 3600 | Loss: 0.01527 | Test Loss: 0.01586 | Valid loss: 0.01511\n",
      "Epoch: 3800 | Loss: 0.01525 | Test Loss: 0.01583 | Valid loss: 0.01509\n",
      "Epoch: 4000 | Loss: 0.01522 | Test Loss: 0.01581 | Valid loss: 0.01506\n",
      "Epoch: 4200 | Loss: 0.01519 | Test Loss: 0.01577 | Valid loss: 0.01503\n",
      "Epoch: 4400 | Loss: 0.01516 | Test Loss: 0.01574 | Valid loss: 0.01500\n",
      "Epoch: 4600 | Loss: 0.01512 | Test Loss: 0.01570 | Valid loss: 0.01496\n",
      "Epoch: 4800 | Loss: 0.01508 | Test Loss: 0.01566 | Valid loss: 0.01492\n",
      "Epoch: 5000 | Loss: 0.01503 | Test Loss: 0.01561 | Valid loss: 0.01487\n",
      "Epoch: 5200 | Loss: 0.01498 | Test Loss: 0.01555 | Valid loss: 0.01482\n",
      "Epoch: 5400 | Loss: 0.01491 | Test Loss: 0.01549 | Valid loss: 0.01476\n",
      "Epoch: 5600 | Loss: 0.01484 | Test Loss: 0.01541 | Valid loss: 0.01469\n",
      "Epoch: 5800 | Loss: 0.01476 | Test Loss: 0.01533 | Valid loss: 0.01460\n",
      "Epoch: 6000 | Loss: 0.01466 | Test Loss: 0.01523 | Valid loss: 0.01451\n",
      "Epoch: 6200 | Loss: 0.01454 | Test Loss: 0.01510 | Valid loss: 0.01439\n",
      "Epoch: 6400 | Loss: 0.01440 | Test Loss: 0.01495 | Valid loss: 0.01425\n",
      "Epoch: 6600 | Loss: 0.01422 | Test Loss: 0.01477 | Valid loss: 0.01407\n",
      "Epoch: 6800 | Loss: 0.01399 | Test Loss: 0.01454 | Valid loss: 0.01385\n",
      "Epoch: 7000 | Loss: 0.01371 | Test Loss: 0.01425 | Valid loss: 0.01357\n",
      "Epoch: 7200 | Loss: 0.01335 | Test Loss: 0.01388 | Valid loss: 0.01322\n",
      "Epoch: 7400 | Loss: 0.01290 | Test Loss: 0.01341 | Valid loss: 0.01277\n",
      "Epoch: 7600 | Loss: 0.01234 | Test Loss: 0.01283 | Valid loss: 0.01222\n",
      "Epoch: 7800 | Loss: 0.01165 | Test Loss: 0.01212 | Valid loss: 0.01154\n",
      "Epoch: 8000 | Loss: 0.01084 | Test Loss: 0.01128 | Valid loss: 0.01075\n",
      "Epoch: 8200 | Loss: 0.00993 | Test Loss: 0.01033 | Valid loss: 0.00985\n",
      "Epoch: 8400 | Loss: 0.00894 | Test Loss: 0.00930 | Valid loss: 0.00888\n",
      "Epoch: 8600 | Loss: 0.00794 | Test Loss: 0.00824 | Valid loss: 0.00790\n",
      "Epoch: 8800 | Loss: 0.00698 | Test Loss: 0.00723 | Valid loss: 0.00697\n",
      "Epoch: 9000 | Loss: 0.00612 | Test Loss: 0.00632 | Valid loss: 0.00613\n",
      "Epoch: 9200 | Loss: 0.00542 | Test Loss: 0.00557 | Valid loss: 0.00544\n",
      "Epoch: 9400 | Loss: 0.00489 | Test Loss: 0.00500 | Valid loss: 0.00492\n",
      "Epoch: 9600 | Loss: 0.00452 | Test Loss: 0.00459 | Valid loss: 0.00456\n",
      "Epoch: 9800 | Loss: 0.00424 | Test Loss: 0.00428 | Valid loss: 0.00426\n",
      "Epoch: 10000 | Loss: 0.00405 | Test Loss: 0.00406 | Valid loss: 0.00406\n",
      "Epoch: 10200 | Loss: 0.00391 | Test Loss: 0.00390 | Valid loss: 0.00390\n",
      "Epoch: 10400 | Loss: 0.00381 | Test Loss: 0.00380 | Valid loss: 0.00383\n",
      "Epoch: 10600 | Loss: 0.00371 | Test Loss: 0.00368 | Valid loss: 0.00370\n",
      "Epoch: 10800 | Loss: 0.00365 | Test Loss: 0.00361 | Valid loss: 0.00363\n",
      "Epoch: 11000 | Loss: 0.00359 | Test Loss: 0.00354 | Valid loss: 0.00356\n",
      "Early stopping at epoch: 11152\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 3.88806 | Test Loss: 3.80991 | Valid loss: 3.79449\n",
      "Epoch: 200 | Loss: 0.01521 | Test Loss: 0.01513 | Valid loss: 0.01501\n",
      "Epoch: 400 | Loss: 0.01480 | Test Loss: 0.01477 | Valid loss: 0.01479\n",
      "Epoch: 600 | Loss: 0.01480 | Test Loss: 0.01477 | Valid loss: 0.01479\n",
      "Epoch: 800 | Loss: 0.01479 | Test Loss: 0.01477 | Valid loss: 0.01479\n",
      "Epoch: 1000 | Loss: 0.01479 | Test Loss: 0.01476 | Valid loss: 0.01478\n",
      "Epoch: 1200 | Loss: 0.01479 | Test Loss: 0.01476 | Valid loss: 0.01478\n",
      "Epoch: 1400 | Loss: 0.01479 | Test Loss: 0.01476 | Valid loss: 0.01478\n",
      "Epoch: 1600 | Loss: 0.01478 | Test Loss: 0.01475 | Valid loss: 0.01477\n",
      "Epoch: 1800 | Loss: 0.01478 | Test Loss: 0.01475 | Valid loss: 0.01477\n",
      "Epoch: 2000 | Loss: 0.01477 | Test Loss: 0.01474 | Valid loss: 0.01476\n",
      "Epoch: 2200 | Loss: 0.01477 | Test Loss: 0.01474 | Valid loss: 0.01476\n",
      "Epoch: 2400 | Loss: 0.01476 | Test Loss: 0.01473 | Valid loss: 0.01475\n",
      "Epoch: 2600 | Loss: 0.01475 | Test Loss: 0.01472 | Valid loss: 0.01475\n",
      "Epoch: 2800 | Loss: 0.01475 | Test Loss: 0.01472 | Valid loss: 0.01474\n",
      "Epoch: 3000 | Loss: 0.01474 | Test Loss: 0.01471 | Valid loss: 0.01473\n",
      "Epoch: 3200 | Loss: 0.01473 | Test Loss: 0.01470 | Valid loss: 0.01472\n",
      "Epoch: 3400 | Loss: 0.01472 | Test Loss: 0.01469 | Valid loss: 0.01471\n",
      "Epoch: 3600 | Loss: 0.01470 | Test Loss: 0.01468 | Valid loss: 0.01470\n",
      "Epoch: 3800 | Loss: 0.01469 | Test Loss: 0.01466 | Valid loss: 0.01468\n",
      "Epoch: 4000 | Loss: 0.01468 | Test Loss: 0.01465 | Valid loss: 0.01467\n",
      "Epoch: 4200 | Loss: 0.01466 | Test Loss: 0.01463 | Valid loss: 0.01465\n",
      "Epoch: 4400 | Loss: 0.01464 | Test Loss: 0.01462 | Valid loss: 0.01464\n",
      "Epoch: 4600 | Loss: 0.01463 | Test Loss: 0.01460 | Valid loss: 0.01462\n",
      "Epoch: 4800 | Loss: 0.01460 | Test Loss: 0.01458 | Valid loss: 0.01460\n",
      "Epoch: 5000 | Loss: 0.01458 | Test Loss: 0.01455 | Valid loss: 0.01457\n",
      "Epoch: 5200 | Loss: 0.01455 | Test Loss: 0.01453 | Valid loss: 0.01455\n",
      "Epoch: 5400 | Loss: 0.01452 | Test Loss: 0.01450 | Valid loss: 0.01452\n",
      "Epoch: 5600 | Loss: 0.01449 | Test Loss: 0.01446 | Valid loss: 0.01449\n",
      "Epoch: 5800 | Loss: 0.01446 | Test Loss: 0.01443 | Valid loss: 0.01445\n",
      "Epoch: 6000 | Loss: 0.01441 | Test Loss: 0.01439 | Valid loss: 0.01441\n",
      "Epoch: 6200 | Loss: 0.01437 | Test Loss: 0.01434 | Valid loss: 0.01436\n",
      "Epoch: 6400 | Loss: 0.01432 | Test Loss: 0.01429 | Valid loss: 0.01431\n",
      "Epoch: 6600 | Loss: 0.01425 | Test Loss: 0.01423 | Valid loss: 0.01425\n",
      "Epoch: 6800 | Loss: 0.01419 | Test Loss: 0.01416 | Valid loss: 0.01418\n",
      "Epoch: 7000 | Loss: 0.01410 | Test Loss: 0.01408 | Valid loss: 0.01410\n",
      "Epoch: 7200 | Loss: 0.01401 | Test Loss: 0.01398 | Valid loss: 0.01400\n",
      "Epoch: 7400 | Loss: 0.01389 | Test Loss: 0.01387 | Valid loss: 0.01389\n",
      "Epoch: 7600 | Loss: 0.01376 | Test Loss: 0.01373 | Valid loss: 0.01375\n",
      "Epoch: 7800 | Loss: 0.01359 | Test Loss: 0.01356 | Valid loss: 0.01358\n",
      "Epoch: 8000 | Loss: 0.01338 | Test Loss: 0.01335 | Valid loss: 0.01338\n",
      "Epoch: 8200 | Loss: 0.01313 | Test Loss: 0.01310 | Valid loss: 0.01312\n",
      "Epoch: 8400 | Loss: 0.01281 | Test Loss: 0.01278 | Valid loss: 0.01280\n",
      "Epoch: 8600 | Loss: 0.01241 | Test Loss: 0.01239 | Valid loss: 0.01241\n",
      "Epoch: 8800 | Loss: 0.01193 | Test Loss: 0.01190 | Valid loss: 0.01193\n",
      "Epoch: 9000 | Loss: 0.01134 | Test Loss: 0.01132 | Valid loss: 0.01134\n",
      "Epoch: 9200 | Loss: 0.01066 | Test Loss: 0.01064 | Valid loss: 0.01066\n",
      "Epoch: 9400 | Loss: 0.00989 | Test Loss: 0.00988 | Valid loss: 0.00990\n",
      "Epoch: 9600 | Loss: 0.00906 | Test Loss: 0.00906 | Valid loss: 0.00910\n",
      "Epoch: 9800 | Loss: 0.00824 | Test Loss: 0.00824 | Valid loss: 0.00829\n",
      "Epoch: 10000 | Loss: 0.00746 | Test Loss: 0.00748 | Valid loss: 0.00753\n",
      "Epoch: 10200 | Loss: 0.00678 | Test Loss: 0.00681 | Valid loss: 0.00688\n",
      "Epoch: 10400 | Loss: 0.00623 | Test Loss: 0.00626 | Valid loss: 0.00634\n",
      "Epoch: 10600 | Loss: 0.00581 | Test Loss: 0.00585 | Valid loss: 0.00595\n",
      "Epoch: 10800 | Loss: 0.00552 | Test Loss: 0.00556 | Valid loss: 0.00567\n",
      "Epoch: 11000 | Loss: 0.00532 | Test Loss: 0.00536 | Valid loss: 0.00547\n",
      "Epoch: 11200 | Loss: 0.00518 | Test Loss: 0.00522 | Valid loss: 0.00533\n",
      "Epoch: 11400 | Loss: 0.00507 | Test Loss: 0.00511 | Valid loss: 0.00523\n",
      "Epoch: 11600 | Loss: 0.00499 | Test Loss: 0.00502 | Valid loss: 0.00514\n",
      "Epoch: 11800 | Loss: 0.00492 | Test Loss: 0.00495 | Valid loss: 0.00507\n",
      "Epoch: 12000 | Loss: 0.00486 | Test Loss: 0.00488 | Valid loss: 0.00501\n",
      "Epoch: 12200 | Loss: 0.00481 | Test Loss: 0.00483 | Valid loss: 0.00495\n",
      "Epoch: 12400 | Loss: 0.00476 | Test Loss: 0.00478 | Valid loss: 0.00490\n",
      "Early stopping at epoch: 12600\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 1.99516 | Test Loss: 1.94270 | Valid loss: 1.93910\n",
      "Epoch: 200 | Loss: 0.01452 | Test Loss: 0.01499 | Valid loss: 0.01466\n",
      "Epoch: 400 | Loss: 0.01452 | Test Loss: 0.01499 | Valid loss: 0.01466\n",
      "Epoch: 600 | Loss: 0.01452 | Test Loss: 0.01499 | Valid loss: 0.01466\n",
      "Epoch: 800 | Loss: 0.01452 | Test Loss: 0.01499 | Valid loss: 0.01465\n",
      "Epoch: 1000 | Loss: 0.01451 | Test Loss: 0.01498 | Valid loss: 0.01465\n",
      "Epoch: 1200 | Loss: 0.01451 | Test Loss: 0.01498 | Valid loss: 0.01464\n",
      "Epoch: 1400 | Loss: 0.01450 | Test Loss: 0.01497 | Valid loss: 0.01464\n",
      "Epoch: 1600 | Loss: 0.01450 | Test Loss: 0.01497 | Valid loss: 0.01463\n",
      "Epoch: 1800 | Loss: 0.01449 | Test Loss: 0.01496 | Valid loss: 0.01463\n",
      "Epoch: 2000 | Loss: 0.01448 | Test Loss: 0.01495 | Valid loss: 0.01462\n",
      "Epoch: 2200 | Loss: 0.01447 | Test Loss: 0.01494 | Valid loss: 0.01461\n",
      "Epoch: 2400 | Loss: 0.01446 | Test Loss: 0.01493 | Valid loss: 0.01460\n",
      "Epoch: 2600 | Loss: 0.01445 | Test Loss: 0.01492 | Valid loss: 0.01459\n",
      "Epoch: 2800 | Loss: 0.01444 | Test Loss: 0.01491 | Valid loss: 0.01458\n",
      "Epoch: 3000 | Loss: 0.01443 | Test Loss: 0.01489 | Valid loss: 0.01456\n",
      "Epoch: 3200 | Loss: 0.01441 | Test Loss: 0.01488 | Valid loss: 0.01455\n",
      "Epoch: 3400 | Loss: 0.01440 | Test Loss: 0.01486 | Valid loss: 0.01453\n",
      "Epoch: 3600 | Loss: 0.01438 | Test Loss: 0.01484 | Valid loss: 0.01451\n",
      "Epoch: 3800 | Loss: 0.01436 | Test Loss: 0.01482 | Valid loss: 0.01449\n",
      "Epoch: 4000 | Loss: 0.01434 | Test Loss: 0.01480 | Valid loss: 0.01447\n",
      "Epoch: 4200 | Loss: 0.01431 | Test Loss: 0.01478 | Valid loss: 0.01445\n",
      "Epoch: 4400 | Loss: 0.01428 | Test Loss: 0.01475 | Valid loss: 0.01442\n",
      "Epoch: 4600 | Loss: 0.01425 | Test Loss: 0.01472 | Valid loss: 0.01439\n",
      "Epoch: 4800 | Loss: 0.01422 | Test Loss: 0.01468 | Valid loss: 0.01435\n",
      "Epoch: 5000 | Loss: 0.01418 | Test Loss: 0.01464 | Valid loss: 0.01431\n",
      "Epoch: 5200 | Loss: 0.01414 | Test Loss: 0.01460 | Valid loss: 0.01427\n",
      "Epoch: 5400 | Loss: 0.01409 | Test Loss: 0.01454 | Valid loss: 0.01422\n",
      "Epoch: 5600 | Loss: 0.01403 | Test Loss: 0.01449 | Valid loss: 0.01416\n",
      "Epoch: 5800 | Loss: 0.01397 | Test Loss: 0.01442 | Valid loss: 0.01409\n",
      "Epoch: 6000 | Loss: 0.01389 | Test Loss: 0.01434 | Valid loss: 0.01402\n",
      "Epoch: 6200 | Loss: 0.01380 | Test Loss: 0.01425 | Valid loss: 0.01392\n",
      "Epoch: 6400 | Loss: 0.01369 | Test Loss: 0.01413 | Valid loss: 0.01381\n",
      "Epoch: 6600 | Loss: 0.01356 | Test Loss: 0.01400 | Valid loss: 0.01368\n",
      "Epoch: 6800 | Loss: 0.01340 | Test Loss: 0.01383 | Valid loss: 0.01351\n",
      "Epoch: 7000 | Loss: 0.01319 | Test Loss: 0.01361 | Valid loss: 0.01330\n",
      "Epoch: 7200 | Loss: 0.01294 | Test Loss: 0.01335 | Valid loss: 0.01304\n",
      "Epoch: 7400 | Loss: 0.01261 | Test Loss: 0.01301 | Valid loss: 0.01270\n",
      "Epoch: 7600 | Loss: 0.01220 | Test Loss: 0.01258 | Valid loss: 0.01228\n",
      "Epoch: 7800 | Loss: 0.01170 | Test Loss: 0.01206 | Valid loss: 0.01177\n",
      "Epoch: 8000 | Loss: 0.01109 | Test Loss: 0.01143 | Valid loss: 0.01115\n",
      "Epoch: 8200 | Loss: 0.01040 | Test Loss: 0.01071 | Valid loss: 0.01045\n",
      "Epoch: 8400 | Loss: 0.00965 | Test Loss: 0.00993 | Valid loss: 0.00969\n",
      "Epoch: 8600 | Loss: 0.00889 | Test Loss: 0.00913 | Valid loss: 0.00892\n",
      "Epoch: 8800 | Loss: 0.00815 | Test Loss: 0.00837 | Valid loss: 0.00819\n",
      "Epoch: 9000 | Loss: 0.00750 | Test Loss: 0.00769 | Valid loss: 0.00754\n",
      "Epoch: 9200 | Loss: 0.00694 | Test Loss: 0.00712 | Valid loss: 0.00700\n",
      "Epoch: 9400 | Loss: 0.00649 | Test Loss: 0.00666 | Valid loss: 0.00657\n",
      "Epoch: 9600 | Loss: 0.00615 | Test Loss: 0.00632 | Valid loss: 0.00624\n",
      "Epoch: 9800 | Loss: 0.00590 | Test Loss: 0.00605 | Valid loss: 0.00600\n",
      "Epoch: 10000 | Loss: 0.00570 | Test Loss: 0.00585 | Valid loss: 0.00581\n",
      "Epoch: 10200 | Loss: 0.00554 | Test Loss: 0.00569 | Valid loss: 0.00565\n",
      "Epoch: 10400 | Loss: 0.00541 | Test Loss: 0.00555 | Valid loss: 0.00553\n",
      "Epoch: 10600 | Loss: 0.00529 | Test Loss: 0.00544 | Valid loss: 0.00542\n",
      "Epoch: 10800 | Loss: 0.00520 | Test Loss: 0.00534 | Valid loss: 0.00532\n",
      "Epoch: 11000 | Loss: 0.00511 | Test Loss: 0.00525 | Valid loss: 0.00524\n",
      "Epoch: 11200 | Loss: 0.00503 | Test Loss: 0.00517 | Valid loss: 0.00517\n",
      "Epoch: 11400 | Loss: 0.00496 | Test Loss: 0.00509 | Valid loss: 0.00510\n",
      "Epoch: 11600 | Loss: 0.00490 | Test Loss: 0.00503 | Valid loss: 0.00504\n",
      "Epoch: 11800 | Loss: 0.00484 | Test Loss: 0.00496 | Valid loss: 0.00498\n",
      "Epoch: 12000 | Loss: 0.00479 | Test Loss: 0.00491 | Valid loss: 0.00492\n",
      "Early stopping at epoch: 12162\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 2.32866 | Test Loss: 2.27477 | Valid loss: 2.27677\n",
      "Epoch: 200 | Loss: 0.01479 | Test Loss: 0.01489 | Valid loss: 0.01515\n",
      "Epoch: 400 | Loss: 0.01479 | Test Loss: 0.01489 | Valid loss: 0.01515\n",
      "Epoch: 600 | Loss: 0.01479 | Test Loss: 0.01489 | Valid loss: 0.01515\n",
      "Epoch: 800 | Loss: 0.01479 | Test Loss: 0.01488 | Valid loss: 0.01515\n",
      "Epoch: 1000 | Loss: 0.01478 | Test Loss: 0.01488 | Valid loss: 0.01514\n",
      "Epoch: 1200 | Loss: 0.01478 | Test Loss: 0.01488 | Valid loss: 0.01514\n",
      "Epoch: 1400 | Loss: 0.01477 | Test Loss: 0.01487 | Valid loss: 0.01513\n",
      "Epoch: 1600 | Loss: 0.01477 | Test Loss: 0.01486 | Valid loss: 0.01513\n",
      "Epoch: 1800 | Loss: 0.01476 | Test Loss: 0.01486 | Valid loss: 0.01512\n",
      "Epoch: 2000 | Loss: 0.01475 | Test Loss: 0.01485 | Valid loss: 0.01511\n",
      "Epoch: 2200 | Loss: 0.01475 | Test Loss: 0.01484 | Valid loss: 0.01511\n",
      "Epoch: 2400 | Loss: 0.01474 | Test Loss: 0.01483 | Valid loss: 0.01510\n",
      "Epoch: 2600 | Loss: 0.01473 | Test Loss: 0.01482 | Valid loss: 0.01509\n",
      "Epoch: 2800 | Loss: 0.01472 | Test Loss: 0.01481 | Valid loss: 0.01508\n",
      "Epoch: 3000 | Loss: 0.01470 | Test Loss: 0.01480 | Valid loss: 0.01506\n",
      "Epoch: 3200 | Loss: 0.01469 | Test Loss: 0.01479 | Valid loss: 0.01505\n",
      "Epoch: 3400 | Loss: 0.01468 | Test Loss: 0.01477 | Valid loss: 0.01503\n",
      "Epoch: 3600 | Loss: 0.01466 | Test Loss: 0.01475 | Valid loss: 0.01502\n",
      "Epoch: 3800 | Loss: 0.01464 | Test Loss: 0.01474 | Valid loss: 0.01500\n",
      "Epoch: 4000 | Loss: 0.01462 | Test Loss: 0.01472 | Valid loss: 0.01498\n",
      "Epoch: 4200 | Loss: 0.01460 | Test Loss: 0.01469 | Valid loss: 0.01496\n",
      "Epoch: 4400 | Loss: 0.01457 | Test Loss: 0.01467 | Valid loss: 0.01493\n",
      "Epoch: 4600 | Loss: 0.01455 | Test Loss: 0.01464 | Valid loss: 0.01490\n",
      "Epoch: 4800 | Loss: 0.01452 | Test Loss: 0.01461 | Valid loss: 0.01487\n",
      "Epoch: 5000 | Loss: 0.01448 | Test Loss: 0.01457 | Valid loss: 0.01484\n",
      "Epoch: 5200 | Loss: 0.01445 | Test Loss: 0.01454 | Valid loss: 0.01480\n",
      "Epoch: 5400 | Loss: 0.01440 | Test Loss: 0.01449 | Valid loss: 0.01476\n",
      "Epoch: 5600 | Loss: 0.01435 | Test Loss: 0.01444 | Valid loss: 0.01471\n",
      "Epoch: 5800 | Loss: 0.01430 | Test Loss: 0.01439 | Valid loss: 0.01465\n",
      "Epoch: 6000 | Loss: 0.01424 | Test Loss: 0.01432 | Valid loss: 0.01458\n",
      "Epoch: 6200 | Loss: 0.01416 | Test Loss: 0.01425 | Valid loss: 0.01451\n",
      "Epoch: 6400 | Loss: 0.01407 | Test Loss: 0.01415 | Valid loss: 0.01442\n",
      "Epoch: 6600 | Loss: 0.01397 | Test Loss: 0.01405 | Valid loss: 0.01431\n",
      "Epoch: 6800 | Loss: 0.01384 | Test Loss: 0.01391 | Valid loss: 0.01417\n",
      "Epoch: 7000 | Loss: 0.01368 | Test Loss: 0.01375 | Valid loss: 0.01401\n",
      "Epoch: 7200 | Loss: 0.01348 | Test Loss: 0.01355 | Valid loss: 0.01381\n",
      "Epoch: 7400 | Loss: 0.01323 | Test Loss: 0.01329 | Valid loss: 0.01355\n",
      "Epoch: 7600 | Loss: 0.01292 | Test Loss: 0.01297 | Valid loss: 0.01323\n",
      "Epoch: 7800 | Loss: 0.01252 | Test Loss: 0.01257 | Valid loss: 0.01282\n",
      "Epoch: 8000 | Loss: 0.01204 | Test Loss: 0.01208 | Valid loss: 0.01232\n",
      "Epoch: 8200 | Loss: 0.01146 | Test Loss: 0.01149 | Valid loss: 0.01173\n",
      "Epoch: 8400 | Loss: 0.01080 | Test Loss: 0.01081 | Valid loss: 0.01105\n",
      "Epoch: 8600 | Loss: 0.01008 | Test Loss: 0.01006 | Valid loss: 0.01030\n",
      "Epoch: 8800 | Loss: 0.00932 | Test Loss: 0.00929 | Valid loss: 0.00953\n",
      "Epoch: 9000 | Loss: 0.00857 | Test Loss: 0.00852 | Valid loss: 0.00876\n",
      "Epoch: 9200 | Loss: 0.00786 | Test Loss: 0.00779 | Valid loss: 0.00804\n",
      "Epoch: 9400 | Loss: 0.00722 | Test Loss: 0.00714 | Valid loss: 0.00739\n",
      "Epoch: 9600 | Loss: 0.00667 | Test Loss: 0.00659 | Valid loss: 0.00684\n",
      "Epoch: 9800 | Loss: 0.00623 | Test Loss: 0.00615 | Valid loss: 0.00641\n",
      "Epoch: 10000 | Loss: 0.00589 | Test Loss: 0.00581 | Valid loss: 0.00605\n",
      "Epoch: 10200 | Loss: 0.00562 | Test Loss: 0.00554 | Valid loss: 0.00577\n",
      "Epoch: 10400 | Loss: 0.00540 | Test Loss: 0.00533 | Valid loss: 0.00555\n",
      "Epoch: 10600 | Loss: 0.00522 | Test Loss: 0.00516 | Valid loss: 0.00538\n",
      "Epoch: 10800 | Loss: 0.00508 | Test Loss: 0.00502 | Valid loss: 0.00522\n",
      "Epoch: 11000 | Loss: 0.00496 | Test Loss: 0.00491 | Valid loss: 0.00510\n",
      "Epoch: 11200 | Loss: 0.00485 | Test Loss: 0.00480 | Valid loss: 0.00499\n",
      "Epoch: 11400 | Loss: 0.00476 | Test Loss: 0.00471 | Valid loss: 0.00489\n",
      "Epoch: 11600 | Loss: 0.00467 | Test Loss: 0.00463 | Valid loss: 0.00480\n",
      "Epoch: 11800 | Loss: 0.00460 | Test Loss: 0.00456 | Valid loss: 0.00472\n",
      "Epoch: 12000 | Loss: 0.00453 | Test Loss: 0.00449 | Valid loss: 0.00465\n",
      "Early stopping at epoch: 12175\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 3.26282 | Test Loss: 3.19995 | Valid loss: 3.19389\n",
      "Epoch: 200 | Loss: 0.01478 | Test Loss: 0.01520 | Valid loss: 0.01491\n",
      "Epoch: 400 | Loss: 0.01476 | Test Loss: 0.01516 | Valid loss: 0.01489\n",
      "Epoch: 600 | Loss: 0.01476 | Test Loss: 0.01516 | Valid loss: 0.01488\n",
      "Epoch: 800 | Loss: 0.01475 | Test Loss: 0.01516 | Valid loss: 0.01488\n",
      "Epoch: 1000 | Loss: 0.01475 | Test Loss: 0.01515 | Valid loss: 0.01488\n",
      "Epoch: 1200 | Loss: 0.01475 | Test Loss: 0.01515 | Valid loss: 0.01487\n",
      "Epoch: 1400 | Loss: 0.01474 | Test Loss: 0.01515 | Valid loss: 0.01487\n",
      "Epoch: 1600 | Loss: 0.01474 | Test Loss: 0.01514 | Valid loss: 0.01487\n",
      "Epoch: 1800 | Loss: 0.01473 | Test Loss: 0.01514 | Valid loss: 0.01486\n",
      "Epoch: 2000 | Loss: 0.01473 | Test Loss: 0.01513 | Valid loss: 0.01485\n",
      "Epoch: 2200 | Loss: 0.01472 | Test Loss: 0.01512 | Valid loss: 0.01485\n",
      "Epoch: 2400 | Loss: 0.01471 | Test Loss: 0.01512 | Valid loss: 0.01484\n",
      "Epoch: 2600 | Loss: 0.01471 | Test Loss: 0.01511 | Valid loss: 0.01483\n",
      "Epoch: 2800 | Loss: 0.01470 | Test Loss: 0.01510 | Valid loss: 0.01482\n",
      "Epoch: 3000 | Loss: 0.01469 | Test Loss: 0.01509 | Valid loss: 0.01481\n",
      "Epoch: 3200 | Loss: 0.01468 | Test Loss: 0.01508 | Valid loss: 0.01480\n",
      "Epoch: 3400 | Loss: 0.01466 | Test Loss: 0.01506 | Valid loss: 0.01479\n",
      "Epoch: 3600 | Loss: 0.01465 | Test Loss: 0.01505 | Valid loss: 0.01478\n",
      "Epoch: 3800 | Loss: 0.01464 | Test Loss: 0.01503 | Valid loss: 0.01476\n",
      "Epoch: 4000 | Loss: 0.01462 | Test Loss: 0.01502 | Valid loss: 0.01475\n",
      "Epoch: 4200 | Loss: 0.01460 | Test Loss: 0.01500 | Valid loss: 0.01473\n",
      "Epoch: 4400 | Loss: 0.01458 | Test Loss: 0.01498 | Valid loss: 0.01471\n",
      "Epoch: 4600 | Loss: 0.01456 | Test Loss: 0.01495 | Valid loss: 0.01469\n",
      "Epoch: 4800 | Loss: 0.01454 | Test Loss: 0.01493 | Valid loss: 0.01466\n",
      "Epoch: 5000 | Loss: 0.01451 | Test Loss: 0.01490 | Valid loss: 0.01463\n",
      "Epoch: 5200 | Loss: 0.01448 | Test Loss: 0.01487 | Valid loss: 0.01460\n",
      "Epoch: 5400 | Loss: 0.01445 | Test Loss: 0.01484 | Valid loss: 0.01457\n",
      "Epoch: 5600 | Loss: 0.01441 | Test Loss: 0.01480 | Valid loss: 0.01453\n",
      "Epoch: 5800 | Loss: 0.01437 | Test Loss: 0.01475 | Valid loss: 0.01449\n",
      "Epoch: 6000 | Loss: 0.01432 | Test Loss: 0.01470 | Valid loss: 0.01444\n",
      "Epoch: 6200 | Loss: 0.01427 | Test Loss: 0.01465 | Valid loss: 0.01439\n",
      "Epoch: 6400 | Loss: 0.01421 | Test Loss: 0.01459 | Valid loss: 0.01433\n",
      "Epoch: 6600 | Loss: 0.01414 | Test Loss: 0.01451 | Valid loss: 0.01426\n",
      "Epoch: 6800 | Loss: 0.01406 | Test Loss: 0.01443 | Valid loss: 0.01418\n",
      "Epoch: 7000 | Loss: 0.01396 | Test Loss: 0.01433 | Valid loss: 0.01408\n",
      "Epoch: 7200 | Loss: 0.01385 | Test Loss: 0.01421 | Valid loss: 0.01397\n",
      "Epoch: 7400 | Loss: 0.01371 | Test Loss: 0.01407 | Valid loss: 0.01383\n",
      "Epoch: 7600 | Loss: 0.01355 | Test Loss: 0.01390 | Valid loss: 0.01367\n",
      "Epoch: 7800 | Loss: 0.01335 | Test Loss: 0.01368 | Valid loss: 0.01346\n",
      "Epoch: 8000 | Loss: 0.01310 | Test Loss: 0.01342 | Valid loss: 0.01321\n",
      "Epoch: 8200 | Loss: 0.01278 | Test Loss: 0.01310 | Valid loss: 0.01290\n",
      "Epoch: 8400 | Loss: 0.01240 | Test Loss: 0.01269 | Valid loss: 0.01251\n",
      "Epoch: 8600 | Loss: 0.01194 | Test Loss: 0.01220 | Valid loss: 0.01204\n",
      "Epoch: 8800 | Loss: 0.01139 | Test Loss: 0.01163 | Valid loss: 0.01149\n",
      "Epoch: 9000 | Loss: 0.01076 | Test Loss: 0.01096 | Valid loss: 0.01086\n",
      "Epoch: 9200 | Loss: 0.01007 | Test Loss: 0.01023 | Valid loss: 0.01016\n",
      "Epoch: 9400 | Loss: 0.00934 | Test Loss: 0.00947 | Valid loss: 0.00943\n",
      "Epoch: 9600 | Loss: 0.00860 | Test Loss: 0.00868 | Valid loss: 0.00869\n",
      "Epoch: 9800 | Loss: 0.00788 | Test Loss: 0.00792 | Valid loss: 0.00796\n",
      "Epoch: 10000 | Loss: 0.00719 | Test Loss: 0.00721 | Valid loss: 0.00728\n",
      "Epoch: 10200 | Loss: 0.00657 | Test Loss: 0.00657 | Valid loss: 0.00666\n",
      "Epoch: 10400 | Loss: 0.00606 | Test Loss: 0.00605 | Valid loss: 0.00614\n",
      "Epoch: 10600 | Loss: 0.00566 | Test Loss: 0.00565 | Valid loss: 0.00574\n",
      "Epoch: 10800 | Loss: 0.00536 | Test Loss: 0.00535 | Valid loss: 0.00543\n",
      "Epoch: 11000 | Loss: 0.00513 | Test Loss: 0.00512 | Valid loss: 0.00520\n",
      "Epoch: 11200 | Loss: 0.00495 | Test Loss: 0.00494 | Valid loss: 0.00501\n",
      "Epoch: 11400 | Loss: 0.00481 | Test Loss: 0.00480 | Valid loss: 0.00486\n",
      "Epoch: 11600 | Loss: 0.00469 | Test Loss: 0.00470 | Valid loss: 0.00474\n",
      "Epoch: 11800 | Loss: 0.00459 | Test Loss: 0.00460 | Valid loss: 0.00463\n",
      "Epoch: 12000 | Loss: 0.00451 | Test Loss: 0.00452 | Valid loss: 0.00454\n",
      "Epoch: 12200 | Loss: 0.00444 | Test Loss: 0.00444 | Valid loss: 0.00446\n",
      "Epoch: 12400 | Loss: 0.00437 | Test Loss: 0.00438 | Valid loss: 0.00439\n",
      "Epoch: 12600 | Loss: 0.00431 | Test Loss: 0.00433 | Valid loss: 0.00433\n",
      "Epoch: 12800 | Loss: 0.00426 | Test Loss: 0.00427 | Valid loss: 0.00427\n",
      "Epoch: 13000 | Loss: 0.00421 | Test Loss: 0.00422 | Valid loss: 0.00421\n",
      "Epoch: 13200 | Loss: 0.00416 | Test Loss: 0.00418 | Valid loss: 0.00416\n",
      "Epoch: 13400 | Loss: 0.00412 | Test Loss: 0.00414 | Valid loss: 0.00411\n",
      "Epoch: 13600 | Loss: 0.00408 | Test Loss: 0.00410 | Valid loss: 0.00407\n",
      "Epoch: 13800 | Loss: 0.00404 | Test Loss: 0.00406 | Valid loss: 0.00403\n",
      "Epoch: 14000 | Loss: 0.00400 | Test Loss: 0.00403 | Valid loss: 0.00399\n",
      "Epoch: 14200 | Loss: 0.00396 | Test Loss: 0.00399 | Valid loss: 0.00395\n",
      "Epoch: 14400 | Loss: 0.00393 | Test Loss: 0.00396 | Valid loss: 0.00391\n",
      "Epoch: 14600 | Loss: 0.00390 | Test Loss: 0.00393 | Valid loss: 0.00388\n",
      "Early stopping at epoch: 14689\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 2.33034 | Test Loss: 2.29016 | Valid loss: 2.27201\n",
      "Epoch: 200 | Loss: 0.01474 | Test Loss: 0.01523 | Valid loss: 0.01478\n",
      "Epoch: 400 | Loss: 0.01473 | Test Loss: 0.01524 | Valid loss: 0.01477\n",
      "Epoch: 600 | Loss: 0.01473 | Test Loss: 0.01524 | Valid loss: 0.01477\n",
      "Epoch: 800 | Loss: 0.01473 | Test Loss: 0.01523 | Valid loss: 0.01477\n",
      "Epoch: 1000 | Loss: 0.01472 | Test Loss: 0.01523 | Valid loss: 0.01477\n",
      "Epoch: 1200 | Loss: 0.01472 | Test Loss: 0.01522 | Valid loss: 0.01476\n",
      "Epoch: 1400 | Loss: 0.01471 | Test Loss: 0.01522 | Valid loss: 0.01476\n",
      "Epoch: 1600 | Loss: 0.01471 | Test Loss: 0.01521 | Valid loss: 0.01475\n",
      "Epoch: 1800 | Loss: 0.01470 | Test Loss: 0.01521 | Valid loss: 0.01474\n",
      "Epoch: 2000 | Loss: 0.01469 | Test Loss: 0.01520 | Valid loss: 0.01474\n",
      "Epoch: 2200 | Loss: 0.01469 | Test Loss: 0.01519 | Valid loss: 0.01473\n",
      "Epoch: 2400 | Loss: 0.01468 | Test Loss: 0.01518 | Valid loss: 0.01472\n",
      "Epoch: 2600 | Loss: 0.01467 | Test Loss: 0.01517 | Valid loss: 0.01471\n",
      "Epoch: 2800 | Loss: 0.01466 | Test Loss: 0.01516 | Valid loss: 0.01470\n",
      "Epoch: 3000 | Loss: 0.01464 | Test Loss: 0.01514 | Valid loss: 0.01469\n",
      "Epoch: 3200 | Loss: 0.01463 | Test Loss: 0.01513 | Valid loss: 0.01467\n",
      "Epoch: 3400 | Loss: 0.01461 | Test Loss: 0.01511 | Valid loss: 0.01466\n",
      "Epoch: 3600 | Loss: 0.01460 | Test Loss: 0.01509 | Valid loss: 0.01464\n",
      "Epoch: 3800 | Loss: 0.01458 | Test Loss: 0.01507 | Valid loss: 0.01462\n",
      "Epoch: 4000 | Loss: 0.01456 | Test Loss: 0.01505 | Valid loss: 0.01460\n",
      "Epoch: 4200 | Loss: 0.01453 | Test Loss: 0.01503 | Valid loss: 0.01458\n",
      "Epoch: 4400 | Loss: 0.01451 | Test Loss: 0.01500 | Valid loss: 0.01455\n",
      "Epoch: 4600 | Loss: 0.01448 | Test Loss: 0.01497 | Valid loss: 0.01452\n",
      "Epoch: 4800 | Loss: 0.01445 | Test Loss: 0.01494 | Valid loss: 0.01449\n",
      "Epoch: 5000 | Loss: 0.01441 | Test Loss: 0.01490 | Valid loss: 0.01446\n",
      "Epoch: 5200 | Loss: 0.01437 | Test Loss: 0.01486 | Valid loss: 0.01442\n",
      "Epoch: 5400 | Loss: 0.01433 | Test Loss: 0.01482 | Valid loss: 0.01438\n",
      "Epoch: 5600 | Loss: 0.01428 | Test Loss: 0.01476 | Valid loss: 0.01433\n",
      "Epoch: 5800 | Loss: 0.01423 | Test Loss: 0.01471 | Valid loss: 0.01427\n",
      "Epoch: 6000 | Loss: 0.01416 | Test Loss: 0.01464 | Valid loss: 0.01421\n",
      "Epoch: 6200 | Loss: 0.01409 | Test Loss: 0.01456 | Valid loss: 0.01414\n",
      "Epoch: 6400 | Loss: 0.01401 | Test Loss: 0.01448 | Valid loss: 0.01406\n",
      "Epoch: 6600 | Loss: 0.01391 | Test Loss: 0.01437 | Valid loss: 0.01396\n",
      "Epoch: 6800 | Loss: 0.01380 | Test Loss: 0.01425 | Valid loss: 0.01385\n",
      "Epoch: 7000 | Loss: 0.01366 | Test Loss: 0.01411 | Valid loss: 0.01372\n",
      "Epoch: 7200 | Loss: 0.01350 | Test Loss: 0.01394 | Valid loss: 0.01355\n",
      "Epoch: 7400 | Loss: 0.01329 | Test Loss: 0.01372 | Valid loss: 0.01336\n",
      "Epoch: 7600 | Loss: 0.01305 | Test Loss: 0.01346 | Valid loss: 0.01311\n",
      "Epoch: 7800 | Loss: 0.01274 | Test Loss: 0.01314 | Valid loss: 0.01281\n",
      "Epoch: 8000 | Loss: 0.01237 | Test Loss: 0.01274 | Valid loss: 0.01244\n",
      "Epoch: 8200 | Loss: 0.01191 | Test Loss: 0.01226 | Valid loss: 0.01199\n",
      "Epoch: 8400 | Loss: 0.01137 | Test Loss: 0.01169 | Valid loss: 0.01146\n",
      "Epoch: 8600 | Loss: 0.01075 | Test Loss: 0.01104 | Valid loss: 0.01084\n",
      "Epoch: 8800 | Loss: 0.01006 | Test Loss: 0.01032 | Valid loss: 0.01017\n",
      "Epoch: 9000 | Loss: 0.00933 | Test Loss: 0.00955 | Valid loss: 0.00944\n",
      "Epoch: 9200 | Loss: 0.00857 | Test Loss: 0.00876 | Valid loss: 0.00869\n",
      "Epoch: 9400 | Loss: 0.00781 | Test Loss: 0.00798 | Valid loss: 0.00793\n",
      "Epoch: 9600 | Loss: 0.00708 | Test Loss: 0.00723 | Valid loss: 0.00720\n",
      "Epoch: 9800 | Loss: 0.00641 | Test Loss: 0.00655 | Valid loss: 0.00653\n",
      "Epoch: 10000 | Loss: 0.00586 | Test Loss: 0.00598 | Valid loss: 0.00597\n",
      "Epoch: 10200 | Loss: 0.00545 | Test Loss: 0.00556 | Valid loss: 0.00555\n",
      "Epoch: 10400 | Loss: 0.00515 | Test Loss: 0.00525 | Valid loss: 0.00524\n",
      "Epoch: 10600 | Loss: 0.00493 | Test Loss: 0.00503 | Valid loss: 0.00502\n",
      "Epoch: 10800 | Loss: 0.00477 | Test Loss: 0.00487 | Valid loss: 0.00485\n",
      "Epoch: 11000 | Loss: 0.00465 | Test Loss: 0.00475 | Valid loss: 0.00472\n",
      "Epoch: 11200 | Loss: 0.00456 | Test Loss: 0.00465 | Valid loss: 0.00462\n",
      "Epoch: 11400 | Loss: 0.00449 | Test Loss: 0.00458 | Valid loss: 0.00454\n",
      "Epoch: 11600 | Loss: 0.00443 | Test Loss: 0.00451 | Valid loss: 0.00448\n",
      "Epoch: 11800 | Loss: 0.00437 | Test Loss: 0.00446 | Valid loss: 0.00442\n",
      "Epoch: 12000 | Loss: 0.00433 | Test Loss: 0.00441 | Valid loss: 0.00437\n",
      "Epoch: 12200 | Loss: 0.00429 | Test Loss: 0.00437 | Valid loss: 0.00433\n",
      "Epoch: 12400 | Loss: 0.00425 | Test Loss: 0.00433 | Valid loss: 0.00428\n",
      "Epoch: 12600 | Loss: 0.00422 | Test Loss: 0.00431 | Valid loss: 0.00425\n",
      "Epoch: 12800 | Loss: 0.00418 | Test Loss: 0.00426 | Valid loss: 0.00421\n",
      "Epoch: 13000 | Loss: 0.00415 | Test Loss: 0.00423 | Valid loss: 0.00418\n",
      "Epoch: 13200 | Loss: 0.00412 | Test Loss: 0.00420 | Valid loss: 0.00415\n",
      "Early stopping at epoch: 13343\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 1.50463 | Test Loss: 1.47027 | Valid loss: 1.46428\n",
      "Epoch: 200 | Loss: 0.01474 | Test Loss: 0.01494 | Valid loss: 0.01477\n",
      "Epoch: 400 | Loss: 0.01474 | Test Loss: 0.01494 | Valid loss: 0.01477\n",
      "Epoch: 600 | Loss: 0.01474 | Test Loss: 0.01493 | Valid loss: 0.01476\n",
      "Epoch: 800 | Loss: 0.01473 | Test Loss: 0.01493 | Valid loss: 0.01476\n",
      "Epoch: 1000 | Loss: 0.01473 | Test Loss: 0.01492 | Valid loss: 0.01475\n",
      "Epoch: 1200 | Loss: 0.01472 | Test Loss: 0.01492 | Valid loss: 0.01475\n",
      "Epoch: 1400 | Loss: 0.01472 | Test Loss: 0.01491 | Valid loss: 0.01474\n",
      "Epoch: 1600 | Loss: 0.01471 | Test Loss: 0.01490 | Valid loss: 0.01473\n",
      "Epoch: 1800 | Loss: 0.01470 | Test Loss: 0.01489 | Valid loss: 0.01472\n",
      "Epoch: 2000 | Loss: 0.01469 | Test Loss: 0.01488 | Valid loss: 0.01471\n",
      "Epoch: 2200 | Loss: 0.01468 | Test Loss: 0.01487 | Valid loss: 0.01470\n",
      "Epoch: 2400 | Loss: 0.01467 | Test Loss: 0.01486 | Valid loss: 0.01469\n",
      "Epoch: 2600 | Loss: 0.01465 | Test Loss: 0.01485 | Valid loss: 0.01468\n",
      "Epoch: 2800 | Loss: 0.01464 | Test Loss: 0.01483 | Valid loss: 0.01466\n",
      "Epoch: 3000 | Loss: 0.01462 | Test Loss: 0.01481 | Valid loss: 0.01465\n",
      "Epoch: 3200 | Loss: 0.01460 | Test Loss: 0.01480 | Valid loss: 0.01463\n",
      "Epoch: 3400 | Loss: 0.01458 | Test Loss: 0.01477 | Valid loss: 0.01461\n",
      "Epoch: 3600 | Loss: 0.01456 | Test Loss: 0.01475 | Valid loss: 0.01458\n",
      "Epoch: 3800 | Loss: 0.01453 | Test Loss: 0.01473 | Valid loss: 0.01456\n",
      "Epoch: 4000 | Loss: 0.01451 | Test Loss: 0.01470 | Valid loss: 0.01453\n",
      "Epoch: 4200 | Loss: 0.01448 | Test Loss: 0.01467 | Valid loss: 0.01450\n",
      "Epoch: 4400 | Loss: 0.01444 | Test Loss: 0.01463 | Valid loss: 0.01446\n",
      "Epoch: 4600 | Loss: 0.01440 | Test Loss: 0.01459 | Valid loss: 0.01443\n",
      "Epoch: 4800 | Loss: 0.01436 | Test Loss: 0.01455 | Valid loss: 0.01438\n",
      "Epoch: 5000 | Loss: 0.01431 | Test Loss: 0.01450 | Valid loss: 0.01433\n",
      "Epoch: 5200 | Loss: 0.01425 | Test Loss: 0.01444 | Valid loss: 0.01428\n",
      "Epoch: 5400 | Loss: 0.01419 | Test Loss: 0.01438 | Valid loss: 0.01421\n",
      "Epoch: 5600 | Loss: 0.01412 | Test Loss: 0.01430 | Valid loss: 0.01414\n",
      "Epoch: 5800 | Loss: 0.01403 | Test Loss: 0.01421 | Valid loss: 0.01405\n",
      "Epoch: 6000 | Loss: 0.01393 | Test Loss: 0.01411 | Valid loss: 0.01395\n",
      "Epoch: 6200 | Loss: 0.01380 | Test Loss: 0.01399 | Valid loss: 0.01382\n",
      "Epoch: 6400 | Loss: 0.01365 | Test Loss: 0.01384 | Valid loss: 0.01367\n",
      "Epoch: 6600 | Loss: 0.01347 | Test Loss: 0.01365 | Valid loss: 0.01349\n",
      "Epoch: 6800 | Loss: 0.01324 | Test Loss: 0.01342 | Valid loss: 0.01326\n",
      "Epoch: 7000 | Loss: 0.01295 | Test Loss: 0.01313 | Valid loss: 0.01297\n",
      "Epoch: 7200 | Loss: 0.01259 | Test Loss: 0.01277 | Valid loss: 0.01261\n",
      "Epoch: 7400 | Loss: 0.01215 | Test Loss: 0.01233 | Valid loss: 0.01217\n",
      "Epoch: 7600 | Loss: 0.01163 | Test Loss: 0.01181 | Valid loss: 0.01164\n",
      "Epoch: 7800 | Loss: 0.01102 | Test Loss: 0.01120 | Valid loss: 0.01103\n",
      "Epoch: 8000 | Loss: 0.01034 | Test Loss: 0.01053 | Valid loss: 0.01035\n",
      "Epoch: 8200 | Loss: 0.00961 | Test Loss: 0.00980 | Valid loss: 0.00962\n",
      "Epoch: 8400 | Loss: 0.00885 | Test Loss: 0.00905 | Valid loss: 0.00886\n",
      "Epoch: 8600 | Loss: 0.00809 | Test Loss: 0.00829 | Valid loss: 0.00809\n",
      "Epoch: 8800 | Loss: 0.00734 | Test Loss: 0.00754 | Valid loss: 0.00734\n",
      "Epoch: 9000 | Loss: 0.00665 | Test Loss: 0.00685 | Valid loss: 0.00665\n",
      "Epoch: 9200 | Loss: 0.00606 | Test Loss: 0.00625 | Valid loss: 0.00606\n",
      "Epoch: 9400 | Loss: 0.00560 | Test Loss: 0.00578 | Valid loss: 0.00560\n",
      "Epoch: 9600 | Loss: 0.00527 | Test Loss: 0.00544 | Valid loss: 0.00526\n",
      "Epoch: 9800 | Loss: 0.00504 | Test Loss: 0.00520 | Valid loss: 0.00503\n",
      "Epoch: 10000 | Loss: 0.00487 | Test Loss: 0.00503 | Valid loss: 0.00486\n",
      "Epoch: 10200 | Loss: 0.00476 | Test Loss: 0.00490 | Valid loss: 0.00474\n",
      "Epoch: 10400 | Loss: 0.00467 | Test Loss: 0.00481 | Valid loss: 0.00466\n",
      "Epoch: 10600 | Loss: 0.00461 | Test Loss: 0.00474 | Valid loss: 0.00460\n",
      "Epoch: 10800 | Loss: 0.00455 | Test Loss: 0.00468 | Valid loss: 0.00454\n",
      "Epoch: 11000 | Loss: 0.00451 | Test Loss: 0.00463 | Valid loss: 0.00450\n",
      "Epoch: 11200 | Loss: 0.00447 | Test Loss: 0.00459 | Valid loss: 0.00446\n",
      "Early stopping at epoch: 11312\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.93481 | Test Loss: 1.89815 | Valid loss: 1.89264\n",
      "Epoch: 200 | Loss: 0.01460 | Test Loss: 0.01437 | Valid loss: 0.01474\n",
      "Epoch: 400 | Loss: 0.01459 | Test Loss: 0.01437 | Valid loss: 0.01474\n",
      "Epoch: 600 | Loss: 0.01459 | Test Loss: 0.01437 | Valid loss: 0.01474\n",
      "Epoch: 800 | Loss: 0.01459 | Test Loss: 0.01436 | Valid loss: 0.01473\n",
      "Epoch: 1000 | Loss: 0.01458 | Test Loss: 0.01436 | Valid loss: 0.01473\n",
      "Epoch: 1200 | Loss: 0.01458 | Test Loss: 0.01435 | Valid loss: 0.01473\n",
      "Epoch: 1400 | Loss: 0.01457 | Test Loss: 0.01435 | Valid loss: 0.01472\n",
      "Epoch: 1600 | Loss: 0.01457 | Test Loss: 0.01434 | Valid loss: 0.01471\n",
      "Epoch: 1800 | Loss: 0.01456 | Test Loss: 0.01433 | Valid loss: 0.01471\n",
      "Epoch: 2000 | Loss: 0.01455 | Test Loss: 0.01433 | Valid loss: 0.01470\n",
      "Epoch: 2200 | Loss: 0.01454 | Test Loss: 0.01432 | Valid loss: 0.01469\n",
      "Epoch: 2400 | Loss: 0.01453 | Test Loss: 0.01431 | Valid loss: 0.01468\n",
      "Epoch: 2600 | Loss: 0.01452 | Test Loss: 0.01430 | Valid loss: 0.01467\n",
      "Epoch: 2800 | Loss: 0.01451 | Test Loss: 0.01428 | Valid loss: 0.01465\n",
      "Epoch: 3000 | Loss: 0.01449 | Test Loss: 0.01427 | Valid loss: 0.01464\n",
      "Epoch: 3200 | Loss: 0.01448 | Test Loss: 0.01426 | Valid loss: 0.01462\n",
      "Epoch: 3400 | Loss: 0.01446 | Test Loss: 0.01424 | Valid loss: 0.01461\n",
      "Epoch: 3600 | Loss: 0.01444 | Test Loss: 0.01422 | Valid loss: 0.01459\n",
      "Epoch: 3800 | Loss: 0.01442 | Test Loss: 0.01420 | Valid loss: 0.01457\n",
      "Epoch: 4000 | Loss: 0.01440 | Test Loss: 0.01418 | Valid loss: 0.01454\n",
      "Epoch: 4200 | Loss: 0.01437 | Test Loss: 0.01415 | Valid loss: 0.01452\n",
      "Epoch: 4400 | Loss: 0.01434 | Test Loss: 0.01412 | Valid loss: 0.01449\n",
      "Epoch: 4600 | Loss: 0.01431 | Test Loss: 0.01409 | Valid loss: 0.01445\n",
      "Epoch: 4800 | Loss: 0.01428 | Test Loss: 0.01406 | Valid loss: 0.01442\n",
      "Epoch: 5000 | Loss: 0.01424 | Test Loss: 0.01402 | Valid loss: 0.01438\n",
      "Epoch: 5200 | Loss: 0.01419 | Test Loss: 0.01397 | Valid loss: 0.01433\n",
      "Epoch: 5400 | Loss: 0.01414 | Test Loss: 0.01392 | Valid loss: 0.01428\n",
      "Epoch: 5600 | Loss: 0.01408 | Test Loss: 0.01386 | Valid loss: 0.01421\n",
      "Epoch: 5800 | Loss: 0.01401 | Test Loss: 0.01380 | Valid loss: 0.01414\n",
      "Epoch: 6000 | Loss: 0.01392 | Test Loss: 0.01372 | Valid loss: 0.01406\n",
      "Epoch: 6200 | Loss: 0.01383 | Test Loss: 0.01362 | Valid loss: 0.01396\n",
      "Epoch: 6400 | Loss: 0.01371 | Test Loss: 0.01351 | Valid loss: 0.01384\n",
      "Epoch: 6600 | Loss: 0.01356 | Test Loss: 0.01336 | Valid loss: 0.01369\n",
      "Epoch: 6800 | Loss: 0.01338 | Test Loss: 0.01319 | Valid loss: 0.01351\n",
      "Epoch: 7000 | Loss: 0.01316 | Test Loss: 0.01297 | Valid loss: 0.01328\n",
      "Epoch: 7200 | Loss: 0.01287 | Test Loss: 0.01270 | Valid loss: 0.01299\n",
      "Epoch: 7400 | Loss: 0.01252 | Test Loss: 0.01236 | Valid loss: 0.01262\n",
      "Epoch: 7600 | Loss: 0.01208 | Test Loss: 0.01194 | Valid loss: 0.01218\n",
      "Epoch: 7800 | Loss: 0.01156 | Test Loss: 0.01144 | Valid loss: 0.01164\n",
      "Epoch: 8000 | Loss: 0.01095 | Test Loss: 0.01086 | Valid loss: 0.01102\n",
      "Epoch: 8200 | Loss: 0.01026 | Test Loss: 0.01021 | Valid loss: 0.01033\n",
      "Epoch: 8400 | Loss: 0.00953 | Test Loss: 0.00951 | Valid loss: 0.00959\n",
      "Epoch: 8600 | Loss: 0.00876 | Test Loss: 0.00877 | Valid loss: 0.00881\n",
      "Epoch: 8800 | Loss: 0.00798 | Test Loss: 0.00803 | Valid loss: 0.00804\n",
      "Epoch: 9000 | Loss: 0.00722 | Test Loss: 0.00730 | Valid loss: 0.00729\n",
      "Epoch: 9200 | Loss: 0.00652 | Test Loss: 0.00663 | Valid loss: 0.00661\n",
      "Epoch: 9400 | Loss: 0.00595 | Test Loss: 0.00608 | Valid loss: 0.00605\n",
      "Epoch: 9600 | Loss: 0.00551 | Test Loss: 0.00568 | Valid loss: 0.00563\n",
      "Epoch: 9800 | Loss: 0.00520 | Test Loss: 0.00537 | Valid loss: 0.00532\n",
      "Epoch: 10000 | Loss: 0.00498 | Test Loss: 0.00515 | Valid loss: 0.00510\n",
      "Epoch: 10200 | Loss: 0.00482 | Test Loss: 0.00500 | Valid loss: 0.00495\n",
      "Epoch: 10400 | Loss: 0.00471 | Test Loss: 0.00489 | Valid loss: 0.00484\n",
      "Epoch: 10600 | Loss: 0.00462 | Test Loss: 0.00481 | Valid loss: 0.00475\n",
      "Epoch: 10800 | Loss: 0.00456 | Test Loss: 0.00474 | Valid loss: 0.00469\n",
      "Epoch: 11000 | Loss: 0.00450 | Test Loss: 0.00469 | Valid loss: 0.00464\n",
      "Epoch: 11200 | Loss: 0.00445 | Test Loss: 0.00465 | Valid loss: 0.00459\n",
      "Epoch: 11400 | Loss: 0.00441 | Test Loss: 0.00460 | Valid loss: 0.00455\n",
      "Epoch: 11600 | Loss: 0.00438 | Test Loss: 0.00457 | Valid loss: 0.00451\n",
      "Early stopping at epoch: 11753\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.12098 | Test Loss: 1.08165 | Valid loss: 1.08751\n",
      "Epoch: 200 | Loss: 0.01460 | Test Loss: 0.01491 | Valid loss: 0.01494\n",
      "Epoch: 400 | Loss: 0.01460 | Test Loss: 0.01491 | Valid loss: 0.01493\n",
      "Epoch: 600 | Loss: 0.01459 | Test Loss: 0.01490 | Valid loss: 0.01493\n",
      "Epoch: 800 | Loss: 0.01459 | Test Loss: 0.01490 | Valid loss: 0.01493\n",
      "Epoch: 1000 | Loss: 0.01458 | Test Loss: 0.01489 | Valid loss: 0.01492\n",
      "Epoch: 1200 | Loss: 0.01458 | Test Loss: 0.01489 | Valid loss: 0.01491\n",
      "Epoch: 1400 | Loss: 0.01457 | Test Loss: 0.01488 | Valid loss: 0.01490\n",
      "Epoch: 1600 | Loss: 0.01456 | Test Loss: 0.01487 | Valid loss: 0.01490\n",
      "Epoch: 1800 | Loss: 0.01455 | Test Loss: 0.01486 | Valid loss: 0.01489\n",
      "Epoch: 2000 | Loss: 0.01454 | Test Loss: 0.01485 | Valid loss: 0.01487\n",
      "Epoch: 2200 | Loss: 0.01453 | Test Loss: 0.01484 | Valid loss: 0.01486\n",
      "Epoch: 2400 | Loss: 0.01451 | Test Loss: 0.01482 | Valid loss: 0.01485\n",
      "Epoch: 2600 | Loss: 0.01450 | Test Loss: 0.01481 | Valid loss: 0.01483\n",
      "Epoch: 2800 | Loss: 0.01448 | Test Loss: 0.01479 | Valid loss: 0.01481\n",
      "Epoch: 3000 | Loss: 0.01446 | Test Loss: 0.01477 | Valid loss: 0.01479\n",
      "Epoch: 3200 | Loss: 0.01444 | Test Loss: 0.01475 | Valid loss: 0.01477\n",
      "Epoch: 3400 | Loss: 0.01441 | Test Loss: 0.01472 | Valid loss: 0.01475\n",
      "Epoch: 3600 | Loss: 0.01439 | Test Loss: 0.01470 | Valid loss: 0.01472\n",
      "Epoch: 3800 | Loss: 0.01436 | Test Loss: 0.01467 | Valid loss: 0.01469\n",
      "Epoch: 4000 | Loss: 0.01433 | Test Loss: 0.01464 | Valid loss: 0.01466\n",
      "Epoch: 4200 | Loss: 0.01429 | Test Loss: 0.01460 | Valid loss: 0.01462\n",
      "Epoch: 4400 | Loss: 0.01425 | Test Loss: 0.01456 | Valid loss: 0.01458\n",
      "Epoch: 4600 | Loss: 0.01420 | Test Loss: 0.01451 | Valid loss: 0.01453\n",
      "Epoch: 4800 | Loss: 0.01415 | Test Loss: 0.01446 | Valid loss: 0.01448\n",
      "Epoch: 5000 | Loss: 0.01409 | Test Loss: 0.01440 | Valid loss: 0.01442\n",
      "Epoch: 5200 | Loss: 0.01402 | Test Loss: 0.01433 | Valid loss: 0.01435\n",
      "Epoch: 5400 | Loss: 0.01394 | Test Loss: 0.01425 | Valid loss: 0.01426\n",
      "Epoch: 5600 | Loss: 0.01385 | Test Loss: 0.01416 | Valid loss: 0.01416\n",
      "Epoch: 5800 | Loss: 0.01373 | Test Loss: 0.01404 | Valid loss: 0.01404\n",
      "Epoch: 6000 | Loss: 0.01358 | Test Loss: 0.01389 | Valid loss: 0.01390\n",
      "Epoch: 6200 | Loss: 0.01340 | Test Loss: 0.01371 | Valid loss: 0.01371\n",
      "Epoch: 6400 | Loss: 0.01317 | Test Loss: 0.01349 | Valid loss: 0.01347\n",
      "Epoch: 6600 | Loss: 0.01288 | Test Loss: 0.01319 | Valid loss: 0.01317\n",
      "Epoch: 6800 | Loss: 0.01251 | Test Loss: 0.01283 | Valid loss: 0.01279\n",
      "Epoch: 7000 | Loss: 0.01205 | Test Loss: 0.01237 | Valid loss: 0.01232\n",
      "Epoch: 7200 | Loss: 0.01150 | Test Loss: 0.01182 | Valid loss: 0.01175\n",
      "Epoch: 7400 | Loss: 0.01086 | Test Loss: 0.01118 | Valid loss: 0.01109\n",
      "Epoch: 7600 | Loss: 0.01016 | Test Loss: 0.01047 | Valid loss: 0.01037\n",
      "Epoch: 7800 | Loss: 0.00941 | Test Loss: 0.00971 | Valid loss: 0.00960\n",
      "Epoch: 8000 | Loss: 0.00865 | Test Loss: 0.00893 | Valid loss: 0.00883\n",
      "Epoch: 8200 | Loss: 0.00790 | Test Loss: 0.00816 | Valid loss: 0.00807\n",
      "Epoch: 8400 | Loss: 0.00719 | Test Loss: 0.00742 | Valid loss: 0.00735\n",
      "Epoch: 8600 | Loss: 0.00656 | Test Loss: 0.00676 | Valid loss: 0.00670\n",
      "Epoch: 8800 | Loss: 0.00604 | Test Loss: 0.00621 | Valid loss: 0.00617\n",
      "Epoch: 9000 | Loss: 0.00563 | Test Loss: 0.00579 | Valid loss: 0.00576\n",
      "Epoch: 9200 | Loss: 0.00533 | Test Loss: 0.00547 | Valid loss: 0.00545\n",
      "Epoch: 9400 | Loss: 0.00510 | Test Loss: 0.00522 | Valid loss: 0.00521\n",
      "Epoch: 9600 | Loss: 0.00493 | Test Loss: 0.00504 | Valid loss: 0.00503\n",
      "Epoch: 9800 | Loss: 0.00479 | Test Loss: 0.00490 | Valid loss: 0.00489\n",
      "Epoch: 10000 | Loss: 0.00468 | Test Loss: 0.00478 | Valid loss: 0.00478\n",
      "Epoch: 10200 | Loss: 0.00459 | Test Loss: 0.00469 | Valid loss: 0.00469\n",
      "Epoch: 10400 | Loss: 0.00452 | Test Loss: 0.00461 | Valid loss: 0.00461\n",
      "Epoch: 10600 | Loss: 0.00445 | Test Loss: 0.00454 | Valid loss: 0.00454\n",
      "Epoch: 10800 | Loss: 0.00439 | Test Loss: 0.00448 | Valid loss: 0.00448\n",
      "Epoch: 11000 | Loss: 0.00433 | Test Loss: 0.00442 | Valid loss: 0.00442\n",
      "Epoch: 11200 | Loss: 0.00428 | Test Loss: 0.00437 | Valid loss: 0.00437\n",
      "Epoch: 11400 | Loss: 0.00423 | Test Loss: 0.00432 | Valid loss: 0.00432\n",
      "Early stopping at epoch: 11438\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 1.43593 | Test Loss: 1.38111 | Valid loss: 1.39078\n",
      "Epoch: 200 | Loss: 0.01471 | Test Loss: 0.01453 | Valid loss: 0.01499\n",
      "Epoch: 400 | Loss: 0.01471 | Test Loss: 0.01453 | Valid loss: 0.01499\n",
      "Epoch: 600 | Loss: 0.01470 | Test Loss: 0.01453 | Valid loss: 0.01499\n",
      "Epoch: 800 | Loss: 0.01470 | Test Loss: 0.01452 | Valid loss: 0.01498\n",
      "Epoch: 1000 | Loss: 0.01469 | Test Loss: 0.01452 | Valid loss: 0.01498\n",
      "Epoch: 1200 | Loss: 0.01469 | Test Loss: 0.01451 | Valid loss: 0.01497\n",
      "Epoch: 1400 | Loss: 0.01468 | Test Loss: 0.01451 | Valid loss: 0.01497\n",
      "Epoch: 1600 | Loss: 0.01468 | Test Loss: 0.01450 | Valid loss: 0.01496\n",
      "Epoch: 1800 | Loss: 0.01467 | Test Loss: 0.01449 | Valid loss: 0.01495\n",
      "Epoch: 2000 | Loss: 0.01466 | Test Loss: 0.01448 | Valid loss: 0.01494\n",
      "Epoch: 2200 | Loss: 0.01465 | Test Loss: 0.01447 | Valid loss: 0.01493\n",
      "Epoch: 2400 | Loss: 0.01463 | Test Loss: 0.01446 | Valid loss: 0.01492\n",
      "Epoch: 2600 | Loss: 0.01462 | Test Loss: 0.01445 | Valid loss: 0.01490\n",
      "Epoch: 2800 | Loss: 0.01461 | Test Loss: 0.01443 | Valid loss: 0.01489\n",
      "Epoch: 3000 | Loss: 0.01459 | Test Loss: 0.01441 | Valid loss: 0.01487\n",
      "Epoch: 3200 | Loss: 0.01457 | Test Loss: 0.01440 | Valid loss: 0.01486\n",
      "Epoch: 3400 | Loss: 0.01455 | Test Loss: 0.01438 | Valid loss: 0.01483\n",
      "Epoch: 3600 | Loss: 0.01453 | Test Loss: 0.01435 | Valid loss: 0.01481\n",
      "Epoch: 3800 | Loss: 0.01451 | Test Loss: 0.01433 | Valid loss: 0.01479\n",
      "Epoch: 4000 | Loss: 0.01448 | Test Loss: 0.01430 | Valid loss: 0.01476\n",
      "Epoch: 4200 | Loss: 0.01445 | Test Loss: 0.01427 | Valid loss: 0.01473\n",
      "Epoch: 4400 | Loss: 0.01441 | Test Loss: 0.01424 | Valid loss: 0.01469\n",
      "Epoch: 4600 | Loss: 0.01437 | Test Loss: 0.01420 | Valid loss: 0.01466\n",
      "Epoch: 4800 | Loss: 0.01433 | Test Loss: 0.01415 | Valid loss: 0.01461\n",
      "Epoch: 5000 | Loss: 0.01428 | Test Loss: 0.01410 | Valid loss: 0.01456\n",
      "Epoch: 5200 | Loss: 0.01423 | Test Loss: 0.01405 | Valid loss: 0.01450\n",
      "Epoch: 5400 | Loss: 0.01416 | Test Loss: 0.01398 | Valid loss: 0.01444\n",
      "Epoch: 5600 | Loss: 0.01409 | Test Loss: 0.01391 | Valid loss: 0.01436\n",
      "Epoch: 5800 | Loss: 0.01400 | Test Loss: 0.01382 | Valid loss: 0.01427\n",
      "Epoch: 6000 | Loss: 0.01389 | Test Loss: 0.01371 | Valid loss: 0.01416\n",
      "Epoch: 6200 | Loss: 0.01376 | Test Loss: 0.01358 | Valid loss: 0.01403\n",
      "Epoch: 6400 | Loss: 0.01360 | Test Loss: 0.01341 | Valid loss: 0.01387\n",
      "Epoch: 6600 | Loss: 0.01340 | Test Loss: 0.01321 | Valid loss: 0.01367\n",
      "Epoch: 6800 | Loss: 0.01315 | Test Loss: 0.01296 | Valid loss: 0.01342\n",
      "Epoch: 7000 | Loss: 0.01283 | Test Loss: 0.01264 | Valid loss: 0.01310\n",
      "Epoch: 7200 | Loss: 0.01244 | Test Loss: 0.01224 | Valid loss: 0.01270\n",
      "Epoch: 7400 | Loss: 0.01195 | Test Loss: 0.01175 | Valid loss: 0.01221\n",
      "Epoch: 7600 | Loss: 0.01138 | Test Loss: 0.01117 | Valid loss: 0.01164\n",
      "Epoch: 7800 | Loss: 0.01074 | Test Loss: 0.01051 | Valid loss: 0.01098\n",
      "Epoch: 8000 | Loss: 0.01003 | Test Loss: 0.00979 | Valid loss: 0.01027\n",
      "Epoch: 8200 | Loss: 0.00931 | Test Loss: 0.00905 | Valid loss: 0.00953\n",
      "Epoch: 8400 | Loss: 0.00860 | Test Loss: 0.00832 | Valid loss: 0.00880\n",
      "Epoch: 8600 | Loss: 0.00792 | Test Loss: 0.00763 | Valid loss: 0.00810\n",
      "Epoch: 8800 | Loss: 0.00729 | Test Loss: 0.00699 | Valid loss: 0.00745\n",
      "Epoch: 9000 | Loss: 0.00674 | Test Loss: 0.00643 | Valid loss: 0.00687\n",
      "Epoch: 9200 | Loss: 0.00628 | Test Loss: 0.00597 | Valid loss: 0.00639\n",
      "Epoch: 9400 | Loss: 0.00591 | Test Loss: 0.00561 | Valid loss: 0.00600\n",
      "Epoch: 9600 | Loss: 0.00562 | Test Loss: 0.00532 | Valid loss: 0.00569\n",
      "Epoch: 9800 | Loss: 0.00540 | Test Loss: 0.00513 | Valid loss: 0.00546\n",
      "Epoch: 10000 | Loss: 0.00521 | Test Loss: 0.00493 | Valid loss: 0.00525\n",
      "Epoch: 10200 | Loss: 0.00506 | Test Loss: 0.00479 | Valid loss: 0.00509\n",
      "Epoch: 10400 | Loss: 0.00494 | Test Loss: 0.00468 | Valid loss: 0.00496\n",
      "Epoch: 10600 | Loss: 0.00483 | Test Loss: 0.00458 | Valid loss: 0.00484\n",
      "Epoch: 10800 | Loss: 0.00474 | Test Loss: 0.00450 | Valid loss: 0.00474\n",
      "Epoch: 11000 | Loss: 0.00465 | Test Loss: 0.00442 | Valid loss: 0.00465\n",
      "Epoch: 11200 | Loss: 0.00457 | Test Loss: 0.00435 | Valid loss: 0.00457\n",
      "Epoch: 11400 | Loss: 0.00450 | Test Loss: 0.00429 | Valid loss: 0.00449\n",
      "Epoch: 11600 | Loss: 0.00443 | Test Loss: 0.00423 | Valid loss: 0.00442\n",
      "Epoch: 11800 | Loss: 0.00437 | Test Loss: 0.00417 | Valid loss: 0.00435\n",
      "Early stopping at epoch: 11945\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 0.64354 | Test Loss: 0.60635 | Valid loss: 0.61092\n",
      "Epoch: 200 | Loss: 0.01485 | Test Loss: 0.01485 | Valid loss: 0.01461\n",
      "Epoch: 400 | Loss: 0.01485 | Test Loss: 0.01484 | Valid loss: 0.01461\n",
      "Epoch: 600 | Loss: 0.01484 | Test Loss: 0.01484 | Valid loss: 0.01460\n",
      "Epoch: 800 | Loss: 0.01483 | Test Loss: 0.01483 | Valid loss: 0.01459\n",
      "Epoch: 1000 | Loss: 0.01482 | Test Loss: 0.01482 | Valid loss: 0.01458\n",
      "Epoch: 1200 | Loss: 0.01481 | Test Loss: 0.01481 | Valid loss: 0.01457\n",
      "Epoch: 1400 | Loss: 0.01480 | Test Loss: 0.01480 | Valid loss: 0.01456\n",
      "Epoch: 1600 | Loss: 0.01479 | Test Loss: 0.01478 | Valid loss: 0.01455\n",
      "Epoch: 1800 | Loss: 0.01477 | Test Loss: 0.01477 | Valid loss: 0.01453\n",
      "Epoch: 2000 | Loss: 0.01475 | Test Loss: 0.01475 | Valid loss: 0.01451\n",
      "Epoch: 2200 | Loss: 0.01473 | Test Loss: 0.01473 | Valid loss: 0.01449\n",
      "Epoch: 2400 | Loss: 0.01471 | Test Loss: 0.01471 | Valid loss: 0.01447\n",
      "Epoch: 2600 | Loss: 0.01468 | Test Loss: 0.01468 | Valid loss: 0.01444\n",
      "Epoch: 2800 | Loss: 0.01465 | Test Loss: 0.01465 | Valid loss: 0.01442\n",
      "Epoch: 3000 | Loss: 0.01462 | Test Loss: 0.01462 | Valid loss: 0.01438\n",
      "Epoch: 3200 | Loss: 0.01459 | Test Loss: 0.01458 | Valid loss: 0.01435\n",
      "Epoch: 3400 | Loss: 0.01455 | Test Loss: 0.01454 | Valid loss: 0.01431\n",
      "Epoch: 3600 | Loss: 0.01450 | Test Loss: 0.01450 | Valid loss: 0.01427\n",
      "Epoch: 3800 | Loss: 0.01445 | Test Loss: 0.01445 | Valid loss: 0.01421\n",
      "Epoch: 4000 | Loss: 0.01439 | Test Loss: 0.01439 | Valid loss: 0.01416\n",
      "Epoch: 4200 | Loss: 0.01432 | Test Loss: 0.01432 | Valid loss: 0.01409\n",
      "Epoch: 4400 | Loss: 0.01424 | Test Loss: 0.01424 | Valid loss: 0.01401\n",
      "Epoch: 4600 | Loss: 0.01415 | Test Loss: 0.01415 | Valid loss: 0.01391\n",
      "Epoch: 4800 | Loss: 0.01403 | Test Loss: 0.01403 | Valid loss: 0.01380\n",
      "Epoch: 5000 | Loss: 0.01389 | Test Loss: 0.01389 | Valid loss: 0.01366\n",
      "Epoch: 5200 | Loss: 0.01371 | Test Loss: 0.01371 | Valid loss: 0.01348\n",
      "Epoch: 5400 | Loss: 0.01348 | Test Loss: 0.01348 | Valid loss: 0.01325\n",
      "Epoch: 5600 | Loss: 0.01318 | Test Loss: 0.01318 | Valid loss: 0.01295\n",
      "Epoch: 5800 | Loss: 0.01280 | Test Loss: 0.01280 | Valid loss: 0.01258\n",
      "Epoch: 6000 | Loss: 0.01232 | Test Loss: 0.01232 | Valid loss: 0.01210\n",
      "Epoch: 6200 | Loss: 0.01174 | Test Loss: 0.01173 | Valid loss: 0.01152\n",
      "Epoch: 6400 | Loss: 0.01105 | Test Loss: 0.01104 | Valid loss: 0.01085\n",
      "Epoch: 6600 | Loss: 0.01030 | Test Loss: 0.01028 | Valid loss: 0.01011\n",
      "Epoch: 6800 | Loss: 0.00951 | Test Loss: 0.00949 | Valid loss: 0.00933\n",
      "Epoch: 7000 | Loss: 0.00873 | Test Loss: 0.00870 | Valid loss: 0.00856\n",
      "Epoch: 7200 | Loss: 0.00800 | Test Loss: 0.00796 | Valid loss: 0.00785\n",
      "Epoch: 7400 | Loss: 0.00736 | Test Loss: 0.00731 | Valid loss: 0.00723\n",
      "Epoch: 7600 | Loss: 0.00682 | Test Loss: 0.00677 | Valid loss: 0.00671\n",
      "Epoch: 7800 | Loss: 0.00639 | Test Loss: 0.00633 | Valid loss: 0.00630\n",
      "Epoch: 8000 | Loss: 0.00606 | Test Loss: 0.00599 | Valid loss: 0.00598\n",
      "Epoch: 8200 | Loss: 0.00581 | Test Loss: 0.00574 | Valid loss: 0.00575\n",
      "Epoch: 8400 | Loss: 0.00561 | Test Loss: 0.00554 | Valid loss: 0.00556\n",
      "Epoch: 8600 | Loss: 0.00546 | Test Loss: 0.00537 | Valid loss: 0.00541\n",
      "Epoch: 8800 | Loss: 0.00533 | Test Loss: 0.00525 | Valid loss: 0.00529\n",
      "Epoch: 9000 | Loss: 0.00522 | Test Loss: 0.00514 | Valid loss: 0.00518\n",
      "Epoch: 9200 | Loss: 0.00513 | Test Loss: 0.00504 | Valid loss: 0.00509\n",
      "Epoch: 9400 | Loss: 0.00504 | Test Loss: 0.00496 | Valid loss: 0.00501\n",
      "Epoch: 9600 | Loss: 0.00497 | Test Loss: 0.00488 | Valid loss: 0.00494\n",
      "Epoch: 9800 | Loss: 0.00490 | Test Loss: 0.00483 | Valid loss: 0.00488\n",
      "Epoch: 10000 | Loss: 0.00484 | Test Loss: 0.00475 | Valid loss: 0.00481\n",
      "Early stopping at epoch: 10069\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 2.04659 | Test Loss: 1.97762 | Valid loss: 1.98861\n",
      "Epoch: 200 | Loss: 0.01476 | Test Loss: 0.01452 | Valid loss: 0.01499\n",
      "Epoch: 400 | Loss: 0.01476 | Test Loss: 0.01451 | Valid loss: 0.01499\n",
      "Epoch: 600 | Loss: 0.01476 | Test Loss: 0.01451 | Valid loss: 0.01499\n",
      "Epoch: 800 | Loss: 0.01476 | Test Loss: 0.01451 | Valid loss: 0.01498\n",
      "Epoch: 1000 | Loss: 0.01475 | Test Loss: 0.01450 | Valid loss: 0.01498\n",
      "Epoch: 1200 | Loss: 0.01475 | Test Loss: 0.01450 | Valid loss: 0.01497\n",
      "Epoch: 1400 | Loss: 0.01474 | Test Loss: 0.01450 | Valid loss: 0.01497\n",
      "Epoch: 1600 | Loss: 0.01474 | Test Loss: 0.01449 | Valid loss: 0.01496\n",
      "Epoch: 1800 | Loss: 0.01473 | Test Loss: 0.01448 | Valid loss: 0.01496\n",
      "Epoch: 2000 | Loss: 0.01472 | Test Loss: 0.01448 | Valid loss: 0.01495\n",
      "Epoch: 2200 | Loss: 0.01471 | Test Loss: 0.01447 | Valid loss: 0.01494\n",
      "Epoch: 2400 | Loss: 0.01470 | Test Loss: 0.01446 | Valid loss: 0.01493\n",
      "Epoch: 2600 | Loss: 0.01469 | Test Loss: 0.01445 | Valid loss: 0.01492\n",
      "Epoch: 2800 | Loss: 0.01468 | Test Loss: 0.01444 | Valid loss: 0.01491\n",
      "Epoch: 3000 | Loss: 0.01467 | Test Loss: 0.01442 | Valid loss: 0.01489\n",
      "Epoch: 3200 | Loss: 0.01465 | Test Loss: 0.01441 | Valid loss: 0.01488\n",
      "Epoch: 3400 | Loss: 0.01464 | Test Loss: 0.01439 | Valid loss: 0.01486\n",
      "Epoch: 3600 | Loss: 0.01462 | Test Loss: 0.01438 | Valid loss: 0.01485\n",
      "Epoch: 3800 | Loss: 0.01460 | Test Loss: 0.01436 | Valid loss: 0.01483\n",
      "Epoch: 4000 | Loss: 0.01458 | Test Loss: 0.01434 | Valid loss: 0.01481\n",
      "Epoch: 4200 | Loss: 0.01456 | Test Loss: 0.01431 | Valid loss: 0.01478\n",
      "Epoch: 4400 | Loss: 0.01453 | Test Loss: 0.01429 | Valid loss: 0.01475\n",
      "Epoch: 4600 | Loss: 0.01450 | Test Loss: 0.01426 | Valid loss: 0.01472\n",
      "Epoch: 4800 | Loss: 0.01447 | Test Loss: 0.01422 | Valid loss: 0.01469\n",
      "Epoch: 5000 | Loss: 0.01443 | Test Loss: 0.01419 | Valid loss: 0.01465\n",
      "Epoch: 5200 | Loss: 0.01439 | Test Loss: 0.01415 | Valid loss: 0.01461\n",
      "Epoch: 5400 | Loss: 0.01434 | Test Loss: 0.01410 | Valid loss: 0.01456\n",
      "Epoch: 5600 | Loss: 0.01429 | Test Loss: 0.01405 | Valid loss: 0.01451\n",
      "Epoch: 5800 | Loss: 0.01423 | Test Loss: 0.01399 | Valid loss: 0.01445\n",
      "Epoch: 6000 | Loss: 0.01416 | Test Loss: 0.01392 | Valid loss: 0.01438\n",
      "Epoch: 6200 | Loss: 0.01407 | Test Loss: 0.01384 | Valid loss: 0.01430\n",
      "Epoch: 6400 | Loss: 0.01398 | Test Loss: 0.01374 | Valid loss: 0.01420\n",
      "Epoch: 6600 | Loss: 0.01386 | Test Loss: 0.01363 | Valid loss: 0.01408\n",
      "Epoch: 6800 | Loss: 0.01372 | Test Loss: 0.01349 | Valid loss: 0.01394\n",
      "Epoch: 7000 | Loss: 0.01354 | Test Loss: 0.01331 | Valid loss: 0.01376\n",
      "Epoch: 7200 | Loss: 0.01332 | Test Loss: 0.01310 | Valid loss: 0.01354\n",
      "Epoch: 7400 | Loss: 0.01305 | Test Loss: 0.01283 | Valid loss: 0.01327\n",
      "Epoch: 7600 | Loss: 0.01271 | Test Loss: 0.01249 | Valid loss: 0.01292\n",
      "Epoch: 7800 | Loss: 0.01228 | Test Loss: 0.01206 | Valid loss: 0.01249\n",
      "Epoch: 8000 | Loss: 0.01176 | Test Loss: 0.01155 | Valid loss: 0.01196\n",
      "Epoch: 8200 | Loss: 0.01114 | Test Loss: 0.01093 | Valid loss: 0.01134\n",
      "Epoch: 8400 | Loss: 0.01043 | Test Loss: 0.01022 | Valid loss: 0.01062\n",
      "Epoch: 8600 | Loss: 0.00966 | Test Loss: 0.00945 | Valid loss: 0.00984\n",
      "Epoch: 8800 | Loss: 0.00886 | Test Loss: 0.00866 | Valid loss: 0.00902\n",
      "Epoch: 9000 | Loss: 0.00807 | Test Loss: 0.00788 | Valid loss: 0.00823\n",
      "Epoch: 9200 | Loss: 0.00734 | Test Loss: 0.00716 | Valid loss: 0.00749\n",
      "Epoch: 9400 | Loss: 0.00671 | Test Loss: 0.00654 | Valid loss: 0.00685\n",
      "Epoch: 9600 | Loss: 0.00620 | Test Loss: 0.00605 | Valid loss: 0.00634\n",
      "Epoch: 9800 | Loss: 0.00582 | Test Loss: 0.00568 | Valid loss: 0.00595\n",
      "Epoch: 10000 | Loss: 0.00555 | Test Loss: 0.00542 | Valid loss: 0.00568\n",
      "Epoch: 10200 | Loss: 0.00536 | Test Loss: 0.00524 | Valid loss: 0.00549\n",
      "Epoch: 10400 | Loss: 0.00522 | Test Loss: 0.00511 | Valid loss: 0.00534\n",
      "Epoch: 10600 | Loss: 0.00511 | Test Loss: 0.00501 | Valid loss: 0.00523\n",
      "Epoch: 10800 | Loss: 0.00503 | Test Loss: 0.00494 | Valid loss: 0.00515\n",
      "Epoch: 11000 | Loss: 0.00496 | Test Loss: 0.00487 | Valid loss: 0.00508\n",
      "Epoch: 11200 | Loss: 0.00490 | Test Loss: 0.00482 | Valid loss: 0.00502\n",
      "Epoch: 11400 | Loss: 0.00485 | Test Loss: 0.00477 | Valid loss: 0.00496\n",
      "Epoch: 11600 | Loss: 0.00481 | Test Loss: 0.00473 | Valid loss: 0.00492\n",
      "Epoch: 11800 | Loss: 0.00477 | Test Loss: 0.00470 | Valid loss: 0.00488\n",
      "Epoch: 12000 | Loss: 0.00473 | Test Loss: 0.00466 | Valid loss: 0.00484\n",
      "Epoch: 12200 | Loss: 0.00469 | Test Loss: 0.00463 | Valid loss: 0.00480\n",
      "Epoch: 12400 | Loss: 0.00466 | Test Loss: 0.00459 | Valid loss: 0.00477\n",
      "Epoch: 12600 | Loss: 0.00463 | Test Loss: 0.00457 | Valid loss: 0.00473\n",
      "Epoch: 12800 | Loss: 0.00460 | Test Loss: 0.00454 | Valid loss: 0.00470\n",
      "Epoch: 13000 | Loss: 0.00457 | Test Loss: 0.00451 | Valid loss: 0.00467\n",
      "Epoch: 13200 | Loss: 0.00454 | Test Loss: 0.00449 | Valid loss: 0.00464\n",
      "Epoch: 13400 | Loss: 0.00452 | Test Loss: 0.00449 | Valid loss: 0.00463\n",
      "Epoch: 13600 | Loss: 0.00449 | Test Loss: 0.00443 | Valid loss: 0.00459\n",
      "Epoch: 13800 | Loss: 0.00446 | Test Loss: 0.00441 | Valid loss: 0.00456\n",
      "Epoch: 14000 | Loss: 0.00444 | Test Loss: 0.00438 | Valid loss: 0.00453\n",
      "Epoch: 14200 | Loss: 0.00441 | Test Loss: 0.00436 | Valid loss: 0.00451\n",
      "Early stopping at epoch: 14312\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.18964 | Test Loss: 2.12334 | Valid loss: 2.11860\n",
      "Epoch: 200 | Loss: 0.01490 | Test Loss: 0.01469 | Valid loss: 0.01532\n",
      "Epoch: 400 | Loss: 0.01490 | Test Loss: 0.01468 | Valid loss: 0.01531\n",
      "Epoch: 600 | Loss: 0.01490 | Test Loss: 0.01468 | Valid loss: 0.01531\n",
      "Epoch: 800 | Loss: 0.01490 | Test Loss: 0.01468 | Valid loss: 0.01531\n",
      "Epoch: 1000 | Loss: 0.01489 | Test Loss: 0.01467 | Valid loss: 0.01530\n",
      "Epoch: 1200 | Loss: 0.01489 | Test Loss: 0.01467 | Valid loss: 0.01530\n",
      "Epoch: 1400 | Loss: 0.01488 | Test Loss: 0.01466 | Valid loss: 0.01529\n",
      "Epoch: 1600 | Loss: 0.01488 | Test Loss: 0.01466 | Valid loss: 0.01528\n",
      "Epoch: 1800 | Loss: 0.01487 | Test Loss: 0.01465 | Valid loss: 0.01528\n",
      "Epoch: 2000 | Loss: 0.01486 | Test Loss: 0.01464 | Valid loss: 0.01527\n",
      "Epoch: 2200 | Loss: 0.01485 | Test Loss: 0.01463 | Valid loss: 0.01526\n",
      "Epoch: 2400 | Loss: 0.01484 | Test Loss: 0.01463 | Valid loss: 0.01525\n",
      "Epoch: 2600 | Loss: 0.01483 | Test Loss: 0.01462 | Valid loss: 0.01524\n",
      "Epoch: 2800 | Loss: 0.01482 | Test Loss: 0.01460 | Valid loss: 0.01523\n",
      "Epoch: 3000 | Loss: 0.01481 | Test Loss: 0.01459 | Valid loss: 0.01521\n",
      "Epoch: 3200 | Loss: 0.01479 | Test Loss: 0.01458 | Valid loss: 0.01520\n",
      "Epoch: 3400 | Loss: 0.01477 | Test Loss: 0.01456 | Valid loss: 0.01518\n",
      "Epoch: 3600 | Loss: 0.01476 | Test Loss: 0.01454 | Valid loss: 0.01516\n",
      "Epoch: 3800 | Loss: 0.01474 | Test Loss: 0.01452 | Valid loss: 0.01514\n",
      "Epoch: 4000 | Loss: 0.01471 | Test Loss: 0.01450 | Valid loss: 0.01512\n",
      "Epoch: 4200 | Loss: 0.01469 | Test Loss: 0.01448 | Valid loss: 0.01509\n",
      "Epoch: 4400 | Loss: 0.01466 | Test Loss: 0.01445 | Valid loss: 0.01507\n",
      "Epoch: 4600 | Loss: 0.01463 | Test Loss: 0.01442 | Valid loss: 0.01503\n",
      "Epoch: 4800 | Loss: 0.01460 | Test Loss: 0.01439 | Valid loss: 0.01500\n",
      "Epoch: 5000 | Loss: 0.01456 | Test Loss: 0.01435 | Valid loss: 0.01496\n",
      "Epoch: 5200 | Loss: 0.01451 | Test Loss: 0.01431 | Valid loss: 0.01492\n",
      "Epoch: 5400 | Loss: 0.01447 | Test Loss: 0.01427 | Valid loss: 0.01487\n",
      "Epoch: 5600 | Loss: 0.01441 | Test Loss: 0.01421 | Valid loss: 0.01481\n",
      "Epoch: 5800 | Loss: 0.01435 | Test Loss: 0.01416 | Valid loss: 0.01475\n",
      "Epoch: 6000 | Loss: 0.01428 | Test Loss: 0.01409 | Valid loss: 0.01467\n",
      "Epoch: 6200 | Loss: 0.01420 | Test Loss: 0.01401 | Valid loss: 0.01459\n",
      "Epoch: 6400 | Loss: 0.01410 | Test Loss: 0.01392 | Valid loss: 0.01449\n",
      "Epoch: 6600 | Loss: 0.01399 | Test Loss: 0.01381 | Valid loss: 0.01438\n",
      "Epoch: 6800 | Loss: 0.01385 | Test Loss: 0.01368 | Valid loss: 0.01424\n",
      "Epoch: 7000 | Loss: 0.01369 | Test Loss: 0.01352 | Valid loss: 0.01407\n",
      "Epoch: 7200 | Loss: 0.01349 | Test Loss: 0.01333 | Valid loss: 0.01387\n",
      "Epoch: 7400 | Loss: 0.01324 | Test Loss: 0.01310 | Valid loss: 0.01362\n",
      "Epoch: 7600 | Loss: 0.01294 | Test Loss: 0.01281 | Valid loss: 0.01331\n",
      "Epoch: 7800 | Loss: 0.01257 | Test Loss: 0.01245 | Valid loss: 0.01292\n",
      "Epoch: 8000 | Loss: 0.01212 | Test Loss: 0.01201 | Valid loss: 0.01245\n",
      "Epoch: 8200 | Loss: 0.01157 | Test Loss: 0.01148 | Valid loss: 0.01189\n",
      "Epoch: 8400 | Loss: 0.01093 | Test Loss: 0.01086 | Valid loss: 0.01123\n",
      "Epoch: 8600 | Loss: 0.01020 | Test Loss: 0.01016 | Valid loss: 0.01048\n",
      "Epoch: 8800 | Loss: 0.00942 | Test Loss: 0.00939 | Valid loss: 0.00966\n",
      "Epoch: 9000 | Loss: 0.00860 | Test Loss: 0.00859 | Valid loss: 0.00881\n",
      "Epoch: 9200 | Loss: 0.00778 | Test Loss: 0.00779 | Valid loss: 0.00797\n",
      "Epoch: 9400 | Loss: 0.00701 | Test Loss: 0.00703 | Valid loss: 0.00717\n",
      "Epoch: 9600 | Loss: 0.00631 | Test Loss: 0.00635 | Valid loss: 0.00645\n",
      "Epoch: 9800 | Loss: 0.00573 | Test Loss: 0.00579 | Valid loss: 0.00586\n",
      "Epoch: 10000 | Loss: 0.00530 | Test Loss: 0.00536 | Valid loss: 0.00542\n",
      "Epoch: 10200 | Loss: 0.00497 | Test Loss: 0.00504 | Valid loss: 0.00508\n",
      "Epoch: 10400 | Loss: 0.00474 | Test Loss: 0.00481 | Valid loss: 0.00484\n",
      "Epoch: 10600 | Loss: 0.00456 | Test Loss: 0.00465 | Valid loss: 0.00466\n",
      "Epoch: 10800 | Loss: 0.00443 | Test Loss: 0.00452 | Valid loss: 0.00453\n",
      "Epoch: 11000 | Loss: 0.00433 | Test Loss: 0.00442 | Valid loss: 0.00443\n",
      "Epoch: 11200 | Loss: 0.00425 | Test Loss: 0.00434 | Valid loss: 0.00434\n",
      "Epoch: 11400 | Loss: 0.00418 | Test Loss: 0.00428 | Valid loss: 0.00427\n",
      "Epoch: 11600 | Loss: 0.00412 | Test Loss: 0.00422 | Valid loss: 0.00421\n",
      "Epoch: 11800 | Loss: 0.00408 | Test Loss: 0.00418 | Valid loss: 0.00417\n",
      "Early stopping at epoch: 11817\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.36977 | Test Loss: 1.33293 | Valid loss: 1.32073\n",
      "Epoch: 200 | Loss: 0.01490 | Test Loss: 0.01425 | Valid loss: 0.01462\n",
      "Epoch: 400 | Loss: 0.01490 | Test Loss: 0.01425 | Valid loss: 0.01461\n",
      "Epoch: 600 | Loss: 0.01490 | Test Loss: 0.01424 | Valid loss: 0.01461\n",
      "Epoch: 800 | Loss: 0.01489 | Test Loss: 0.01424 | Valid loss: 0.01461\n",
      "Epoch: 1000 | Loss: 0.01489 | Test Loss: 0.01423 | Valid loss: 0.01460\n",
      "Epoch: 1200 | Loss: 0.01488 | Test Loss: 0.01423 | Valid loss: 0.01459\n",
      "Epoch: 1400 | Loss: 0.01487 | Test Loss: 0.01422 | Valid loss: 0.01459\n",
      "Epoch: 1600 | Loss: 0.01486 | Test Loss: 0.01421 | Valid loss: 0.01458\n",
      "Epoch: 1800 | Loss: 0.01485 | Test Loss: 0.01420 | Valid loss: 0.01457\n",
      "Epoch: 2000 | Loss: 0.01484 | Test Loss: 0.01419 | Valid loss: 0.01456\n",
      "Epoch: 2200 | Loss: 0.01483 | Test Loss: 0.01418 | Valid loss: 0.01454\n",
      "Epoch: 2400 | Loss: 0.01482 | Test Loss: 0.01417 | Valid loss: 0.01453\n",
      "Epoch: 2600 | Loss: 0.01480 | Test Loss: 0.01415 | Valid loss: 0.01451\n",
      "Epoch: 2800 | Loss: 0.01478 | Test Loss: 0.01414 | Valid loss: 0.01450\n",
      "Epoch: 3000 | Loss: 0.01476 | Test Loss: 0.01412 | Valid loss: 0.01448\n",
      "Epoch: 3200 | Loss: 0.01474 | Test Loss: 0.01410 | Valid loss: 0.01446\n",
      "Epoch: 3400 | Loss: 0.01472 | Test Loss: 0.01407 | Valid loss: 0.01443\n",
      "Epoch: 3600 | Loss: 0.01469 | Test Loss: 0.01405 | Valid loss: 0.01441\n",
      "Epoch: 3800 | Loss: 0.01466 | Test Loss: 0.01402 | Valid loss: 0.01438\n",
      "Epoch: 4000 | Loss: 0.01463 | Test Loss: 0.01399 | Valid loss: 0.01434\n",
      "Epoch: 4200 | Loss: 0.01459 | Test Loss: 0.01396 | Valid loss: 0.01431\n",
      "Epoch: 4400 | Loss: 0.01455 | Test Loss: 0.01392 | Valid loss: 0.01427\n",
      "Epoch: 4600 | Loss: 0.01451 | Test Loss: 0.01387 | Valid loss: 0.01422\n",
      "Epoch: 4800 | Loss: 0.01445 | Test Loss: 0.01382 | Valid loss: 0.01417\n",
      "Epoch: 5000 | Loss: 0.01439 | Test Loss: 0.01377 | Valid loss: 0.01411\n",
      "Epoch: 5200 | Loss: 0.01432 | Test Loss: 0.01370 | Valid loss: 0.01404\n",
      "Epoch: 5400 | Loss: 0.01424 | Test Loss: 0.01363 | Valid loss: 0.01396\n",
      "Epoch: 5600 | Loss: 0.01415 | Test Loss: 0.01354 | Valid loss: 0.01387\n",
      "Epoch: 5800 | Loss: 0.01404 | Test Loss: 0.01343 | Valid loss: 0.01375\n",
      "Epoch: 6000 | Loss: 0.01390 | Test Loss: 0.01330 | Valid loss: 0.01362\n",
      "Epoch: 6200 | Loss: 0.01374 | Test Loss: 0.01315 | Valid loss: 0.01345\n",
      "Epoch: 6400 | Loss: 0.01353 | Test Loss: 0.01295 | Valid loss: 0.01325\n",
      "Epoch: 6600 | Loss: 0.01328 | Test Loss: 0.01271 | Valid loss: 0.01300\n",
      "Epoch: 6800 | Loss: 0.01296 | Test Loss: 0.01241 | Valid loss: 0.01268\n",
      "Epoch: 7000 | Loss: 0.01257 | Test Loss: 0.01203 | Valid loss: 0.01228\n",
      "Epoch: 7200 | Loss: 0.01208 | Test Loss: 0.01157 | Valid loss: 0.01180\n",
      "Epoch: 7400 | Loss: 0.01150 | Test Loss: 0.01101 | Valid loss: 0.01121\n",
      "Epoch: 7600 | Loss: 0.01081 | Test Loss: 0.01036 | Valid loss: 0.01052\n",
      "Epoch: 7800 | Loss: 0.01003 | Test Loss: 0.00962 | Valid loss: 0.00975\n",
      "Epoch: 8000 | Loss: 0.00919 | Test Loss: 0.00881 | Valid loss: 0.00891\n",
      "Epoch: 8200 | Loss: 0.00830 | Test Loss: 0.00796 | Valid loss: 0.00803\n",
      "Epoch: 8400 | Loss: 0.00741 | Test Loss: 0.00710 | Valid loss: 0.00716\n",
      "Epoch: 8600 | Loss: 0.00656 | Test Loss: 0.00628 | Valid loss: 0.00634\n",
      "Epoch: 8800 | Loss: 0.00580 | Test Loss: 0.00556 | Valid loss: 0.00561\n",
      "Epoch: 9000 | Loss: 0.00516 | Test Loss: 0.00495 | Valid loss: 0.00500\n",
      "Epoch: 9200 | Loss: 0.00465 | Test Loss: 0.00447 | Valid loss: 0.00452\n",
      "Epoch: 9400 | Loss: 0.00426 | Test Loss: 0.00411 | Valid loss: 0.00416\n",
      "Epoch: 9600 | Loss: 0.00396 | Test Loss: 0.00383 | Valid loss: 0.00389\n",
      "Epoch: 9800 | Loss: 0.00374 | Test Loss: 0.00363 | Valid loss: 0.00369\n",
      "Epoch: 10000 | Loss: 0.00357 | Test Loss: 0.00347 | Valid loss: 0.00353\n",
      "Epoch: 10200 | Loss: 0.00343 | Test Loss: 0.00335 | Valid loss: 0.00341\n",
      "Epoch: 10400 | Loss: 0.00333 | Test Loss: 0.00325 | Valid loss: 0.00331\n",
      "Epoch: 10600 | Loss: 0.00324 | Test Loss: 0.00316 | Valid loss: 0.00322\n",
      "Epoch: 10800 | Loss: 0.00316 | Test Loss: 0.00309 | Valid loss: 0.00315\n",
      "Epoch: 11000 | Loss: 0.00312 | Test Loss: 0.00304 | Valid loss: 0.00311\n",
      "Epoch: 11200 | Loss: 0.00304 | Test Loss: 0.00298 | Valid loss: 0.00304\n",
      "Epoch: 11400 | Loss: 0.00299 | Test Loss: 0.00294 | Valid loss: 0.00299\n",
      "Epoch: 11600 | Loss: 0.00295 | Test Loss: 0.00289 | Valid loss: 0.00295\n",
      "Epoch: 11800 | Loss: 0.00291 | Test Loss: 0.00285 | Valid loss: 0.00291\n",
      "Epoch: 12000 | Loss: 0.00287 | Test Loss: 0.00283 | Valid loss: 0.00289\n",
      "Epoch: 12200 | Loss: 0.00284 | Test Loss: 0.00279 | Valid loss: 0.00284\n",
      "Early stopping at epoch: 12360\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 15000\n",
    "learning_rate = 0.001\n",
    "momentum = 0.3\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate,\n",
    "                                 weight_decay=1e-7)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_MT_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_MT_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_MT_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aaaeec11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 13.9972 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_MT_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "MT_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(MT_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fa1b8fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14113162984643648,\n",
       " 0.14093573865007625,\n",
       " 0.14150706285893597,\n",
       " 0.14094939812629195,\n",
       " 0.14113730263189933,\n",
       " 0.14113730263189933]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MT_rel_errors = []\n",
    "MT_rel_errors.append(MT_rel_error)\n",
    "\n",
    "MT_rel_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bdf7eeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.14113162984643648,\n",
       "  0.14093573865007625,\n",
       "  0.14150706285893597,\n",
       "  0.14094939812629195,\n",
       "  0.14113730263189933,\n",
       "  0.14113730263189933],\n",
       " 0.00018823017015562438)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MT_st_dev = statistics.pstdev(MT_rel_errors)\n",
    "\n",
    "MT_rel_errors, MT_st_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24596204",
   "metadata": {},
   "source": [
    "# Intagration - GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32d4670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 62\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "G_M_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    G_M_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "573acb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_G_M_train, y_train = X_G_M_train.to(device), y_train.to(device)\n",
    "X_G_M_test, y_test = X_G_M_test.to(device), y_test.to(device)\n",
    "X_G_M_valid, y_valid = X_G_M_valid.to(device), y_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9d13319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.85840 | Test Loss: 1.60044 | Valid Loss: 1.56695\n",
      "Epoch: 200 | Loss: 0.00303 | Test Loss: 0.00307 | Valid Loss: 0.00294\n",
      "Epoch: 400 | Loss: 0.00227 | Test Loss: 0.00231 | Valid Loss: 0.00222\n",
      "Epoch: 600 | Loss: 0.00197 | Test Loss: 0.00202 | Valid Loss: 0.00193\n",
      "Epoch: 800 | Loss: 0.00178 | Test Loss: 0.00183 | Valid Loss: 0.00174\n",
      "Epoch: 1000 | Loss: 0.00163 | Test Loss: 0.00169 | Valid Loss: 0.00160\n",
      "Epoch: 1200 | Loss: 0.00151 | Test Loss: 0.00156 | Valid Loss: 0.00148\n",
      "Epoch: 1400 | Loss: 0.00140 | Test Loss: 0.00146 | Valid Loss: 0.00138\n",
      "Epoch: 1600 | Loss: 0.00130 | Test Loss: 0.00135 | Valid Loss: 0.00129\n",
      "Epoch: 1800 | Loss: 0.00120 | Test Loss: 0.00125 | Valid Loss: 0.00119\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00101 | Test Loss: 0.00104 | Valid Loss: 0.00100\n",
      "Epoch: 2400 | Loss: 0.00093 | Test Loss: 0.00096 | Valid Loss: 0.00093\n",
      "Epoch: 2600 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 2800 | Loss: 0.00083 | Test Loss: 0.00086 | Valid Loss: 0.00083\n",
      "Epoch: 3000 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3200 | Loss: 0.00078 | Test Loss: 0.00080 | Valid Loss: 0.00078\n",
      "Epoch: 3400 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 3600 | Loss: 0.00073 | Test Loss: 0.00076 | Valid Loss: 0.00074\n",
      "Epoch: 3800 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00073\n",
      "Epoch: 4000 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 4200 | Loss: 0.00068 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Early stopping at epoch: 4303\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.91414 | Test Loss: 1.59218 | Valid Loss: 1.58507\n",
      "Epoch: 200 | Loss: 0.00346 | Test Loss: 0.00353 | Valid Loss: 0.00344\n",
      "Epoch: 400 | Loss: 0.00235 | Test Loss: 0.00236 | Valid Loss: 0.00235\n",
      "Epoch: 600 | Loss: 0.00181 | Test Loss: 0.00181 | Valid Loss: 0.00181\n",
      "Epoch: 800 | Loss: 0.00145 | Test Loss: 0.00147 | Valid Loss: 0.00146\n",
      "Epoch: 1000 | Loss: 0.00121 | Test Loss: 0.00124 | Valid Loss: 0.00122\n",
      "Epoch: 1200 | Loss: 0.00106 | Test Loss: 0.00109 | Valid Loss: 0.00107\n",
      "Epoch: 1400 | Loss: 0.00096 | Test Loss: 0.00099 | Valid Loss: 0.00098\n",
      "Epoch: 1600 | Loss: 0.00090 | Test Loss: 0.00093 | Valid Loss: 0.00092\n",
      "Epoch: 1800 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 2000 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 2200 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00080\n",
      "Epoch: 2400 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 2600 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 2800 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 3000 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 3200 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 3400 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00068\n",
      "Epoch: 3600 | Loss: 0.00067 | Test Loss: 0.00067 | Valid Loss: 0.00067\n",
      "Epoch: 3800 | Loss: 0.00066 | Test Loss: 0.00066 | Valid Loss: 0.00066\n",
      "Epoch: 4000 | Loss: 0.00065 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 4200 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00065\n",
      "Epoch: 4400 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 4600 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 4800 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 5000 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 5200 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 5400 | Loss: 0.00111 | Test Loss: 0.00114 | Valid Loss: 0.00117\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 5800 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00059\n",
      "Epoch: 6000 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Early stopping at epoch: 6714\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 2.95168 | Test Loss: 2.50264 | Valid Loss: 2.49421\n",
      "Epoch: 200 | Loss: 0.00375 | Test Loss: 0.00363 | Valid Loss: 0.00370\n",
      "Epoch: 400 | Loss: 0.00250 | Test Loss: 0.00240 | Valid Loss: 0.00248\n",
      "Epoch: 600 | Loss: 0.00193 | Test Loss: 0.00185 | Valid Loss: 0.00191\n",
      "Epoch: 800 | Loss: 0.00157 | Test Loss: 0.00151 | Valid Loss: 0.00156\n",
      "Epoch: 1000 | Loss: 0.00132 | Test Loss: 0.00127 | Valid Loss: 0.00131\n",
      "Epoch: 1200 | Loss: 0.00114 | Test Loss: 0.00110 | Valid Loss: 0.00113\n",
      "Epoch: 1400 | Loss: 0.00102 | Test Loss: 0.00098 | Valid Loss: 0.00101\n",
      "Epoch: 1600 | Loss: 0.00094 | Test Loss: 0.00091 | Valid Loss: 0.00093\n",
      "Epoch: 1800 | Loss: 0.00089 | Test Loss: 0.00085 | Valid Loss: 0.00088\n",
      "Epoch: 2000 | Loss: 0.00084 | Test Loss: 0.00081 | Valid Loss: 0.00083\n",
      "Epoch: 2200 | Loss: 0.00081 | Test Loss: 0.00078 | Valid Loss: 0.00080\n",
      "Epoch: 2400 | Loss: 0.00077 | Test Loss: 0.00075 | Valid Loss: 0.00077\n",
      "Epoch: 2600 | Loss: 0.00075 | Test Loss: 0.00073 | Valid Loss: 0.00074\n",
      "Epoch: 2800 | Loss: 0.00072 | Test Loss: 0.00070 | Valid Loss: 0.00072\n",
      "Epoch: 3000 | Loss: 0.00070 | Test Loss: 0.00068 | Valid Loss: 0.00069\n",
      "Epoch: 3200 | Loss: 0.00067 | Test Loss: 0.00066 | Valid Loss: 0.00067\n",
      "Epoch: 3400 | Loss: 0.00066 | Test Loss: 0.00064 | Valid Loss: 0.00065\n",
      "Epoch: 3600 | Loss: 0.00064 | Test Loss: 0.00062 | Valid Loss: 0.00064\n",
      "Epoch: 3800 | Loss: 0.00062 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 4000 | Loss: 0.00061 | Test Loss: 0.00059 | Valid Loss: 0.00061\n",
      "Epoch: 4200 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00060\n",
      "Epoch: 4400 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00059\n",
      "Epoch: 4600 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00058\n",
      "Epoch: 4800 | Loss: 0.00057 | Test Loss: 0.00055 | Valid Loss: 0.00057\n",
      "Epoch: 5000 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 5200 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 5400 | Loss: 0.00054 | Test Loss: 0.00053 | Valid Loss: 0.00055\n",
      "Epoch: 5600 | Loss: 0.00054 | Test Loss: 0.00052 | Valid Loss: 0.00054\n",
      "Epoch: 5800 | Loss: 0.00053 | Test Loss: 0.00051 | Valid Loss: 0.00053\n",
      "Epoch: 6000 | Loss: 0.00054 | Test Loss: 0.00050 | Valid Loss: 0.00053\n",
      "Epoch: 6200 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00052\n",
      "Epoch: 6400 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 6600 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00050\n",
      "Epoch: 6800 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00049\n",
      "Early stopping at epoch: 6998\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 1.53245 | Test Loss: 1.25168 | Valid Loss: 1.26741\n",
      "Epoch: 200 | Loss: 0.00293 | Test Loss: 0.00290 | Valid Loss: 0.00286\n",
      "Epoch: 400 | Loss: 0.00199 | Test Loss: 0.00197 | Valid Loss: 0.00196\n",
      "Epoch: 600 | Loss: 0.00153 | Test Loss: 0.00151 | Valid Loss: 0.00151\n",
      "Epoch: 800 | Loss: 0.00121 | Test Loss: 0.00120 | Valid Loss: 0.00120\n",
      "Epoch: 1000 | Loss: 0.00099 | Test Loss: 0.00098 | Valid Loss: 0.00099\n",
      "Epoch: 1200 | Loss: 0.00085 | Test Loss: 0.00083 | Valid Loss: 0.00086\n",
      "Epoch: 1400 | Loss: 0.00077 | Test Loss: 0.00075 | Valid Loss: 0.00078\n",
      "Epoch: 1600 | Loss: 0.00072 | Test Loss: 0.00070 | Valid Loss: 0.00072\n",
      "Epoch: 1800 | Loss: 0.00068 | Test Loss: 0.00066 | Valid Loss: 0.00068\n",
      "Epoch: 2000 | Loss: 0.00064 | Test Loss: 0.00062 | Valid Loss: 0.00065\n",
      "Epoch: 2200 | Loss: 0.00062 | Test Loss: 0.00059 | Valid Loss: 0.00062\n",
      "Epoch: 2400 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00060\n",
      "Epoch: 2600 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00058\n",
      "Epoch: 2800 | Loss: 0.00056 | Test Loss: 0.00054 | Valid Loss: 0.00057\n",
      "Epoch: 3000 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00056\n",
      "Epoch: 3200 | Loss: 0.00054 | Test Loss: 0.00052 | Valid Loss: 0.00055\n",
      "Epoch: 3400 | Loss: 0.00053 | Test Loss: 0.00051 | Valid Loss: 0.00054\n",
      "Epoch: 3600 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00053\n",
      "Epoch: 3800 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00052\n",
      "Epoch: 4000 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 4200 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00050\n",
      "Epoch: 4400 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00047 | Test Loss: 0.00046 | Valid Loss: 0.00049\n",
      "Epoch: 4800 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00047\n",
      "Epoch: 5000 | Loss: 0.00045 | Test Loss: 0.00044 | Valid Loss: 0.00046\n",
      "Epoch: 5200 | Loss: 0.00044 | Test Loss: 0.00043 | Valid Loss: 0.00045\n",
      "Epoch: 5400 | Loss: 0.00043 | Test Loss: 0.00042 | Valid Loss: 0.00044\n",
      "Early stopping at epoch: 5427\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 1.90268 | Test Loss: 1.49036 | Valid Loss: 1.50115\n",
      "Epoch: 200 | Loss: 0.00326 | Test Loss: 0.00332 | Valid Loss: 0.00317\n",
      "Epoch: 400 | Loss: 0.00213 | Test Loss: 0.00221 | Valid Loss: 0.00213\n",
      "Epoch: 600 | Loss: 0.00163 | Test Loss: 0.00169 | Valid Loss: 0.00164\n",
      "Epoch: 800 | Loss: 0.00128 | Test Loss: 0.00132 | Valid Loss: 0.00129\n",
      "Epoch: 1000 | Loss: 0.00104 | Test Loss: 0.00108 | Valid Loss: 0.00106\n",
      "Epoch: 1200 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00090\n",
      "Epoch: 1400 | Loss: 0.00080 | Test Loss: 0.00081 | Valid Loss: 0.00080\n",
      "Epoch: 1600 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00074\n",
      "Epoch: 1800 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00070\n",
      "Epoch: 2000 | Loss: 0.00067 | Test Loss: 0.00067 | Valid Loss: 0.00068\n",
      "Epoch: 2200 | Loss: 0.00065 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 2400 | Loss: 0.00063 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 2600 | Loss: 0.00061 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 2800 | Loss: 0.00060 | Test Loss: 0.00059 | Valid Loss: 0.00060\n",
      "Epoch: 3000 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00059\n",
      "Epoch: 3200 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 3400 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 3600 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 3800 | Loss: 0.00054 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 4000 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 4200 | Loss: 0.00052 | Test Loss: 0.00051 | Valid Loss: 0.00052\n",
      "Epoch: 4400 | Loss: 0.00051 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 4600 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Epoch: 4800 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00049\n",
      "Epoch: 5000 | Loss: 0.00048 | Test Loss: 0.00047 | Valid Loss: 0.00048\n",
      "Epoch: 5200 | Loss: 0.00047 | Test Loss: 0.00046 | Valid Loss: 0.00047\n",
      "Epoch: 5400 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 5600 | Loss: 0.00045 | Test Loss: 0.00044 | Valid Loss: 0.00045\n",
      "Epoch: 5800 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 6000 | Loss: 0.00043 | Test Loss: 0.00042 | Valid Loss: 0.00043\n",
      "Early stopping at epoch: 6115\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 1.23072 | Test Loss: 0.99142 | Valid Loss: 0.98642\n",
      "Epoch: 200 | Loss: 0.00257 | Test Loss: 0.00252 | Valid Loss: 0.00255\n",
      "Epoch: 400 | Loss: 0.00181 | Test Loss: 0.00177 | Valid Loss: 0.00180\n",
      "Epoch: 600 | Loss: 0.00135 | Test Loss: 0.00133 | Valid Loss: 0.00134\n",
      "Epoch: 800 | Loss: 0.00103 | Test Loss: 0.00101 | Valid Loss: 0.00101\n",
      "Epoch: 1000 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 1200 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00068\n",
      "Epoch: 1400 | Loss: 0.00066 | Test Loss: 0.00065 | Valid Loss: 0.00062\n",
      "Epoch: 1600 | Loss: 0.00062 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 1800 | Loss: 0.00060 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 2000 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 2200 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "Epoch: 2400 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00053\n",
      "Epoch: 2600 | Loss: 0.00054 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 2800 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 3000 | Loss: 0.00052 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 3200 | Loss: 0.00051 | Test Loss: 0.00050 | Valid Loss: 0.00049\n",
      "Epoch: 3400 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00048\n",
      "Epoch: 3600 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 3800 | Loss: 0.00048 | Test Loss: 0.00047 | Valid Loss: 0.00046\n",
      "Epoch: 4000 | Loss: 0.00047 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 4200 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 4400 | Loss: 0.00045 | Test Loss: 0.00044 | Valid Loss: 0.00043\n",
      "Epoch: 4600 | Loss: 0.00044 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 4800 | Loss: 0.00043 | Test Loss: 0.00042 | Valid Loss: 0.00041\n",
      "Epoch: 5000 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 5200 | Loss: 0.00041 | Test Loss: 0.00040 | Valid Loss: 0.00039\n",
      "Epoch: 5400 | Loss: 0.00040 | Test Loss: 0.00039 | Valid Loss: 0.00039\n",
      "Epoch: 5600 | Loss: 0.00041 | Test Loss: 0.00039 | Valid Loss: 0.00039\n",
      "Early stopping at epoch: 5622\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 2.34636 | Test Loss: 1.99968 | Valid Loss: 2.00503\n",
      "Epoch: 200 | Loss: 0.00333 | Test Loss: 0.00334 | Valid Loss: 0.00314\n",
      "Epoch: 400 | Loss: 0.00208 | Test Loss: 0.00211 | Valid Loss: 0.00203\n",
      "Epoch: 600 | Loss: 0.00166 | Test Loss: 0.00170 | Valid Loss: 0.00163\n",
      "Epoch: 800 | Loss: 0.00135 | Test Loss: 0.00139 | Valid Loss: 0.00132\n",
      "Epoch: 1000 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00106\n",
      "Epoch: 1200 | Loss: 0.00088 | Test Loss: 0.00092 | Valid Loss: 0.00085\n",
      "Epoch: 1400 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "Epoch: 1600 | Loss: 0.00068 | Test Loss: 0.00070 | Valid Loss: 0.00065\n",
      "Epoch: 1800 | Loss: 0.00065 | Test Loss: 0.00066 | Valid Loss: 0.00062\n",
      "Epoch: 2000 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00059\n",
      "Epoch: 2200 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Epoch: 2400 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00056\n",
      "Epoch: 2600 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00055\n",
      "Epoch: 2800 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00054\n",
      "Epoch: 3000 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00053\n",
      "Epoch: 3200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00052\n",
      "Epoch: 3400 | Loss: 0.00054 | Test Loss: 0.00054 | Valid Loss: 0.00051\n",
      "Epoch: 3600 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00050\n",
      "Epoch: 3800 | Loss: 0.00052 | Test Loss: 0.00052 | Valid Loss: 0.00049\n",
      "Epoch: 4000 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00049\n",
      "Epoch: 4200 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 4400 | Loss: 0.00049 | Test Loss: 0.00049 | Valid Loss: 0.00047\n",
      "Epoch: 4600 | Loss: 0.00048 | Test Loss: 0.00048 | Valid Loss: 0.00046\n",
      "Epoch: 4800 | Loss: 0.00047 | Test Loss: 0.00047 | Valid Loss: 0.00045\n",
      "Epoch: 5000 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00044\n",
      "Epoch: 5200 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00043\n",
      "Epoch: 5400 | Loss: 0.00044 | Test Loss: 0.00044 | Valid Loss: 0.00042\n",
      "Epoch: 5600 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00041\n",
      "Epoch: 5800 | Loss: 0.00042 | Test Loss: 0.00042 | Valid Loss: 0.00040\n",
      "Early stopping at epoch: 5894\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 1.14271 | Test Loss: 0.88324 | Valid Loss: 0.86311\n",
      "Epoch: 200 | Loss: 0.00293 | Test Loss: 0.00297 | Valid Loss: 0.00289\n",
      "Epoch: 400 | Loss: 0.00196 | Test Loss: 0.00197 | Valid Loss: 0.00195\n",
      "Epoch: 600 | Loss: 0.00133 | Test Loss: 0.00133 | Valid Loss: 0.00133\n",
      "Epoch: 800 | Loss: 0.00096 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 1000 | Loss: 0.00077 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 1200 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 1400 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00064\n",
      "Epoch: 1600 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 1800 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 2000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 2200 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00056\n",
      "Epoch: 2400 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 2600 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00054\n",
      "Epoch: 2800 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00052\n",
      "Epoch: 3000 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 3200 | Loss: 0.00051 | Test Loss: 0.00052 | Valid Loss: 0.00050\n",
      "Epoch: 3400 | Loss: 0.00050 | Test Loss: 0.00051 | Valid Loss: 0.00049\n",
      "Epoch: 3600 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 3800 | Loss: 0.00047 | Test Loss: 0.00049 | Valid Loss: 0.00047\n",
      "Epoch: 4000 | Loss: 0.00046 | Test Loss: 0.00048 | Valid Loss: 0.00046\n",
      "Epoch: 4200 | Loss: 0.00045 | Test Loss: 0.00047 | Valid Loss: 0.00045\n",
      "Epoch: 4400 | Loss: 0.00044 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 4600 | Loss: 0.00044 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 4800 | Loss: 0.00042 | Test Loss: 0.00044 | Valid Loss: 0.00043\n",
      "Epoch: 5000 | Loss: 0.00042 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 5200 | Loss: 0.00041 | Test Loss: 0.00042 | Valid Loss: 0.00041\n",
      "Epoch: 5400 | Loss: 0.00044 | Test Loss: 0.00054 | Valid Loss: 0.00052\n",
      "Early stopping at epoch: 5417\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 0.51674 | Test Loss: 0.36463 | Valid Loss: 0.36408\n",
      "Epoch: 200 | Loss: 0.00206 | Test Loss: 0.00209 | Valid Loss: 0.00210\n",
      "Epoch: 400 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00128\n",
      "Epoch: 600 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00088\n",
      "Epoch: 800 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00070\n",
      "Epoch: 1000 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 1200 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 1400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 1600 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 1800 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00053\n",
      "Epoch: 2000 | Loss: 0.00052 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 2200 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 2400 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 2600 | Loss: 0.00049 | Test Loss: 0.00049 | Valid Loss: 0.00047\n",
      "Epoch: 2800 | Loss: 0.00047 | Test Loss: 0.00048 | Valid Loss: 0.00046\n",
      "Epoch: 3000 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00045\n",
      "Epoch: 3200 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Early stopping at epoch: 3250\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.33934 | Test Loss: 1.05312 | Valid Loss: 1.07535\n",
      "Epoch: 200 | Loss: 0.00287 | Test Loss: 0.00292 | Valid Loss: 0.00287\n",
      "Epoch: 400 | Loss: 0.00181 | Test Loss: 0.00186 | Valid Loss: 0.00183\n",
      "Epoch: 600 | Loss: 0.00131 | Test Loss: 0.00134 | Valid Loss: 0.00132\n",
      "Epoch: 800 | Loss: 0.00098 | Test Loss: 0.00100 | Valid Loss: 0.00099\n",
      "Epoch: 1000 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00079\n",
      "Epoch: 1200 | Loss: 0.00070 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 1400 | Loss: 0.00065 | Test Loss: 0.00065 | Valid Loss: 0.00064\n",
      "Epoch: 1600 | Loss: 0.00063 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 1800 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 2000 | Loss: 0.00059 | Test Loss: 0.00058 | Valid Loss: 0.00059\n",
      "Epoch: 2200 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 2400 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 2600 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 2800 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 3000 | Loss: 0.00054 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 3200 | Loss: 0.00053 | Test Loss: 0.00051 | Valid Loss: 0.00052\n",
      "Epoch: 3400 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 3600 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Epoch: 3800 | Loss: 0.00050 | Test Loss: 0.00048 | Valid Loss: 0.00050\n",
      "Epoch: 4000 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00049\n",
      "Epoch: 4200 | Loss: 0.00047 | Test Loss: 0.00046 | Valid Loss: 0.00048\n",
      "Epoch: 4400 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00047\n",
      "Epoch: 4600 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 4800 | Loss: 0.00046 | Test Loss: 0.00044 | Valid Loss: 0.00046\n",
      "Epoch: 5000 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 5200 | Loss: 0.00042 | Test Loss: 0.00042 | Valid Loss: 0.00043\n",
      "Epoch: 5400 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Early stopping at epoch: 5423\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 1.52810 | Test Loss: 1.21658 | Valid Loss: 1.21962\n",
      "Epoch: 200 | Loss: 0.00321 | Test Loss: 0.00334 | Valid Loss: 0.00332\n",
      "Epoch: 400 | Loss: 0.00200 | Test Loss: 0.00208 | Valid Loss: 0.00211\n",
      "Epoch: 600 | Loss: 0.00145 | Test Loss: 0.00150 | Valid Loss: 0.00152\n",
      "Epoch: 800 | Loss: 0.00111 | Test Loss: 0.00114 | Valid Loss: 0.00115\n",
      "Epoch: 1000 | Loss: 0.00091 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 1200 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00079\n",
      "Epoch: 1400 | Loss: 0.00073 | Test Loss: 0.00071 | Valid Loss: 0.00072\n",
      "Epoch: 1600 | Loss: 0.00068 | Test Loss: 0.00067 | Valid Loss: 0.00067\n",
      "Epoch: 1800 | Loss: 0.00065 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 2000 | Loss: 0.00063 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 2200 | Loss: 0.00061 | Test Loss: 0.00059 | Valid Loss: 0.00060\n",
      "Epoch: 2400 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 2600 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 2800 | Loss: 0.00056 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 3000 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 3200 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 3400 | Loss: 0.00052 | Test Loss: 0.00051 | Valid Loss: 0.00052\n",
      "Epoch: 3600 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 3800 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Epoch: 4000 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00049\n",
      "Epoch: 4200 | Loss: 0.00048 | Test Loss: 0.00046 | Valid Loss: 0.00048\n",
      "Epoch: 4400 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 4600 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 4800 | Loss: 0.00044 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 5000 | Loss: 0.00043 | Test Loss: 0.00042 | Valid Loss: 0.00043\n",
      "Epoch: 5200 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 5400 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 5600 | Loss: 0.00040 | Test Loss: 0.00039 | Valid Loss: 0.00040\n",
      "Early stopping at epoch: 5656\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 0.84157 | Test Loss: 0.63024 | Valid Loss: 0.63104\n",
      "Epoch: 200 | Loss: 0.00227 | Test Loss: 0.00231 | Valid Loss: 0.00230\n",
      "Epoch: 400 | Loss: 0.00152 | Test Loss: 0.00153 | Valid Loss: 0.00154\n",
      "Epoch: 600 | Loss: 0.00115 | Test Loss: 0.00115 | Valid Loss: 0.00114\n",
      "Epoch: 800 | Loss: 0.00095 | Test Loss: 0.00094 | Valid Loss: 0.00092\n",
      "Epoch: 1000 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 1200 | Loss: 0.00075 | Test Loss: 0.00075 | Valid Loss: 0.00072\n",
      "Epoch: 1400 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00067\n",
      "Epoch: 1600 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 1800 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 2000 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 2200 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "Epoch: 2400 | Loss: 0.00054 | Test Loss: 0.00054 | Valid Loss: 0.00053\n",
      "Epoch: 2600 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 2800 | Loss: 0.00052 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 3000 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 3200 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00049\n",
      "Epoch: 3400 | Loss: 0.00049 | Test Loss: 0.00049 | Valid Loss: 0.00048\n",
      "Epoch: 3600 | Loss: 0.00048 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 3800 | Loss: 0.00047 | Test Loss: 0.00048 | Valid Loss: 0.00046\n",
      "Epoch: 4000 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00046\n",
      "Epoch: 4200 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 4400 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 4600 | Loss: 0.00119 | Test Loss: 0.00126 | Valid Loss: 0.00130\n",
      "Epoch: 4800 | Loss: 0.00043 | Test Loss: 0.00044 | Valid Loss: 0.00043\n",
      "Epoch: 5000 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 5200 | Loss: 0.00043 | Test Loss: 0.00045 | Valid Loss: 0.00043\n",
      "Epoch: 5400 | Loss: 0.00041 | Test Loss: 0.00042 | Valid Loss: 0.00041\n",
      "Epoch: 5600 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 5800 | Loss: 0.00040 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Early stopping at epoch: 5827\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 0.83795 | Test Loss: 0.60600 | Valid Loss: 0.60814\n",
      "Epoch: 200 | Loss: 0.00221 | Test Loss: 0.00221 | Valid Loss: 0.00220\n",
      "Epoch: 400 | Loss: 0.00150 | Test Loss: 0.00150 | Valid Loss: 0.00149\n",
      "Epoch: 600 | Loss: 0.00117 | Test Loss: 0.00116 | Valid Loss: 0.00115\n",
      "Epoch: 800 | Loss: 0.00098 | Test Loss: 0.00097 | Valid Loss: 0.00096\n",
      "Epoch: 1000 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 1200 | Loss: 0.00080 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 1400 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00072\n",
      "Epoch: 1600 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00067\n",
      "Epoch: 1800 | Loss: 0.00065 | Test Loss: 0.00065 | Valid Loss: 0.00063\n",
      "Epoch: 2000 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 2200 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 2400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 2600 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 2800 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00054\n",
      "Epoch: 3000 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00053\n",
      "Epoch: 3200 | Loss: 0.00052 | Test Loss: 0.00054 | Valid Loss: 0.00052\n",
      "Epoch: 3400 | Loss: 0.00051 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 3600 | Loss: 0.00051 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 3800 | Loss: 0.00050 | Test Loss: 0.00052 | Valid Loss: 0.00050\n",
      "Epoch: 4000 | Loss: 0.00049 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 4200 | Loss: 0.00048 | Test Loss: 0.00050 | Valid Loss: 0.00049\n",
      "Early stopping at epoch: 4385\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.15467 | Test Loss: 1.81404 | Valid Loss: 1.80662\n",
      "Epoch: 200 | Loss: 0.00305 | Test Loss: 0.00305 | Valid Loss: 0.00300\n",
      "Epoch: 400 | Loss: 0.00218 | Test Loss: 0.00217 | Valid Loss: 0.00217\n",
      "Epoch: 600 | Loss: 0.00179 | Test Loss: 0.00178 | Valid Loss: 0.00179\n",
      "Epoch: 800 | Loss: 0.00150 | Test Loss: 0.00149 | Valid Loss: 0.00149\n",
      "Epoch: 1000 | Loss: 0.00126 | Test Loss: 0.00126 | Valid Loss: 0.00124\n",
      "Epoch: 1200 | Loss: 0.00106 | Test Loss: 0.00107 | Valid Loss: 0.00104\n",
      "Epoch: 1400 | Loss: 0.00093 | Test Loss: 0.00094 | Valid Loss: 0.00090\n",
      "Epoch: 1600 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00083\n",
      "Epoch: 1800 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00079\n",
      "Epoch: 2000 | Loss: 0.00076 | Test Loss: 0.00080 | Valid Loss: 0.00075\n",
      "Epoch: 2200 | Loss: 0.00074 | Test Loss: 0.00077 | Valid Loss: 0.00073\n",
      "Epoch: 2400 | Loss: 0.00072 | Test Loss: 0.00075 | Valid Loss: 0.00071\n",
      "Epoch: 2600 | Loss: 0.00070 | Test Loss: 0.00073 | Valid Loss: 0.00069\n",
      "Epoch: 2800 | Loss: 0.00068 | Test Loss: 0.00072 | Valid Loss: 0.00067\n",
      "Epoch: 3000 | Loss: 0.00067 | Test Loss: 0.00070 | Valid Loss: 0.00066\n",
      "Epoch: 3200 | Loss: 0.00066 | Test Loss: 0.00069 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00065 | Test Loss: 0.00068 | Valid Loss: 0.00064\n",
      "Epoch: 3600 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00063\n",
      "Epoch: 3800 | Loss: 0.00063 | Test Loss: 0.00066 | Valid Loss: 0.00062\n",
      "Epoch: 4000 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00061\n",
      "Epoch: 4200 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00060\n",
      "Epoch: 4400 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00060\n",
      "Epoch: 4600 | Loss: 0.00060 | Test Loss: 0.00063 | Valid Loss: 0.00059\n",
      "Epoch: 4800 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00058\n",
      "Epoch: 5000 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Epoch: 5200 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00084 | Test Loss: 0.00096 | Valid Loss: 0.00092\n",
      "Early stopping at epoch: 5606\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.55124 | Test Loss: 1.25706 | Valid Loss: 1.24873\n",
      "Epoch: 200 | Loss: 0.00302 | Test Loss: 0.00300 | Valid Loss: 0.00294\n",
      "Epoch: 400 | Loss: 0.00214 | Test Loss: 0.00217 | Valid Loss: 0.00213\n",
      "Epoch: 600 | Loss: 0.00177 | Test Loss: 0.00181 | Valid Loss: 0.00178\n",
      "Epoch: 800 | Loss: 0.00157 | Test Loss: 0.00161 | Valid Loss: 0.00157\n",
      "Epoch: 1000 | Loss: 0.00144 | Test Loss: 0.00149 | Valid Loss: 0.00145\n",
      "Epoch: 1200 | Loss: 0.00135 | Test Loss: 0.00140 | Valid Loss: 0.00137\n",
      "Epoch: 1400 | Loss: 0.00128 | Test Loss: 0.00133 | Valid Loss: 0.00130\n",
      "Epoch: 1600 | Loss: 0.00122 | Test Loss: 0.00127 | Valid Loss: 0.00124\n",
      "Epoch: 1800 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00119\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00115 | Valid Loss: 0.00113\n",
      "Epoch: 2200 | Loss: 0.00105 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00100 | Test Loss: 0.00105 | Valid Loss: 0.00103\n",
      "Epoch: 2600 | Loss: 0.00096 | Test Loss: 0.00100 | Valid Loss: 0.00099\n",
      "Epoch: 2800 | Loss: 0.00092 | Test Loss: 0.00097 | Valid Loss: 0.00095\n",
      "Epoch: 3000 | Loss: 0.00089 | Test Loss: 0.00093 | Valid Loss: 0.00092\n",
      "Epoch: 3200 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 3400 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00087\n",
      "Epoch: 3600 | Loss: 0.00080 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 3800 | Loss: 0.00078 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 4000 | Loss: 0.00075 | Test Loss: 0.00080 | Valid Loss: 0.00077\n",
      "Epoch: 4200 | Loss: 0.00074 | Test Loss: 0.00077 | Valid Loss: 0.00074\n",
      "Epoch: 4400 | Loss: 0.00104 | Test Loss: 0.00087 | Valid Loss: 0.00089\n",
      "Epoch: 4600 | Loss: 0.00069 | Test Loss: 0.00074 | Valid Loss: 0.00070\n",
      "Epoch: 4800 | Loss: 0.00067 | Test Loss: 0.00071 | Valid Loss: 0.00068\n",
      "Epoch: 5000 | Loss: 0.00065 | Test Loss: 0.00069 | Valid Loss: 0.00066\n",
      "Early stopping at epoch: 5086\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 8000\n",
    "learning_rate = 0.0025\n",
    "momentum = 0.8\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_M_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_M_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_M_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_M_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1d894d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 4.7960 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_M_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_M_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_M_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_M_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948ef4c",
   "metadata": {},
   "source": [
    "# Integration - G + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e7242466",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 93\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "G_MT_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    G_MT_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7781c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_G_MT_train, y_train = X_G_MT_train.to(device), y_train.to(device)\n",
    "X_G_MT_test, y_test = X_G_MT_test.to(device), y_test.to(device)\n",
    "X_G_MT_valid, y_valid = X_G_MT_valid.to(device), y_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "563e59ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.50762 | Test Loss: 1.36948 | Valid Loss: 1.35782\n",
      "Epoch: 200 | Loss: 0.00298 | Test Loss: 0.00312 | Valid Loss: 0.00293\n",
      "Epoch: 400 | Loss: 0.00265 | Test Loss: 0.00275 | Valid Loss: 0.00263\n",
      "Epoch: 600 | Loss: 0.00250 | Test Loss: 0.00257 | Valid Loss: 0.00249\n",
      "Epoch: 800 | Loss: 0.00237 | Test Loss: 0.00243 | Valid Loss: 0.00235\n",
      "Epoch: 1000 | Loss: 0.00223 | Test Loss: 0.00229 | Valid Loss: 0.00222\n",
      "Epoch: 1200 | Loss: 0.00209 | Test Loss: 0.00215 | Valid Loss: 0.00208\n",
      "Epoch: 1400 | Loss: 0.00196 | Test Loss: 0.00201 | Valid Loss: 0.00194\n",
      "Epoch: 1600 | Loss: 0.00184 | Test Loss: 0.00189 | Valid Loss: 0.00182\n",
      "Epoch: 1800 | Loss: 0.00174 | Test Loss: 0.00178 | Valid Loss: 0.00172\n",
      "Epoch: 2000 | Loss: 0.00165 | Test Loss: 0.00170 | Valid Loss: 0.00164\n",
      "Epoch: 2200 | Loss: 0.00158 | Test Loss: 0.00162 | Valid Loss: 0.00157\n",
      "Epoch: 2400 | Loss: 0.00152 | Test Loss: 0.00156 | Valid Loss: 0.00151\n",
      "Epoch: 2600 | Loss: 0.00146 | Test Loss: 0.00150 | Valid Loss: 0.00146\n",
      "Epoch: 2800 | Loss: 0.00141 | Test Loss: 0.00145 | Valid Loss: 0.00141\n",
      "Epoch: 3000 | Loss: 0.00136 | Test Loss: 0.00139 | Valid Loss: 0.00136\n",
      "Epoch: 3200 | Loss: 0.00131 | Test Loss: 0.00134 | Valid Loss: 0.00131\n",
      "Epoch: 3400 | Loss: 0.00126 | Test Loss: 0.00129 | Valid Loss: 0.00126\n",
      "Epoch: 3600 | Loss: 0.00121 | Test Loss: 0.00124 | Valid Loss: 0.00121\n",
      "Epoch: 3800 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00117\n",
      "Epoch: 4000 | Loss: 0.00112 | Test Loss: 0.00115 | Valid Loss: 0.00112\n",
      "Epoch: 4200 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 4400 | Loss: 0.00103 | Test Loss: 0.00106 | Valid Loss: 0.00104\n",
      "Epoch: 4600 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00099\n",
      "Epoch: 4800 | Loss: 0.00095 | Test Loss: 0.00097 | Valid Loss: 0.00095\n",
      "Epoch: 5000 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00090\n",
      "Epoch: 5200 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 5400 | Loss: 0.00081 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 5600 | Loss: 0.00076 | Test Loss: 0.00077 | Valid Loss: 0.00076\n",
      "Epoch: 5800 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 6000 | Loss: 0.00089 | Test Loss: 0.00072 | Valid Loss: 0.00072\n",
      "Epoch: 6200 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Early stopping at epoch: 6256\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.77408 | Test Loss: 1.59646 | Valid Loss: 1.56514\n",
      "Epoch: 200 | Loss: 0.00228 | Test Loss: 0.00219 | Valid Loss: 0.00226\n",
      "Epoch: 400 | Loss: 0.00159 | Test Loss: 0.00156 | Valid Loss: 0.00157\n",
      "Epoch: 600 | Loss: 0.00148 | Test Loss: 0.00147 | Valid Loss: 0.00146\n",
      "Epoch: 800 | Loss: 0.00143 | Test Loss: 0.00142 | Valid Loss: 0.00140\n",
      "Epoch: 1000 | Loss: 0.00139 | Test Loss: 0.00139 | Valid Loss: 0.00137\n",
      "Epoch: 1200 | Loss: 0.00136 | Test Loss: 0.00136 | Valid Loss: 0.00134\n",
      "Epoch: 1400 | Loss: 0.00133 | Test Loss: 0.00134 | Valid Loss: 0.00132\n",
      "Epoch: 1600 | Loss: 0.00131 | Test Loss: 0.00133 | Valid Loss: 0.00130\n",
      "Epoch: 1800 | Loss: 0.00130 | Test Loss: 0.00131 | Valid Loss: 0.00129\n",
      "Epoch: 2000 | Loss: 0.00128 | Test Loss: 0.00129 | Valid Loss: 0.00127\n",
      "Epoch: 2200 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00125\n",
      "Epoch: 2400 | Loss: 0.00124 | Test Loss: 0.00126 | Valid Loss: 0.00124\n",
      "Epoch: 2600 | Loss: 0.00123 | Test Loss: 0.00125 | Valid Loss: 0.00122\n",
      "Epoch: 2800 | Loss: 0.00121 | Test Loss: 0.00123 | Valid Loss: 0.00120\n",
      "Epoch: 3000 | Loss: 0.00119 | Test Loss: 0.00121 | Valid Loss: 0.00119\n",
      "Epoch: 3200 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00117\n",
      "Epoch: 3400 | Loss: 0.00115 | Test Loss: 0.00117 | Valid Loss: 0.00115\n",
      "Epoch: 3600 | Loss: 0.00113 | Test Loss: 0.00115 | Valid Loss: 0.00113\n",
      "Epoch: 3800 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00111\n",
      "Epoch: 4000 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 4200 | Loss: 0.00105 | Test Loss: 0.00107 | Valid Loss: 0.00106\n",
      "Epoch: 4400 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00103\n",
      "Epoch: 4600 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00100\n",
      "Epoch: 4800 | Loss: 0.00096 | Test Loss: 0.00098 | Valid Loss: 0.00096\n",
      "Epoch: 5000 | Loss: 0.00092 | Test Loss: 0.00094 | Valid Loss: 0.00093\n",
      "Epoch: 5200 | Loss: 0.00089 | Test Loss: 0.00096 | Valid Loss: 0.00095\n",
      "Epoch: 5400 | Loss: 0.00087 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5600 | Loss: 0.00080 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 5800 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 6000 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00072\n",
      "Epoch: 6200 | Loss: 0.00067 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6400 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 6600 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 6800 | Loss: 0.00054 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 7000 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 7200 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00047\n",
      "Early stopping at epoch: 7295\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 0.66839 | Test Loss: 0.59073 | Valid Loss: 0.58560\n",
      "Epoch: 200 | Loss: 0.00185 | Test Loss: 0.00179 | Valid Loss: 0.00180\n",
      "Epoch: 400 | Loss: 0.00133 | Test Loss: 0.00127 | Valid Loss: 0.00128\n",
      "Epoch: 600 | Loss: 0.00127 | Test Loss: 0.00122 | Valid Loss: 0.00123\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00119 | Valid Loss: 0.00120\n",
      "Epoch: 1000 | Loss: 0.00121 | Test Loss: 0.00116 | Valid Loss: 0.00118\n",
      "Epoch: 1200 | Loss: 0.00119 | Test Loss: 0.00114 | Valid Loss: 0.00117\n",
      "Epoch: 1400 | Loss: 0.00117 | Test Loss: 0.00113 | Valid Loss: 0.00115\n",
      "Epoch: 1600 | Loss: 0.00115 | Test Loss: 0.00111 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00113 | Test Loss: 0.00110 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00112 | Test Loss: 0.00108 | Valid Loss: 0.00110\n",
      "Epoch: 2200 | Loss: 0.00110 | Test Loss: 0.00107 | Valid Loss: 0.00109\n",
      "Epoch: 2400 | Loss: 0.00108 | Test Loss: 0.00105 | Valid Loss: 0.00107\n",
      "Epoch: 2600 | Loss: 0.00106 | Test Loss: 0.00103 | Valid Loss: 0.00105\n",
      "Epoch: 2800 | Loss: 0.00103 | Test Loss: 0.00101 | Valid Loss: 0.00103\n",
      "Epoch: 3000 | Loss: 0.00101 | Test Loss: 0.00098 | Valid Loss: 0.00100\n",
      "Epoch: 3200 | Loss: 0.00098 | Test Loss: 0.00096 | Valid Loss: 0.00098\n",
      "Epoch: 3400 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00095\n",
      "Epoch: 3600 | Loss: 0.00092 | Test Loss: 0.00090 | Valid Loss: 0.00092\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00087 | Valid Loss: 0.00089\n",
      "Epoch: 4000 | Loss: 0.00086 | Test Loss: 0.00083 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00082 | Test Loss: 0.00080 | Valid Loss: 0.00082\n",
      "Epoch: 4400 | Loss: 0.00079 | Test Loss: 0.00077 | Valid Loss: 0.00079\n",
      "Epoch: 4600 | Loss: 0.00084 | Test Loss: 0.00075 | Valid Loss: 0.00078\n",
      "Epoch: 4800 | Loss: 0.00070 | Test Loss: 0.00068 | Valid Loss: 0.00070\n",
      "Epoch: 5000 | Loss: 0.00067 | Test Loss: 0.00064 | Valid Loss: 0.00066\n",
      "Epoch: 5200 | Loss: 0.00063 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "Epoch: 5400 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 5600 | Loss: 0.00085 | Test Loss: 0.00067 | Valid Loss: 0.00070\n",
      "Epoch: 5800 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Early stopping at epoch: 5880\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 2.32113 | Test Loss: 2.12131 | Valid Loss: 2.11415\n",
      "Epoch: 200 | Loss: 0.00191 | Test Loss: 0.00192 | Valid Loss: 0.00188\n",
      "Epoch: 400 | Loss: 0.00146 | Test Loss: 0.00147 | Valid Loss: 0.00144\n",
      "Epoch: 600 | Loss: 0.00134 | Test Loss: 0.00135 | Valid Loss: 0.00133\n",
      "Epoch: 800 | Loss: 0.00128 | Test Loss: 0.00129 | Valid Loss: 0.00127\n",
      "Epoch: 1000 | Loss: 0.00123 | Test Loss: 0.00124 | Valid Loss: 0.00123\n",
      "Epoch: 1200 | Loss: 0.00120 | Test Loss: 0.00120 | Valid Loss: 0.00119\n",
      "Epoch: 1400 | Loss: 0.00116 | Test Loss: 0.00116 | Valid Loss: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00114 | Test Loss: 0.00113 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00112 | Test Loss: 0.00111 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00109 | Test Loss: 0.00109 | Valid Loss: 0.00110\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00107 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00105 | Valid Loss: 0.00106\n",
      "Epoch: 2600 | Loss: 0.00104 | Test Loss: 0.00103 | Valid Loss: 0.00105\n",
      "Epoch: 2800 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 3000 | Loss: 0.00100 | Test Loss: 0.00100 | Valid Loss: 0.00101\n",
      "Epoch: 3200 | Loss: 0.00098 | Test Loss: 0.00098 | Valid Loss: 0.00099\n",
      "Epoch: 3400 | Loss: 0.00096 | Test Loss: 0.00095 | Valid Loss: 0.00097\n",
      "Epoch: 3600 | Loss: 0.00093 | Test Loss: 0.00093 | Valid Loss: 0.00094\n",
      "Epoch: 3800 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00092\n",
      "Epoch: 4000 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 4200 | Loss: 0.00086 | Test Loss: 0.00085 | Valid Loss: 0.00087\n",
      "Epoch: 4400 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00084\n",
      "Epoch: 4600 | Loss: 0.00080 | Test Loss: 0.00080 | Valid Loss: 0.00081\n",
      "Epoch: 4800 | Loss: 0.00077 | Test Loss: 0.00077 | Valid Loss: 0.00078\n",
      "Epoch: 5000 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00075\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00075 | Valid Loss: 0.00079\n",
      "Epoch: 5400 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00070\n",
      "Epoch: 5600 | Loss: 0.00066 | Test Loss: 0.00065 | Valid Loss: 0.00067\n",
      "Epoch: 5800 | Loss: 0.00063 | Test Loss: 0.00062 | Valid Loss: 0.00064\n",
      "Epoch: 6000 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 6200 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00059\n",
      "Epoch: 6400 | Loss: 0.00054 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 6800 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00050\n",
      "Early stopping at epoch: 6838\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 2.67769 | Test Loss: 2.49789 | Valid Loss: 2.50835\n",
      "Epoch: 200 | Loss: 0.00202 | Test Loss: 0.00206 | Valid Loss: 0.00192\n",
      "Epoch: 400 | Loss: 0.00139 | Test Loss: 0.00146 | Valid Loss: 0.00137\n",
      "Epoch: 600 | Loss: 0.00127 | Test Loss: 0.00135 | Valid Loss: 0.00126\n",
      "Epoch: 800 | Loss: 0.00121 | Test Loss: 0.00128 | Valid Loss: 0.00121\n",
      "Epoch: 1000 | Loss: 0.00117 | Test Loss: 0.00124 | Valid Loss: 0.00118\n",
      "Epoch: 1200 | Loss: 0.00114 | Test Loss: 0.00120 | Valid Loss: 0.00115\n",
      "Epoch: 1400 | Loss: 0.00112 | Test Loss: 0.00117 | Valid Loss: 0.00112\n",
      "Epoch: 1600 | Loss: 0.00110 | Test Loss: 0.00115 | Valid Loss: 0.00110\n",
      "Epoch: 1800 | Loss: 0.00107 | Test Loss: 0.00112 | Valid Loss: 0.00108\n",
      "Epoch: 2000 | Loss: 0.00105 | Test Loss: 0.00110 | Valid Loss: 0.00106\n",
      "Epoch: 2200 | Loss: 0.00104 | Test Loss: 0.00108 | Valid Loss: 0.00105\n",
      "Epoch: 2400 | Loss: 0.00102 | Test Loss: 0.00106 | Valid Loss: 0.00103\n",
      "Epoch: 2600 | Loss: 0.00100 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 2800 | Loss: 0.00098 | Test Loss: 0.00103 | Valid Loss: 0.00099\n",
      "Epoch: 3000 | Loss: 0.00096 | Test Loss: 0.00101 | Valid Loss: 0.00098\n",
      "Epoch: 3200 | Loss: 0.00095 | Test Loss: 0.00099 | Valid Loss: 0.00096\n",
      "Epoch: 3400 | Loss: 0.00093 | Test Loss: 0.00097 | Valid Loss: 0.00094\n",
      "Epoch: 3600 | Loss: 0.00091 | Test Loss: 0.00094 | Valid Loss: 0.00092\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00090\n",
      "Epoch: 4000 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 4200 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 4400 | Loss: 0.00082 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 4600 | Loss: 0.00080 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 4800 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 5000 | Loss: 0.00075 | Test Loss: 0.00077 | Valid Loss: 0.00076\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00073\n",
      "Epoch: 5400 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 5600 | Loss: 0.00067 | Test Loss: 0.00068 | Valid Loss: 0.00068\n",
      "Epoch: 5800 | Loss: 0.00064 | Test Loss: 0.00066 | Valid Loss: 0.00066\n",
      "Epoch: 6000 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Early stopping at epoch: 6344\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 1.30831 | Test Loss: 1.18391 | Valid Loss: 1.17167\n",
      "Epoch: 200 | Loss: 0.00172 | Test Loss: 0.00170 | Valid Loss: 0.00170\n",
      "Epoch: 400 | Loss: 0.00134 | Test Loss: 0.00134 | Valid Loss: 0.00132\n",
      "Epoch: 600 | Loss: 0.00126 | Test Loss: 0.00127 | Valid Loss: 0.00124\n",
      "Epoch: 800 | Loss: 0.00121 | Test Loss: 0.00122 | Valid Loss: 0.00119\n",
      "Epoch: 1000 | Loss: 0.00118 | Test Loss: 0.00119 | Valid Loss: 0.00116\n",
      "Epoch: 1200 | Loss: 0.00116 | Test Loss: 0.00117 | Valid Loss: 0.00113\n",
      "Epoch: 1400 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00111\n",
      "Epoch: 1600 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00109\n",
      "Epoch: 1800 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00108\n",
      "Epoch: 2000 | Loss: 0.00108 | Test Loss: 0.00109 | Valid Loss: 0.00106\n",
      "Epoch: 2200 | Loss: 0.00106 | Test Loss: 0.00107 | Valid Loss: 0.00104\n",
      "Epoch: 2400 | Loss: 0.00104 | Test Loss: 0.00105 | Valid Loss: 0.00102\n",
      "Epoch: 2600 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 2800 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00098\n",
      "Epoch: 3000 | Loss: 0.00097 | Test Loss: 0.00098 | Valid Loss: 0.00096\n",
      "Epoch: 3200 | Loss: 0.00094 | Test Loss: 0.00096 | Valid Loss: 0.00093\n",
      "Epoch: 3400 | Loss: 0.00092 | Test Loss: 0.00093 | Valid Loss: 0.00091\n",
      "Epoch: 3600 | Loss: 0.00089 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 3800 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 4000 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 4200 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00080\n",
      "Epoch: 4400 | Loss: 0.00091 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 4600 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 4800 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00070\n",
      "Epoch: 5000 | Loss: 0.00067 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 5200 | Loss: 0.00065 | Test Loss: 0.00066 | Valid Loss: 0.00064\n",
      "Epoch: 5400 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "Epoch: 6000 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 6200 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Early stopping at epoch: 6247\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 2.31573 | Test Loss: 2.15570 | Valid Loss: 2.12466\n",
      "Epoch: 200 | Loss: 0.00201 | Test Loss: 0.00205 | Valid Loss: 0.00191\n",
      "Epoch: 400 | Loss: 0.00137 | Test Loss: 0.00141 | Valid Loss: 0.00128\n",
      "Epoch: 600 | Loss: 0.00127 | Test Loss: 0.00131 | Valid Loss: 0.00119\n",
      "Epoch: 800 | Loss: 0.00122 | Test Loss: 0.00127 | Valid Loss: 0.00116\n",
      "Epoch: 1000 | Loss: 0.00119 | Test Loss: 0.00124 | Valid Loss: 0.00113\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00121 | Valid Loss: 0.00111\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00119 | Valid Loss: 0.00109\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00117 | Valid Loss: 0.00107\n",
      "Epoch: 1800 | Loss: 0.00111 | Test Loss: 0.00115 | Valid Loss: 0.00106\n",
      "Epoch: 2000 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00104\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00111 | Valid Loss: 0.00102\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00109 | Valid Loss: 0.00101\n",
      "Epoch: 2600 | Loss: 0.00104 | Test Loss: 0.00107 | Valid Loss: 0.00099\n",
      "Epoch: 2800 | Loss: 0.00101 | Test Loss: 0.00105 | Valid Loss: 0.00097\n",
      "Epoch: 3000 | Loss: 0.00099 | Test Loss: 0.00103 | Valid Loss: 0.00095\n",
      "Epoch: 3200 | Loss: 0.00097 | Test Loss: 0.00101 | Valid Loss: 0.00093\n",
      "Epoch: 3400 | Loss: 0.00095 | Test Loss: 0.00099 | Valid Loss: 0.00091\n",
      "Epoch: 3600 | Loss: 0.00093 | Test Loss: 0.00096 | Valid Loss: 0.00089\n",
      "Epoch: 3800 | Loss: 0.00090 | Test Loss: 0.00094 | Valid Loss: 0.00087\n",
      "Epoch: 4000 | Loss: 0.00088 | Test Loss: 0.00092 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00085 | Test Loss: 0.00089 | Valid Loss: 0.00083\n",
      "Epoch: 4400 | Loss: 0.00083 | Test Loss: 0.00086 | Valid Loss: 0.00080\n",
      "Epoch: 4600 | Loss: 0.00080 | Test Loss: 0.00084 | Valid Loss: 0.00078\n",
      "Epoch: 4800 | Loss: 0.00079 | Test Loss: 0.00084 | Valid Loss: 0.00078\n",
      "Epoch: 5000 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00075 | Valid Loss: 0.00070\n",
      "Epoch: 5400 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00067\n",
      "Epoch: 5600 | Loss: 0.00067 | Test Loss: 0.00070 | Valid Loss: 0.00064\n",
      "Epoch: 5800 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00062\n",
      "Epoch: 6000 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00059\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00057\n",
      "Epoch: 6400 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00054\n",
      "Epoch: 6600 | Loss: 0.00054 | Test Loss: 0.00056 | Valid Loss: 0.00052\n",
      "Epoch: 6800 | Loss: 0.00051 | Test Loss: 0.00053 | Valid Loss: 0.00049\n",
      "Epoch: 7000 | Loss: 0.00059 | Test Loss: 0.00050 | Valid Loss: 0.00047\n",
      "Epoch: 7200 | Loss: 0.00045 | Test Loss: 0.00047 | Valid Loss: 0.00044\n",
      "Early stopping at epoch: 7228\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.49063 | Test Loss: 2.26924 | Valid Loss: 2.29041\n",
      "Epoch: 200 | Loss: 0.00203 | Test Loss: 0.00212 | Valid Loss: 0.00210\n",
      "Epoch: 400 | Loss: 0.00138 | Test Loss: 0.00142 | Valid Loss: 0.00141\n",
      "Epoch: 600 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00128\n",
      "Epoch: 800 | Loss: 0.00122 | Test Loss: 0.00124 | Valid Loss: 0.00123\n",
      "Epoch: 1000 | Loss: 0.00119 | Test Loss: 0.00121 | Valid Loss: 0.00120\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00118\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00117 | Valid Loss: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00116 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00111 | Test Loss: 0.00114 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00111\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 2400 | Loss: 0.00107 | Test Loss: 0.00109 | Valid Loss: 0.00107\n",
      "Epoch: 2600 | Loss: 0.00105 | Test Loss: 0.00107 | Valid Loss: 0.00106\n",
      "Epoch: 2800 | Loss: 0.00103 | Test Loss: 0.00106 | Valid Loss: 0.00104\n",
      "Epoch: 3000 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00102\n",
      "Epoch: 3200 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00101\n",
      "Epoch: 3400 | Loss: 0.00098 | Test Loss: 0.00100 | Valid Loss: 0.00099\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00099 | Valid Loss: 0.00097\n",
      "Epoch: 3800 | Loss: 0.00095 | Test Loss: 0.00097 | Valid Loss: 0.00095\n",
      "Epoch: 4000 | Loss: 0.00093 | Test Loss: 0.00094 | Valid Loss: 0.00093\n",
      "Epoch: 4200 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00091\n",
      "Epoch: 4400 | Loss: 0.00088 | Test Loss: 0.00090 | Valid Loss: 0.00089\n",
      "Epoch: 4600 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 5000 | Loss: 0.00081 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 5200 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 5400 | Loss: 0.00075 | Test Loss: 0.00077 | Valid Loss: 0.00076\n",
      "Epoch: 5600 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00073\n",
      "Epoch: 5800 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00070\n",
      "Epoch: 6000 | Loss: 0.00067 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 6200 | Loss: 0.00064 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 6400 | Loss: 0.00067 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 6800 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 7000 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00053\n",
      "Epoch: 7200 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00050\n",
      "Early stopping at epoch: 7323\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.17706 | Test Loss: 1.06561 | Valid Loss: 1.05721\n",
      "Epoch: 200 | Loss: 0.00199 | Test Loss: 0.00202 | Valid Loss: 0.00201\n",
      "Epoch: 400 | Loss: 0.00137 | Test Loss: 0.00138 | Valid Loss: 0.00141\n",
      "Epoch: 600 | Loss: 0.00126 | Test Loss: 0.00127 | Valid Loss: 0.00130\n",
      "Epoch: 800 | Loss: 0.00121 | Test Loss: 0.00122 | Valid Loss: 0.00124\n",
      "Epoch: 1000 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00121\n",
      "Epoch: 1200 | Loss: 0.00115 | Test Loss: 0.00116 | Valid Loss: 0.00118\n",
      "Epoch: 1400 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00107 | Test Loss: 0.00108 | Valid Loss: 0.00110\n",
      "Epoch: 2200 | Loss: 0.00105 | Test Loss: 0.00106 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00103 | Test Loss: 0.00104 | Valid Loss: 0.00106\n",
      "Epoch: 2600 | Loss: 0.00101 | Test Loss: 0.00102 | Valid Loss: 0.00104\n",
      "Epoch: 2800 | Loss: 0.00099 | Test Loss: 0.00100 | Valid Loss: 0.00102\n",
      "Epoch: 3000 | Loss: 0.00096 | Test Loss: 0.00098 | Valid Loss: 0.00100\n",
      "Epoch: 3200 | Loss: 0.00094 | Test Loss: 0.00095 | Valid Loss: 0.00097\n",
      "Epoch: 3400 | Loss: 0.00091 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 3600 | Loss: 0.00088 | Test Loss: 0.00089 | Valid Loss: 0.00091\n",
      "Epoch: 3800 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 4000 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00084\n",
      "Epoch: 4200 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00081\n",
      "Epoch: 4400 | Loss: 0.00077 | Test Loss: 0.00077 | Valid Loss: 0.00078\n",
      "Epoch: 4600 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00074\n",
      "Epoch: 4800 | Loss: 0.00068 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 5000 | Loss: 0.00065 | Test Loss: 0.00067 | Valid Loss: 0.00067\n",
      "Epoch: 5200 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 5400 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 5600 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 5800 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00054\n",
      "Epoch: 6000 | Loss: 0.00050 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 6200 | Loss: 0.00047 | Test Loss: 0.00049 | Valid Loss: 0.00049\n",
      "Early stopping at epoch: 6393\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.12623 | Test Loss: 0.97921 | Valid Loss: 0.99002\n",
      "Epoch: 200 | Loss: 0.00197 | Test Loss: 0.00199 | Valid Loss: 0.00202\n",
      "Epoch: 400 | Loss: 0.00132 | Test Loss: 0.00136 | Valid Loss: 0.00135\n",
      "Epoch: 600 | Loss: 0.00124 | Test Loss: 0.00126 | Valid Loss: 0.00126\n",
      "Epoch: 800 | Loss: 0.00120 | Test Loss: 0.00122 | Valid Loss: 0.00122\n",
      "Epoch: 1000 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00119\n",
      "Epoch: 1200 | Loss: 0.00115 | Test Loss: 0.00116 | Valid Loss: 0.00117\n",
      "Epoch: 1400 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00115\n",
      "Epoch: 1600 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00113\n",
      "Epoch: 1800 | Loss: 0.00109 | Test Loss: 0.00111 | Valid Loss: 0.00111\n",
      "Epoch: 2000 | Loss: 0.00107 | Test Loss: 0.00109 | Valid Loss: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00105 | Test Loss: 0.00107 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00104 | Test Loss: 0.00105 | Valid Loss: 0.00106\n",
      "Epoch: 2600 | Loss: 0.00102 | Test Loss: 0.00103 | Valid Loss: 0.00104\n",
      "Epoch: 2800 | Loss: 0.00100 | Test Loss: 0.00101 | Valid Loss: 0.00102\n",
      "Epoch: 3000 | Loss: 0.00097 | Test Loss: 0.00099 | Valid Loss: 0.00100\n",
      "Epoch: 3200 | Loss: 0.00095 | Test Loss: 0.00097 | Valid Loss: 0.00097\n",
      "Epoch: 3400 | Loss: 0.00093 | Test Loss: 0.00094 | Valid Loss: 0.00095\n",
      "Epoch: 3600 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 3800 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 4000 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00087\n",
      "Epoch: 4200 | Loss: 0.00081 | Test Loss: 0.00082 | Valid Loss: 0.00084\n",
      "Epoch: 4400 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00080\n",
      "Epoch: 4600 | Loss: 0.00075 | Test Loss: 0.00076 | Valid Loss: 0.00077\n",
      "Epoch: 4800 | Loss: 0.00071 | Test Loss: 0.00073 | Valid Loss: 0.00074\n",
      "Epoch: 5000 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00071\n",
      "Epoch: 5200 | Loss: 0.00065 | Test Loss: 0.00066 | Valid Loss: 0.00067\n",
      "Epoch: 5400 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00066\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00061\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6000 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 6200 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 6400 | Loss: 0.00079 | Test Loss: 0.00051 | Valid Loss: 0.00053\n",
      "Epoch: 6600 | Loss: 0.00043 | Test Loss: 0.00044 | Valid Loss: 0.00045\n",
      "Epoch: 6800 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Early stopping at epoch: 6907\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 0.83242 | Test Loss: 0.73471 | Valid Loss: 0.73210\n",
      "Epoch: 200 | Loss: 0.00167 | Test Loss: 0.00173 | Valid Loss: 0.00173\n",
      "Epoch: 400 | Loss: 0.00124 | Test Loss: 0.00129 | Valid Loss: 0.00131\n",
      "Epoch: 600 | Loss: 0.00118 | Test Loss: 0.00122 | Valid Loss: 0.00125\n",
      "Epoch: 800 | Loss: 0.00115 | Test Loss: 0.00118 | Valid Loss: 0.00122\n",
      "Epoch: 1000 | Loss: 0.00113 | Test Loss: 0.00116 | Valid Loss: 0.00119\n",
      "Epoch: 1200 | Loss: 0.00111 | Test Loss: 0.00114 | Valid Loss: 0.00117\n",
      "Epoch: 1400 | Loss: 0.00109 | Test Loss: 0.00112 | Valid Loss: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00106 | Test Loss: 0.00108 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00105 | Test Loss: 0.00107 | Valid Loss: 0.00110\n",
      "Epoch: 2200 | Loss: 0.00103 | Test Loss: 0.00105 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00106\n",
      "Epoch: 2600 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00104\n",
      "Epoch: 2800 | Loss: 0.00097 | Test Loss: 0.00099 | Valid Loss: 0.00102\n",
      "Epoch: 3000 | Loss: 0.00095 | Test Loss: 0.00097 | Valid Loss: 0.00099\n",
      "Epoch: 3200 | Loss: 0.00092 | Test Loss: 0.00094 | Valid Loss: 0.00097\n",
      "Epoch: 3400 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 3600 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00091\n",
      "Epoch: 3800 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 4000 | Loss: 0.00081 | Test Loss: 0.00083 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00082\n",
      "Epoch: 4400 | Loss: 0.00075 | Test Loss: 0.00077 | Valid Loss: 0.00078\n",
      "Epoch: 4600 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00075\n",
      "Epoch: 4800 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00071\n",
      "Epoch: 5000 | Loss: 0.00066 | Test Loss: 0.00067 | Valid Loss: 0.00068\n",
      "Epoch: 5200 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 5400 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 5600 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 5800 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 6000 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 6200 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00048\n",
      "Early stopping at epoch: 6392\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 2.07306 | Test Loss: 1.89393 | Valid Loss: 1.89621\n",
      "Epoch: 200 | Loss: 0.00214 | Test Loss: 0.00214 | Valid Loss: 0.00209\n",
      "Epoch: 400 | Loss: 0.00137 | Test Loss: 0.00136 | Valid Loss: 0.00135\n",
      "Epoch: 600 | Loss: 0.00126 | Test Loss: 0.00125 | Valid Loss: 0.00126\n",
      "Epoch: 800 | Loss: 0.00121 | Test Loss: 0.00121 | Valid Loss: 0.00123\n",
      "Epoch: 1000 | Loss: 0.00118 | Test Loss: 0.00118 | Valid Loss: 0.00120\n",
      "Epoch: 1200 | Loss: 0.00116 | Test Loss: 0.00116 | Valid Loss: 0.00118\n",
      "Epoch: 1400 | Loss: 0.00114 | Test Loss: 0.00114 | Valid Loss: 0.00116\n",
      "Epoch: 1600 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00111 | Test Loss: 0.00111 | Valid Loss: 0.00113\n",
      "Epoch: 2000 | Loss: 0.00109 | Test Loss: 0.00109 | Valid Loss: 0.00111\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00108 | Valid Loss: 0.00110\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00107 | Valid Loss: 0.00109\n",
      "Epoch: 2600 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00107\n",
      "Epoch: 2800 | Loss: 0.00103 | Test Loss: 0.00104 | Valid Loss: 0.00106\n",
      "Epoch: 3000 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00104\n",
      "Epoch: 3200 | Loss: 0.00100 | Test Loss: 0.00100 | Valid Loss: 0.00102\n",
      "Epoch: 3400 | Loss: 0.00098 | Test Loss: 0.00099 | Valid Loss: 0.00100\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00097 | Valid Loss: 0.00098\n",
      "Epoch: 3800 | Loss: 0.00094 | Test Loss: 0.00095 | Valid Loss: 0.00096\n",
      "Epoch: 4000 | Loss: 0.00092 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 4200 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00092\n",
      "Epoch: 4400 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 4600 | Loss: 0.00085 | Test Loss: 0.00085 | Valid Loss: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00084\n",
      "Epoch: 5000 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00081\n",
      "Epoch: 5200 | Loss: 0.00076 | Test Loss: 0.00076 | Valid Loss: 0.00078\n",
      "Epoch: 5400 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00075\n",
      "Epoch: 5600 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00072\n",
      "Epoch: 5800 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00069\n",
      "Epoch: 6000 | Loss: 0.00084 | Test Loss: 0.00096 | Valid Loss: 0.00097\n",
      "Epoch: 6200 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 6400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00060\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 7000 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 7200 | Loss: 0.00048 | Test Loss: 0.00050 | Valid Loss: 0.00050\n",
      "Epoch: 7400 | Loss: 0.00044 | Test Loss: 0.00044 | Valid Loss: 0.00045\n",
      "Epoch: 7600 | Loss: 0.00041 | Test Loss: 0.00042 | Valid Loss: 0.00042\n",
      "Early stopping at epoch: 7759\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 2.93380 | Test Loss: 2.69158 | Valid Loss: 2.70697\n",
      "Epoch: 200 | Loss: 0.00190 | Test Loss: 0.00186 | Valid Loss: 0.00189\n",
      "Epoch: 400 | Loss: 0.00137 | Test Loss: 0.00135 | Valid Loss: 0.00138\n",
      "Epoch: 600 | Loss: 0.00127 | Test Loss: 0.00125 | Valid Loss: 0.00127\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00121 | Valid Loss: 0.00123\n",
      "Epoch: 1000 | Loss: 0.00120 | Test Loss: 0.00118 | Valid Loss: 0.00120\n",
      "Epoch: 1200 | Loss: 0.00117 | Test Loss: 0.00116 | Valid Loss: 0.00117\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00114 | Valid Loss: 0.00115\n",
      "Epoch: 1600 | Loss: 0.00114 | Test Loss: 0.00112 | Valid Loss: 0.00113\n",
      "Epoch: 1800 | Loss: 0.00112 | Test Loss: 0.00111 | Valid Loss: 0.00111\n",
      "Epoch: 2000 | Loss: 0.00111 | Test Loss: 0.00109 | Valid Loss: 0.00110\n",
      "Epoch: 2200 | Loss: 0.00109 | Test Loss: 0.00108 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00108 | Test Loss: 0.00106 | Valid Loss: 0.00107\n",
      "Epoch: 2600 | Loss: 0.00106 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 2800 | Loss: 0.00105 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 3000 | Loss: 0.00103 | Test Loss: 0.00101 | Valid Loss: 0.00102\n",
      "Epoch: 3200 | Loss: 0.00101 | Test Loss: 0.00100 | Valid Loss: 0.00100\n",
      "Epoch: 3400 | Loss: 0.00100 | Test Loss: 0.00098 | Valid Loss: 0.00098\n",
      "Epoch: 3600 | Loss: 0.00098 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 3800 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 4000 | Loss: 0.00094 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 4200 | Loss: 0.00092 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 4400 | Loss: 0.00090 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 4600 | Loss: 0.00087 | Test Loss: 0.00085 | Valid Loss: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00083 | Valid Loss: 0.00083\n",
      "Epoch: 5000 | Loss: 0.00082 | Test Loss: 0.00080 | Valid Loss: 0.00081\n",
      "Epoch: 5200 | Loss: 0.00079 | Test Loss: 0.00077 | Valid Loss: 0.00078\n",
      "Epoch: 5400 | Loss: 0.00077 | Test Loss: 0.00075 | Valid Loss: 0.00076\n",
      "Epoch: 5600 | Loss: 0.00074 | Test Loss: 0.00072 | Valid Loss: 0.00073\n",
      "Epoch: 5800 | Loss: 0.00071 | Test Loss: 0.00069 | Valid Loss: 0.00070\n",
      "Epoch: 6000 | Loss: 0.00069 | Test Loss: 0.00066 | Valid Loss: 0.00068\n",
      "Epoch: 6200 | Loss: 0.00065 | Test Loss: 0.00063 | Valid Loss: 0.00065\n",
      "Epoch: 6400 | Loss: 0.00062 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00059\n",
      "Epoch: 6800 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 7000 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 7200 | Loss: 0.00087 | Test Loss: 0.00065 | Valid Loss: 0.00067\n",
      "Epoch: 7400 | Loss: 0.00047 | Test Loss: 0.00046 | Valid Loss: 0.00047\n",
      "Early stopping at epoch: 7426\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.12872 | Test Loss: 1.93070 | Valid Loss: 1.91328\n",
      "Epoch: 200 | Loss: 0.00226 | Test Loss: 0.00226 | Valid Loss: 0.00223\n",
      "Epoch: 400 | Loss: 0.00157 | Test Loss: 0.00154 | Valid Loss: 0.00154\n",
      "Epoch: 600 | Loss: 0.00145 | Test Loss: 0.00143 | Valid Loss: 0.00142\n",
      "Epoch: 800 | Loss: 0.00139 | Test Loss: 0.00138 | Valid Loss: 0.00136\n",
      "Epoch: 1000 | Loss: 0.00135 | Test Loss: 0.00134 | Valid Loss: 0.00132\n",
      "Epoch: 1200 | Loss: 0.00133 | Test Loss: 0.00131 | Valid Loss: 0.00129\n",
      "Epoch: 1400 | Loss: 0.00130 | Test Loss: 0.00129 | Valid Loss: 0.00127\n",
      "Epoch: 1600 | Loss: 0.00128 | Test Loss: 0.00127 | Valid Loss: 0.00125\n",
      "Epoch: 1800 | Loss: 0.00127 | Test Loss: 0.00125 | Valid Loss: 0.00123\n",
      "Epoch: 2000 | Loss: 0.00125 | Test Loss: 0.00124 | Valid Loss: 0.00121\n",
      "Epoch: 2200 | Loss: 0.00123 | Test Loss: 0.00122 | Valid Loss: 0.00120\n",
      "Epoch: 2400 | Loss: 0.00122 | Test Loss: 0.00121 | Valid Loss: 0.00118\n",
      "Epoch: 2600 | Loss: 0.00120 | Test Loss: 0.00119 | Valid Loss: 0.00116\n",
      "Epoch: 2800 | Loss: 0.00119 | Test Loss: 0.00118 | Valid Loss: 0.00115\n",
      "Epoch: 3000 | Loss: 0.00117 | Test Loss: 0.00117 | Valid Loss: 0.00113\n",
      "Epoch: 3200 | Loss: 0.00116 | Test Loss: 0.00115 | Valid Loss: 0.00111\n",
      "Epoch: 3400 | Loss: 0.00114 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 3600 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00108\n",
      "Epoch: 3800 | Loss: 0.00110 | Test Loss: 0.00110 | Valid Loss: 0.00106\n",
      "Epoch: 4000 | Loss: 0.00108 | Test Loss: 0.00108 | Valid Loss: 0.00104\n",
      "Epoch: 4200 | Loss: 0.00106 | Test Loss: 0.00106 | Valid Loss: 0.00102\n",
      "Epoch: 4400 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00100\n",
      "Epoch: 4600 | Loss: 0.00101 | Test Loss: 0.00101 | Valid Loss: 0.00098\n",
      "Epoch: 4800 | Loss: 0.00099 | Test Loss: 0.00098 | Valid Loss: 0.00095\n",
      "Epoch: 5000 | Loss: 0.00096 | Test Loss: 0.00096 | Valid Loss: 0.00092\n",
      "Epoch: 5200 | Loss: 0.00093 | Test Loss: 0.00093 | Valid Loss: 0.00089\n",
      "Epoch: 5400 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00086\n",
      "Epoch: 5600 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00083\n",
      "Epoch: 5800 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00080\n",
      "Epoch: 6000 | Loss: 0.00079 | Test Loss: 0.00080 | Valid Loss: 0.00076\n",
      "Epoch: 6200 | Loss: 0.00076 | Test Loss: 0.00076 | Valid Loss: 0.00073\n",
      "Epoch: 6400 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00070\n",
      "Epoch: 6600 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00065\n",
      "Epoch: 6800 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00062\n",
      "Epoch: 7000 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Epoch: 7200 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00054\n",
      "Early stopping at epoch: 7375\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.40345 | Test Loss: 1.24832 | Valid Loss: 1.23810\n",
      "Epoch: 200 | Loss: 0.00304 | Test Loss: 0.00306 | Valid Loss: 0.00299\n",
      "Epoch: 400 | Loss: 0.00276 | Test Loss: 0.00277 | Valid Loss: 0.00271\n",
      "Epoch: 600 | Loss: 0.00258 | Test Loss: 0.00260 | Valid Loss: 0.00254\n",
      "Epoch: 800 | Loss: 0.00242 | Test Loss: 0.00245 | Valid Loss: 0.00239\n",
      "Epoch: 1000 | Loss: 0.00227 | Test Loss: 0.00230 | Valid Loss: 0.00225\n",
      "Epoch: 1200 | Loss: 0.00213 | Test Loss: 0.00216 | Valid Loss: 0.00211\n",
      "Epoch: 1400 | Loss: 0.00199 | Test Loss: 0.00203 | Valid Loss: 0.00198\n",
      "Epoch: 1600 | Loss: 0.00187 | Test Loss: 0.00190 | Valid Loss: 0.00186\n",
      "Epoch: 1800 | Loss: 0.00175 | Test Loss: 0.00178 | Valid Loss: 0.00175\n",
      "Epoch: 2000 | Loss: 0.00165 | Test Loss: 0.00168 | Valid Loss: 0.00165\n",
      "Epoch: 2200 | Loss: 0.00156 | Test Loss: 0.00158 | Valid Loss: 0.00156\n",
      "Epoch: 2400 | Loss: 0.00148 | Test Loss: 0.00150 | Valid Loss: 0.00148\n",
      "Epoch: 2600 | Loss: 0.00141 | Test Loss: 0.00143 | Valid Loss: 0.00141\n",
      "Epoch: 2800 | Loss: 0.00135 | Test Loss: 0.00136 | Valid Loss: 0.00135\n",
      "Epoch: 3000 | Loss: 0.00130 | Test Loss: 0.00131 | Valid Loss: 0.00130\n",
      "Epoch: 3200 | Loss: 0.00126 | Test Loss: 0.00126 | Valid Loss: 0.00126\n",
      "Epoch: 3400 | Loss: 0.00122 | Test Loss: 0.00122 | Valid Loss: 0.00122\n",
      "Epoch: 3600 | Loss: 0.00118 | Test Loss: 0.00118 | Valid Loss: 0.00118\n",
      "Epoch: 3800 | Loss: 0.00114 | Test Loss: 0.00114 | Valid Loss: 0.00114\n",
      "Epoch: 4000 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 4200 | Loss: 0.00106 | Test Loss: 0.00106 | Valid Loss: 0.00106\n",
      "Epoch: 4400 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00102\n",
      "Epoch: 4600 | Loss: 0.00098 | Test Loss: 0.00098 | Valid Loss: 0.00098\n",
      "Epoch: 4800 | Loss: 0.00094 | Test Loss: 0.00095 | Valid Loss: 0.00094\n",
      "Epoch: 5000 | Loss: 0.00090 | Test Loss: 0.00091 | Valid Loss: 0.00090\n",
      "Epoch: 5200 | Loss: 0.00086 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 5400 | Loss: 0.00155 | Test Loss: 0.00130 | Valid Loss: 0.00123\n",
      "Epoch: 5600 | Loss: 0.00077 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 5800 | Loss: 0.00073 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 6000 | Loss: 0.00070 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 6200 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Early stopping at epoch: 6269\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 8000\n",
    "learning_rate = 0.0015\n",
    "momentum = 0.8\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_MT_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_MT_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_MT_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_MT_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f927a9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 4.9474 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_MT_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_MT_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_MT_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_MT_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114ccbb8",
   "metadata": {},
   "source": [
    "# Integration - M + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "504a46c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 93\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "M_MT_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    M_MT_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb1ee35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_M_MT_train, y_train = X_M_MT_train.to(device), y_train.to(device)\n",
    "X_M_MT_test, y_test = X_M_MT_test.to(device), y_test.to(device)\n",
    "X_M_MT_valid, y_valid = X_M_MT_valid.to(device), y_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e4bad623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.93017 | Test Loss: 1.74044 | Valid Loss: 1.72319\n",
      "Epoch: 200 | Loss: 0.00925 | Test Loss: 0.00953 | Valid Loss: 0.00914\n",
      "Epoch: 400 | Loss: 0.00547 | Test Loss: 0.00563 | Valid Loss: 0.00544\n",
      "Epoch: 600 | Loss: 0.00492 | Test Loss: 0.00500 | Valid Loss: 0.00491\n",
      "Epoch: 800 | Loss: 0.00476 | Test Loss: 0.00482 | Valid Loss: 0.00475\n",
      "Epoch: 1000 | Loss: 0.00462 | Test Loss: 0.00467 | Valid Loss: 0.00460\n",
      "Epoch: 1200 | Loss: 0.00448 | Test Loss: 0.00452 | Valid Loss: 0.00446\n",
      "Epoch: 1400 | Loss: 0.00433 | Test Loss: 0.00437 | Valid Loss: 0.00431\n",
      "Epoch: 1600 | Loss: 0.00418 | Test Loss: 0.00421 | Valid Loss: 0.00415\n",
      "Epoch: 1800 | Loss: 0.00402 | Test Loss: 0.00405 | Valid Loss: 0.00399\n",
      "Epoch: 2000 | Loss: 0.00386 | Test Loss: 0.00388 | Valid Loss: 0.00382\n",
      "Epoch: 2200 | Loss: 0.00369 | Test Loss: 0.00370 | Valid Loss: 0.00365\n",
      "Epoch: 2400 | Loss: 0.00350 | Test Loss: 0.00352 | Valid Loss: 0.00346\n",
      "Epoch: 2600 | Loss: 0.00331 | Test Loss: 0.00332 | Valid Loss: 0.00326\n",
      "Epoch: 2800 | Loss: 0.00311 | Test Loss: 0.00312 | Valid Loss: 0.00305\n",
      "Epoch: 3000 | Loss: 0.00291 | Test Loss: 0.00292 | Valid Loss: 0.00285\n",
      "Epoch: 3200 | Loss: 0.00272 | Test Loss: 0.00272 | Valid Loss: 0.00265\n",
      "Epoch: 3400 | Loss: 0.00253 | Test Loss: 0.00253 | Valid Loss: 0.00245\n",
      "Epoch: 3600 | Loss: 0.00234 | Test Loss: 0.00235 | Valid Loss: 0.00227\n",
      "Epoch: 3800 | Loss: 0.00216 | Test Loss: 0.00217 | Valid Loss: 0.00208\n",
      "Epoch: 4000 | Loss: 0.00197 | Test Loss: 0.00199 | Valid Loss: 0.00190\n",
      "Epoch: 4200 | Loss: 0.00178 | Test Loss: 0.00180 | Valid Loss: 0.00172\n",
      "Epoch: 4400 | Loss: 0.00160 | Test Loss: 0.00162 | Valid Loss: 0.00154\n",
      "Epoch: 4600 | Loss: 0.00142 | Test Loss: 0.00145 | Valid Loss: 0.00137\n",
      "Epoch: 4800 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00121\n",
      "Epoch: 5000 | Loss: 0.00112 | Test Loss: 0.00114 | Valid Loss: 0.00108\n",
      "Epoch: 5200 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00096\n",
      "Epoch: 5400 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00087\n",
      "Epoch: 5600 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00080\n",
      "Epoch: 5800 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 6000 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00068\n",
      "Epoch: 6200 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00064\n",
      "Epoch: 6400 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00060\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00055\n",
      "Epoch: 7000 | Loss: 0.00053 | Test Loss: 0.00055 | Valid Loss: 0.00052\n",
      "Epoch: 7200 | Loss: 0.00051 | Test Loss: 0.00053 | Valid Loss: 0.00050\n",
      "Epoch: 7400 | Loss: 0.00049 | Test Loss: 0.00051 | Valid Loss: 0.00048\n",
      "Early stopping at epoch: 7593\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.59487 | Test Loss: 1.41460 | Valid Loss: 1.39358\n",
      "Epoch: 200 | Loss: 0.00755 | Test Loss: 0.00792 | Valid Loss: 0.00724\n",
      "Epoch: 400 | Loss: 0.00568 | Test Loss: 0.00587 | Valid Loss: 0.00547\n",
      "Epoch: 600 | Loss: 0.00530 | Test Loss: 0.00539 | Valid Loss: 0.00512\n",
      "Epoch: 800 | Loss: 0.00507 | Test Loss: 0.00513 | Valid Loss: 0.00491\n",
      "Epoch: 1000 | Loss: 0.00483 | Test Loss: 0.00488 | Valid Loss: 0.00469\n",
      "Epoch: 1200 | Loss: 0.00460 | Test Loss: 0.00464 | Valid Loss: 0.00446\n",
      "Epoch: 1400 | Loss: 0.00436 | Test Loss: 0.00439 | Valid Loss: 0.00424\n",
      "Epoch: 1600 | Loss: 0.00414 | Test Loss: 0.00416 | Valid Loss: 0.00404\n",
      "Epoch: 1800 | Loss: 0.00393 | Test Loss: 0.00395 | Valid Loss: 0.00384\n",
      "Epoch: 2000 | Loss: 0.00373 | Test Loss: 0.00375 | Valid Loss: 0.00366\n",
      "Epoch: 2200 | Loss: 0.00355 | Test Loss: 0.00356 | Valid Loss: 0.00349\n",
      "Epoch: 2400 | Loss: 0.00338 | Test Loss: 0.00339 | Valid Loss: 0.00332\n",
      "Epoch: 2600 | Loss: 0.00321 | Test Loss: 0.00322 | Valid Loss: 0.00316\n",
      "Epoch: 2800 | Loss: 0.00304 | Test Loss: 0.00305 | Valid Loss: 0.00300\n",
      "Epoch: 3000 | Loss: 0.00287 | Test Loss: 0.00288 | Valid Loss: 0.00283\n",
      "Epoch: 3200 | Loss: 0.00270 | Test Loss: 0.00271 | Valid Loss: 0.00266\n",
      "Epoch: 3400 | Loss: 0.00254 | Test Loss: 0.00255 | Valid Loss: 0.00251\n",
      "Epoch: 3600 | Loss: 0.00239 | Test Loss: 0.00241 | Valid Loss: 0.00237\n",
      "Epoch: 3800 | Loss: 0.00225 | Test Loss: 0.00228 | Valid Loss: 0.00223\n",
      "Epoch: 4000 | Loss: 0.00212 | Test Loss: 0.00214 | Valid Loss: 0.00210\n",
      "Epoch: 4200 | Loss: 0.00198 | Test Loss: 0.00200 | Valid Loss: 0.00196\n",
      "Epoch: 4400 | Loss: 0.00183 | Test Loss: 0.00186 | Valid Loss: 0.00182\n",
      "Epoch: 4600 | Loss: 0.00168 | Test Loss: 0.00170 | Valid Loss: 0.00167\n",
      "Epoch: 4800 | Loss: 0.00153 | Test Loss: 0.00155 | Valid Loss: 0.00152\n",
      "Epoch: 5000 | Loss: 0.00138 | Test Loss: 0.00140 | Valid Loss: 0.00137\n",
      "Epoch: 5200 | Loss: 0.00124 | Test Loss: 0.00128 | Valid Loss: 0.00123\n",
      "Epoch: 5400 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 5600 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00098\n",
      "Epoch: 5800 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00090\n",
      "Epoch: 6000 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00089 | Valid Loss: 0.00088\n",
      "Epoch: 6400 | Loss: 0.00080 | Test Loss: 0.00090 | Valid Loss: 0.00085\n",
      "Epoch: 6600 | Loss: 0.00067 | Test Loss: 0.00070 | Valid Loss: 0.00068\n",
      "Epoch: 6800 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00064\n",
      "Epoch: 7000 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00061\n",
      "Epoch: 7200 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Early stopping at epoch: 7220\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 0.83923 | Test Loss: 0.72750 | Valid Loss: 0.72213\n",
      "Epoch: 200 | Loss: 0.00627 | Test Loss: 0.00629 | Valid Loss: 0.00618\n",
      "Epoch: 400 | Loss: 0.00539 | Test Loss: 0.00532 | Valid Loss: 0.00534\n",
      "Epoch: 600 | Loss: 0.00517 | Test Loss: 0.00506 | Valid Loss: 0.00513\n",
      "Epoch: 800 | Loss: 0.00496 | Test Loss: 0.00484 | Valid Loss: 0.00492\n",
      "Epoch: 1000 | Loss: 0.00473 | Test Loss: 0.00461 | Valid Loss: 0.00469\n",
      "Epoch: 1200 | Loss: 0.00448 | Test Loss: 0.00436 | Valid Loss: 0.00445\n",
      "Epoch: 1400 | Loss: 0.00421 | Test Loss: 0.00409 | Valid Loss: 0.00418\n",
      "Epoch: 1600 | Loss: 0.00392 | Test Loss: 0.00380 | Valid Loss: 0.00389\n",
      "Epoch: 1800 | Loss: 0.00362 | Test Loss: 0.00350 | Valid Loss: 0.00359\n",
      "Epoch: 2000 | Loss: 0.00331 | Test Loss: 0.00320 | Valid Loss: 0.00328\n",
      "Epoch: 2200 | Loss: 0.00301 | Test Loss: 0.00292 | Valid Loss: 0.00298\n",
      "Epoch: 2400 | Loss: 0.00274 | Test Loss: 0.00267 | Valid Loss: 0.00271\n",
      "Epoch: 2600 | Loss: 0.00250 | Test Loss: 0.00246 | Valid Loss: 0.00248\n",
      "Epoch: 2800 | Loss: 0.00231 | Test Loss: 0.00228 | Valid Loss: 0.00228\n",
      "Epoch: 3000 | Loss: 0.00215 | Test Loss: 0.00214 | Valid Loss: 0.00211\n",
      "Epoch: 3200 | Loss: 0.00201 | Test Loss: 0.00201 | Valid Loss: 0.00197\n",
      "Epoch: 3400 | Loss: 0.00189 | Test Loss: 0.00190 | Valid Loss: 0.00185\n",
      "Epoch: 3600 | Loss: 0.00177 | Test Loss: 0.00178 | Valid Loss: 0.00173\n",
      "Epoch: 3800 | Loss: 0.00165 | Test Loss: 0.00167 | Valid Loss: 0.00162\n",
      "Epoch: 4000 | Loss: 0.00153 | Test Loss: 0.00155 | Valid Loss: 0.00150\n",
      "Epoch: 4200 | Loss: 0.00140 | Test Loss: 0.00142 | Valid Loss: 0.00138\n",
      "Epoch: 4400 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00124\n",
      "Epoch: 4600 | Loss: 0.00113 | Test Loss: 0.00115 | Valid Loss: 0.00112\n",
      "Epoch: 4800 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 5000 | Loss: 0.00093 | Test Loss: 0.00095 | Valid Loss: 0.00092\n",
      "Epoch: 5200 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00084\n",
      "Epoch: 5400 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Epoch: 5600 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 5800 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00069\n",
      "Epoch: 6000 | Loss: 0.00066 | Test Loss: 0.00072 | Valid Loss: 0.00070\n",
      "Epoch: 6200 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 6400 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00059\n",
      "Epoch: 6600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00062 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Early stopping at epoch: 6859\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 2.64189 | Test Loss: 2.40245 | Valid Loss: 2.40299\n",
      "Epoch: 200 | Loss: 0.00812 | Test Loss: 0.00829 | Valid Loss: 0.00803\n",
      "Epoch: 400 | Loss: 0.00528 | Test Loss: 0.00534 | Valid Loss: 0.00535\n",
      "Epoch: 600 | Loss: 0.00485 | Test Loss: 0.00486 | Valid Loss: 0.00495\n",
      "Epoch: 800 | Loss: 0.00469 | Test Loss: 0.00467 | Valid Loss: 0.00479\n",
      "Epoch: 1000 | Loss: 0.00458 | Test Loss: 0.00454 | Valid Loss: 0.00467\n",
      "Epoch: 1200 | Loss: 0.00446 | Test Loss: 0.00442 | Valid Loss: 0.00455\n",
      "Epoch: 1400 | Loss: 0.00433 | Test Loss: 0.00429 | Valid Loss: 0.00441\n",
      "Epoch: 1600 | Loss: 0.00419 | Test Loss: 0.00415 | Valid Loss: 0.00427\n",
      "Epoch: 1800 | Loss: 0.00403 | Test Loss: 0.00399 | Valid Loss: 0.00411\n",
      "Epoch: 2000 | Loss: 0.00387 | Test Loss: 0.00383 | Valid Loss: 0.00394\n",
      "Epoch: 2200 | Loss: 0.00369 | Test Loss: 0.00365 | Valid Loss: 0.00375\n",
      "Epoch: 2400 | Loss: 0.00350 | Test Loss: 0.00347 | Valid Loss: 0.00355\n",
      "Epoch: 2600 | Loss: 0.00330 | Test Loss: 0.00327 | Valid Loss: 0.00335\n",
      "Epoch: 2800 | Loss: 0.00310 | Test Loss: 0.00307 | Valid Loss: 0.00313\n",
      "Epoch: 3000 | Loss: 0.00289 | Test Loss: 0.00286 | Valid Loss: 0.00292\n",
      "Epoch: 3200 | Loss: 0.00269 | Test Loss: 0.00266 | Valid Loss: 0.00270\n",
      "Epoch: 3400 | Loss: 0.00249 | Test Loss: 0.00247 | Valid Loss: 0.00250\n",
      "Epoch: 3600 | Loss: 0.00232 | Test Loss: 0.00230 | Valid Loss: 0.00232\n",
      "Epoch: 3800 | Loss: 0.00217 | Test Loss: 0.00215 | Valid Loss: 0.00217\n",
      "Epoch: 4000 | Loss: 0.00204 | Test Loss: 0.00203 | Valid Loss: 0.00204\n",
      "Epoch: 4200 | Loss: 0.00193 | Test Loss: 0.00192 | Valid Loss: 0.00193\n",
      "Epoch: 4400 | Loss: 0.00184 | Test Loss: 0.00184 | Valid Loss: 0.00184\n",
      "Epoch: 4600 | Loss: 0.00176 | Test Loss: 0.00175 | Valid Loss: 0.00175\n",
      "Epoch: 4800 | Loss: 0.00167 | Test Loss: 0.00167 | Valid Loss: 0.00167\n",
      "Epoch: 5000 | Loss: 0.00159 | Test Loss: 0.00159 | Valid Loss: 0.00159\n",
      "Epoch: 5200 | Loss: 0.00150 | Test Loss: 0.00151 | Valid Loss: 0.00151\n",
      "Epoch: 5400 | Loss: 0.00142 | Test Loss: 0.00142 | Valid Loss: 0.00143\n",
      "Epoch: 5600 | Loss: 0.00134 | Test Loss: 0.00134 | Valid Loss: 0.00135\n",
      "Epoch: 5800 | Loss: 0.00125 | Test Loss: 0.00125 | Valid Loss: 0.00127\n",
      "Epoch: 6000 | Loss: 0.00117 | Test Loss: 0.00117 | Valid Loss: 0.00119\n",
      "Epoch: 6200 | Loss: 0.00108 | Test Loss: 0.00108 | Valid Loss: 0.00110\n",
      "Epoch: 6400 | Loss: 0.00099 | Test Loss: 0.00099 | Valid Loss: 0.00101\n",
      "Epoch: 6600 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00093\n",
      "Epoch: 6800 | Loss: 0.00084 | Test Loss: 0.00084 | Valid Loss: 0.00086\n",
      "Epoch: 7000 | Loss: 0.00077 | Test Loss: 0.00077 | Valid Loss: 0.00079\n",
      "Epoch: 7200 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00074\n",
      "Epoch: 7400 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00069\n",
      "Epoch: 7600 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00066\n",
      "Epoch: 7800 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 3.08941 | Test Loss: 2.85725 | Valid Loss: 2.86570\n",
      "Epoch: 200 | Loss: 0.00838 | Test Loss: 0.00842 | Valid Loss: 0.00855\n",
      "Epoch: 400 | Loss: 0.00517 | Test Loss: 0.00513 | Valid Loss: 0.00531\n",
      "Epoch: 600 | Loss: 0.00472 | Test Loss: 0.00467 | Valid Loss: 0.00489\n",
      "Epoch: 800 | Loss: 0.00456 | Test Loss: 0.00451 | Valid Loss: 0.00474\n",
      "Epoch: 1000 | Loss: 0.00446 | Test Loss: 0.00440 | Valid Loss: 0.00464\n",
      "Epoch: 1200 | Loss: 0.00436 | Test Loss: 0.00429 | Valid Loss: 0.00453\n",
      "Epoch: 1400 | Loss: 0.00424 | Test Loss: 0.00417 | Valid Loss: 0.00441\n",
      "Epoch: 1600 | Loss: 0.00410 | Test Loss: 0.00404 | Valid Loss: 0.00427\n",
      "Epoch: 1800 | Loss: 0.00395 | Test Loss: 0.00390 | Valid Loss: 0.00412\n",
      "Epoch: 2000 | Loss: 0.00379 | Test Loss: 0.00373 | Valid Loss: 0.00394\n",
      "Epoch: 2200 | Loss: 0.00360 | Test Loss: 0.00355 | Valid Loss: 0.00375\n",
      "Epoch: 2400 | Loss: 0.00340 | Test Loss: 0.00335 | Valid Loss: 0.00355\n",
      "Epoch: 2600 | Loss: 0.00319 | Test Loss: 0.00315 | Valid Loss: 0.00333\n",
      "Epoch: 2800 | Loss: 0.00297 | Test Loss: 0.00293 | Valid Loss: 0.00310\n",
      "Epoch: 3000 | Loss: 0.00275 | Test Loss: 0.00271 | Valid Loss: 0.00287\n",
      "Epoch: 3200 | Loss: 0.00253 | Test Loss: 0.00249 | Valid Loss: 0.00263\n",
      "Epoch: 3400 | Loss: 0.00232 | Test Loss: 0.00228 | Valid Loss: 0.00241\n",
      "Epoch: 3600 | Loss: 0.00212 | Test Loss: 0.00209 | Valid Loss: 0.00220\n",
      "Epoch: 3800 | Loss: 0.00195 | Test Loss: 0.00191 | Valid Loss: 0.00201\n",
      "Epoch: 4000 | Loss: 0.00180 | Test Loss: 0.00176 | Valid Loss: 0.00186\n",
      "Epoch: 4200 | Loss: 0.00168 | Test Loss: 0.00164 | Valid Loss: 0.00173\n",
      "Epoch: 4400 | Loss: 0.00158 | Test Loss: 0.00154 | Valid Loss: 0.00162\n",
      "Epoch: 4600 | Loss: 0.00148 | Test Loss: 0.00144 | Valid Loss: 0.00152\n",
      "Epoch: 4800 | Loss: 0.00140 | Test Loss: 0.00136 | Valid Loss: 0.00144\n",
      "Epoch: 5000 | Loss: 0.00132 | Test Loss: 0.00127 | Valid Loss: 0.00135\n",
      "Epoch: 5200 | Loss: 0.00123 | Test Loss: 0.00119 | Valid Loss: 0.00127\n",
      "Epoch: 5400 | Loss: 0.00115 | Test Loss: 0.00111 | Valid Loss: 0.00120\n",
      "Epoch: 5600 | Loss: 0.00107 | Test Loss: 0.00104 | Valid Loss: 0.00112\n",
      "Epoch: 5800 | Loss: 0.00099 | Test Loss: 0.00096 | Valid Loss: 0.00104\n",
      "Epoch: 6000 | Loss: 0.00092 | Test Loss: 0.00089 | Valid Loss: 0.00097\n",
      "Epoch: 6200 | Loss: 0.00086 | Test Loss: 0.00083 | Valid Loss: 0.00090\n",
      "Epoch: 6400 | Loss: 0.00080 | Test Loss: 0.00077 | Valid Loss: 0.00085\n",
      "Epoch: 6600 | Loss: 0.00075 | Test Loss: 0.00072 | Valid Loss: 0.00079\n",
      "Epoch: 6800 | Loss: 0.00070 | Test Loss: 0.00068 | Valid Loss: 0.00075\n",
      "Epoch: 7000 | Loss: 0.00066 | Test Loss: 0.00064 | Valid Loss: 0.00071\n",
      "Epoch: 7200 | Loss: 0.00063 | Test Loss: 0.00061 | Valid Loss: 0.00067\n",
      "Epoch: 7400 | Loss: 0.00059 | Test Loss: 0.00058 | Valid Loss: 0.00065\n",
      "Epoch: 7600 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00061\n",
      "Epoch: 7800 | Loss: 0.00054 | Test Loss: 0.00052 | Valid Loss: 0.00058\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 1.51729 | Test Loss: 1.35571 | Valid Loss: 1.34871\n",
      "Epoch: 200 | Loss: 0.00538 | Test Loss: 0.00524 | Valid Loss: 0.00536\n",
      "Epoch: 400 | Loss: 0.00472 | Test Loss: 0.00454 | Valid Loss: 0.00470\n",
      "Epoch: 600 | Loss: 0.00456 | Test Loss: 0.00437 | Valid Loss: 0.00455\n",
      "Epoch: 800 | Loss: 0.00442 | Test Loss: 0.00424 | Valid Loss: 0.00442\n",
      "Epoch: 1000 | Loss: 0.00428 | Test Loss: 0.00411 | Valid Loss: 0.00428\n",
      "Epoch: 1200 | Loss: 0.00412 | Test Loss: 0.00396 | Valid Loss: 0.00412\n",
      "Epoch: 1400 | Loss: 0.00395 | Test Loss: 0.00380 | Valid Loss: 0.00395\n",
      "Epoch: 1600 | Loss: 0.00376 | Test Loss: 0.00363 | Valid Loss: 0.00376\n",
      "Epoch: 1800 | Loss: 0.00355 | Test Loss: 0.00345 | Valid Loss: 0.00356\n",
      "Epoch: 2000 | Loss: 0.00334 | Test Loss: 0.00325 | Valid Loss: 0.00335\n",
      "Epoch: 2200 | Loss: 0.00312 | Test Loss: 0.00305 | Valid Loss: 0.00313\n",
      "Epoch: 2400 | Loss: 0.00289 | Test Loss: 0.00285 | Valid Loss: 0.00291\n",
      "Epoch: 2600 | Loss: 0.00267 | Test Loss: 0.00264 | Valid Loss: 0.00270\n",
      "Epoch: 2800 | Loss: 0.00246 | Test Loss: 0.00244 | Valid Loss: 0.00248\n",
      "Epoch: 3000 | Loss: 0.00225 | Test Loss: 0.00224 | Valid Loss: 0.00228\n",
      "Epoch: 3200 | Loss: 0.00206 | Test Loss: 0.00205 | Valid Loss: 0.00208\n",
      "Epoch: 3400 | Loss: 0.00189 | Test Loss: 0.00189 | Valid Loss: 0.00191\n",
      "Epoch: 3600 | Loss: 0.00174 | Test Loss: 0.00173 | Valid Loss: 0.00176\n",
      "Epoch: 3800 | Loss: 0.00160 | Test Loss: 0.00160 | Valid Loss: 0.00162\n",
      "Epoch: 4000 | Loss: 0.00148 | Test Loss: 0.00147 | Valid Loss: 0.00149\n",
      "Epoch: 4200 | Loss: 0.00136 | Test Loss: 0.00136 | Valid Loss: 0.00138\n",
      "Epoch: 4400 | Loss: 0.00126 | Test Loss: 0.00125 | Valid Loss: 0.00128\n",
      "Epoch: 4600 | Loss: 0.00116 | Test Loss: 0.00116 | Valid Loss: 0.00118\n",
      "Epoch: 4800 | Loss: 0.00107 | Test Loss: 0.00107 | Valid Loss: 0.00109\n",
      "Epoch: 5000 | Loss: 0.00098 | Test Loss: 0.00099 | Valid Loss: 0.00101\n",
      "Epoch: 5200 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00094\n",
      "Epoch: 5400 | Loss: 0.00085 | Test Loss: 0.00085 | Valid Loss: 0.00087\n",
      "Epoch: 5600 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00082\n",
      "Epoch: 5800 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00077\n",
      "Epoch: 6000 | Loss: 0.00074 | Test Loss: 0.00071 | Valid Loss: 0.00074\n",
      "Epoch: 6200 | Loss: 0.00067 | Test Loss: 0.00069 | Valid Loss: 0.00071\n",
      "Epoch: 6400 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00065\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00061\n",
      "Epoch: 6800 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00058\n",
      "Epoch: 7000 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00056\n",
      "Epoch: 7200 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00053\n",
      "Epoch: 7400 | Loss: 0.00050 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 7600 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00049\n",
      "Early stopping at epoch: 7700\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 2.77760 | Test Loss: 2.55460 | Valid Loss: 2.53650\n",
      "Epoch: 200 | Loss: 0.00591 | Test Loss: 0.00603 | Valid Loss: 0.00577\n",
      "Epoch: 400 | Loss: 0.00451 | Test Loss: 0.00461 | Valid Loss: 0.00439\n",
      "Epoch: 600 | Loss: 0.00437 | Test Loss: 0.00447 | Valid Loss: 0.00426\n",
      "Epoch: 800 | Loss: 0.00429 | Test Loss: 0.00438 | Valid Loss: 0.00418\n",
      "Epoch: 1000 | Loss: 0.00420 | Test Loss: 0.00430 | Valid Loss: 0.00409\n",
      "Epoch: 1200 | Loss: 0.00411 | Test Loss: 0.00420 | Valid Loss: 0.00400\n",
      "Epoch: 1400 | Loss: 0.00400 | Test Loss: 0.00409 | Valid Loss: 0.00389\n",
      "Epoch: 1600 | Loss: 0.00388 | Test Loss: 0.00398 | Valid Loss: 0.00378\n",
      "Epoch: 1800 | Loss: 0.00374 | Test Loss: 0.00384 | Valid Loss: 0.00365\n",
      "Epoch: 2000 | Loss: 0.00360 | Test Loss: 0.00370 | Valid Loss: 0.00350\n",
      "Epoch: 2200 | Loss: 0.00344 | Test Loss: 0.00354 | Valid Loss: 0.00335\n",
      "Epoch: 2400 | Loss: 0.00326 | Test Loss: 0.00337 | Valid Loss: 0.00318\n",
      "Epoch: 2600 | Loss: 0.00308 | Test Loss: 0.00319 | Valid Loss: 0.00300\n",
      "Epoch: 2800 | Loss: 0.00290 | Test Loss: 0.00300 | Valid Loss: 0.00282\n",
      "Epoch: 3000 | Loss: 0.00271 | Test Loss: 0.00281 | Valid Loss: 0.00264\n",
      "Epoch: 3200 | Loss: 0.00253 | Test Loss: 0.00263 | Valid Loss: 0.00246\n",
      "Epoch: 3400 | Loss: 0.00236 | Test Loss: 0.00246 | Valid Loss: 0.00230\n",
      "Epoch: 3600 | Loss: 0.00221 | Test Loss: 0.00231 | Valid Loss: 0.00215\n",
      "Epoch: 3800 | Loss: 0.00207 | Test Loss: 0.00217 | Valid Loss: 0.00202\n",
      "Epoch: 4000 | Loss: 0.00194 | Test Loss: 0.00203 | Valid Loss: 0.00189\n",
      "Epoch: 4200 | Loss: 0.00182 | Test Loss: 0.00191 | Valid Loss: 0.00178\n",
      "Epoch: 4400 | Loss: 0.00171 | Test Loss: 0.00178 | Valid Loss: 0.00166\n",
      "Epoch: 4600 | Loss: 0.00159 | Test Loss: 0.00166 | Valid Loss: 0.00155\n",
      "Epoch: 4800 | Loss: 0.00147 | Test Loss: 0.00154 | Valid Loss: 0.00143\n",
      "Epoch: 5000 | Loss: 0.00135 | Test Loss: 0.00141 | Valid Loss: 0.00132\n",
      "Epoch: 5200 | Loss: 0.00124 | Test Loss: 0.00129 | Valid Loss: 0.00121\n",
      "Epoch: 5400 | Loss: 0.00113 | Test Loss: 0.00117 | Valid Loss: 0.00111\n",
      "Epoch: 5600 | Loss: 0.00103 | Test Loss: 0.00107 | Valid Loss: 0.00101\n",
      "Epoch: 5800 | Loss: 0.00094 | Test Loss: 0.00098 | Valid Loss: 0.00093\n",
      "Epoch: 6000 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00087\n",
      "Epoch: 6200 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 6400 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00076\n",
      "Epoch: 6600 | Loss: 0.00072 | Test Loss: 0.00075 | Valid Loss: 0.00072\n",
      "Epoch: 6800 | Loss: 0.00067 | Test Loss: 0.00071 | Valid Loss: 0.00068\n",
      "Epoch: 7000 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00064\n",
      "Epoch: 7200 | Loss: 0.00060 | Test Loss: 0.00063 | Valid Loss: 0.00061\n",
      "Epoch: 7400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 7600 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Early stopping at epoch: 7721\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.54108 | Test Loss: 2.32155 | Valid Loss: 2.31725\n",
      "Epoch: 200 | Loss: 0.00596 | Test Loss: 0.00610 | Valid Loss: 0.00598\n",
      "Epoch: 400 | Loss: 0.00445 | Test Loss: 0.00459 | Valid Loss: 0.00450\n",
      "Epoch: 600 | Loss: 0.00432 | Test Loss: 0.00445 | Valid Loss: 0.00438\n",
      "Epoch: 800 | Loss: 0.00425 | Test Loss: 0.00438 | Valid Loss: 0.00431\n",
      "Epoch: 1000 | Loss: 0.00418 | Test Loss: 0.00430 | Valid Loss: 0.00424\n",
      "Epoch: 1200 | Loss: 0.00409 | Test Loss: 0.00422 | Valid Loss: 0.00415\n",
      "Epoch: 1400 | Loss: 0.00399 | Test Loss: 0.00412 | Valid Loss: 0.00405\n",
      "Epoch: 1600 | Loss: 0.00388 | Test Loss: 0.00400 | Valid Loss: 0.00394\n",
      "Epoch: 1800 | Loss: 0.00376 | Test Loss: 0.00388 | Valid Loss: 0.00382\n",
      "Epoch: 2000 | Loss: 0.00362 | Test Loss: 0.00374 | Valid Loss: 0.00368\n",
      "Epoch: 2200 | Loss: 0.00348 | Test Loss: 0.00359 | Valid Loss: 0.00353\n",
      "Epoch: 2400 | Loss: 0.00332 | Test Loss: 0.00343 | Valid Loss: 0.00338\n",
      "Epoch: 2600 | Loss: 0.00316 | Test Loss: 0.00326 | Valid Loss: 0.00321\n",
      "Epoch: 2800 | Loss: 0.00299 | Test Loss: 0.00308 | Valid Loss: 0.00304\n",
      "Epoch: 3000 | Loss: 0.00282 | Test Loss: 0.00291 | Valid Loss: 0.00287\n",
      "Epoch: 3200 | Loss: 0.00266 | Test Loss: 0.00274 | Valid Loss: 0.00270\n",
      "Epoch: 3400 | Loss: 0.00251 | Test Loss: 0.00258 | Valid Loss: 0.00254\n",
      "Epoch: 3600 | Loss: 0.00236 | Test Loss: 0.00244 | Valid Loss: 0.00240\n",
      "Epoch: 3800 | Loss: 0.00224 | Test Loss: 0.00231 | Valid Loss: 0.00227\n",
      "Epoch: 4000 | Loss: 0.00212 | Test Loss: 0.00219 | Valid Loss: 0.00215\n",
      "Epoch: 4200 | Loss: 0.00202 | Test Loss: 0.00209 | Valid Loss: 0.00204\n",
      "Epoch: 4400 | Loss: 0.00192 | Test Loss: 0.00199 | Valid Loss: 0.00194\n",
      "Epoch: 4600 | Loss: 0.00182 | Test Loss: 0.00189 | Valid Loss: 0.00184\n",
      "Epoch: 4800 | Loss: 0.00172 | Test Loss: 0.00179 | Valid Loss: 0.00174\n",
      "Epoch: 5000 | Loss: 0.00161 | Test Loss: 0.00168 | Valid Loss: 0.00163\n",
      "Epoch: 5200 | Loss: 0.00150 | Test Loss: 0.00156 | Valid Loss: 0.00151\n",
      "Epoch: 5400 | Loss: 0.00137 | Test Loss: 0.00143 | Valid Loss: 0.00138\n",
      "Epoch: 5600 | Loss: 0.00123 | Test Loss: 0.00129 | Valid Loss: 0.00125\n",
      "Epoch: 5800 | Loss: 0.00112 | Test Loss: 0.00117 | Valid Loss: 0.00114\n",
      "Epoch: 6000 | Loss: 0.00102 | Test Loss: 0.00107 | Valid Loss: 0.00104\n",
      "Epoch: 6200 | Loss: 0.00093 | Test Loss: 0.00098 | Valid Loss: 0.00095\n",
      "Epoch: 6400 | Loss: 0.00085 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6600 | Loss: 0.00079 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 6800 | Loss: 0.00074 | Test Loss: 0.00078 | Valid Loss: 0.00076\n",
      "Epoch: 7000 | Loss: 0.00070 | Test Loss: 0.00074 | Valid Loss: 0.00072\n",
      "Epoch: 7200 | Loss: 0.00065 | Test Loss: 0.00069 | Valid Loss: 0.00067\n",
      "Epoch: 7400 | Loss: 0.00062 | Test Loss: 0.00066 | Valid Loss: 0.00064\n",
      "Epoch: 7600 | Loss: 0.00059 | Test Loss: 0.00063 | Valid Loss: 0.00061\n",
      "Epoch: 7800 | Loss: 0.00056 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.26535 | Test Loss: 1.12852 | Valid Loss: 1.12190\n",
      "Epoch: 200 | Loss: 0.00489 | Test Loss: 0.00486 | Valid Loss: 0.00498\n",
      "Epoch: 400 | Loss: 0.00454 | Test Loss: 0.00453 | Valid Loss: 0.00465\n",
      "Epoch: 600 | Loss: 0.00444 | Test Loss: 0.00445 | Valid Loss: 0.00455\n",
      "Epoch: 800 | Loss: 0.00433 | Test Loss: 0.00434 | Valid Loss: 0.00444\n",
      "Epoch: 1000 | Loss: 0.00420 | Test Loss: 0.00422 | Valid Loss: 0.00431\n",
      "Epoch: 1200 | Loss: 0.00406 | Test Loss: 0.00409 | Valid Loss: 0.00417\n",
      "Epoch: 1400 | Loss: 0.00391 | Test Loss: 0.00394 | Valid Loss: 0.00401\n",
      "Epoch: 1600 | Loss: 0.00374 | Test Loss: 0.00378 | Valid Loss: 0.00383\n",
      "Epoch: 1800 | Loss: 0.00356 | Test Loss: 0.00361 | Valid Loss: 0.00365\n",
      "Epoch: 2000 | Loss: 0.00336 | Test Loss: 0.00342 | Valid Loss: 0.00345\n",
      "Epoch: 2200 | Loss: 0.00316 | Test Loss: 0.00323 | Valid Loss: 0.00324\n",
      "Epoch: 2400 | Loss: 0.00296 | Test Loss: 0.00303 | Valid Loss: 0.00303\n",
      "Epoch: 2600 | Loss: 0.00276 | Test Loss: 0.00284 | Valid Loss: 0.00282\n",
      "Epoch: 2800 | Loss: 0.00257 | Test Loss: 0.00265 | Valid Loss: 0.00262\n",
      "Epoch: 3000 | Loss: 0.00240 | Test Loss: 0.00248 | Valid Loss: 0.00244\n",
      "Epoch: 3200 | Loss: 0.00225 | Test Loss: 0.00234 | Valid Loss: 0.00229\n",
      "Epoch: 3400 | Loss: 0.00212 | Test Loss: 0.00220 | Valid Loss: 0.00215\n",
      "Epoch: 3600 | Loss: 0.00200 | Test Loss: 0.00209 | Valid Loss: 0.00202\n",
      "Epoch: 3800 | Loss: 0.00189 | Test Loss: 0.00197 | Valid Loss: 0.00191\n",
      "Epoch: 4000 | Loss: 0.00177 | Test Loss: 0.00185 | Valid Loss: 0.00179\n",
      "Epoch: 4200 | Loss: 0.00164 | Test Loss: 0.00171 | Valid Loss: 0.00165\n",
      "Epoch: 4400 | Loss: 0.00149 | Test Loss: 0.00156 | Valid Loss: 0.00150\n",
      "Epoch: 4600 | Loss: 0.00137 | Test Loss: 0.00143 | Valid Loss: 0.00137\n",
      "Epoch: 4800 | Loss: 0.00125 | Test Loss: 0.00130 | Valid Loss: 0.00125\n",
      "Epoch: 5000 | Loss: 0.00114 | Test Loss: 0.00118 | Valid Loss: 0.00114\n",
      "Epoch: 5200 | Loss: 0.00104 | Test Loss: 0.00108 | Valid Loss: 0.00105\n",
      "Epoch: 5400 | Loss: 0.00097 | Test Loss: 0.00099 | Valid Loss: 0.00098\n",
      "Epoch: 5600 | Loss: 0.00113 | Test Loss: 0.00110 | Valid Loss: 0.00110\n",
      "Epoch: 5800 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 6000 | Loss: 0.00078 | Test Loss: 0.00080 | Valid Loss: 0.00079\n",
      "Epoch: 6200 | Loss: 0.00073 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 6400 | Loss: 0.00071 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 6600 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00068\n",
      "Epoch: 6800 | Loss: 0.00063 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 7000 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 7200 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 7400 | Loss: 0.00069 | Test Loss: 0.00067 | Valid Loss: 0.00066\n",
      "Epoch: 7600 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 7800 | Loss: 0.00051 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.15312 | Test Loss: 1.00225 | Valid Loss: 1.01037\n",
      "Epoch: 200 | Loss: 0.00494 | Test Loss: 0.00485 | Valid Loss: 0.00509\n",
      "Epoch: 400 | Loss: 0.00454 | Test Loss: 0.00446 | Valid Loss: 0.00468\n",
      "Epoch: 600 | Loss: 0.00442 | Test Loss: 0.00435 | Valid Loss: 0.00455\n",
      "Epoch: 800 | Loss: 0.00429 | Test Loss: 0.00423 | Valid Loss: 0.00442\n",
      "Epoch: 1000 | Loss: 0.00414 | Test Loss: 0.00409 | Valid Loss: 0.00427\n",
      "Epoch: 1200 | Loss: 0.00399 | Test Loss: 0.00394 | Valid Loss: 0.00411\n",
      "Epoch: 1400 | Loss: 0.00383 | Test Loss: 0.00378 | Valid Loss: 0.00394\n",
      "Epoch: 1600 | Loss: 0.00365 | Test Loss: 0.00361 | Valid Loss: 0.00376\n",
      "Epoch: 1800 | Loss: 0.00347 | Test Loss: 0.00343 | Valid Loss: 0.00357\n",
      "Epoch: 2000 | Loss: 0.00328 | Test Loss: 0.00325 | Valid Loss: 0.00337\n",
      "Epoch: 2200 | Loss: 0.00309 | Test Loss: 0.00306 | Valid Loss: 0.00317\n",
      "Epoch: 2400 | Loss: 0.00289 | Test Loss: 0.00287 | Valid Loss: 0.00296\n",
      "Epoch: 2600 | Loss: 0.00270 | Test Loss: 0.00269 | Valid Loss: 0.00276\n",
      "Epoch: 2800 | Loss: 0.00251 | Test Loss: 0.00251 | Valid Loss: 0.00257\n",
      "Epoch: 3000 | Loss: 0.00233 | Test Loss: 0.00234 | Valid Loss: 0.00239\n",
      "Epoch: 3200 | Loss: 0.00217 | Test Loss: 0.00219 | Valid Loss: 0.00222\n",
      "Epoch: 3400 | Loss: 0.00202 | Test Loss: 0.00204 | Valid Loss: 0.00206\n",
      "Epoch: 3600 | Loss: 0.00188 | Test Loss: 0.00190 | Valid Loss: 0.00191\n",
      "Epoch: 3800 | Loss: 0.00174 | Test Loss: 0.00176 | Valid Loss: 0.00176\n",
      "Epoch: 4000 | Loss: 0.00158 | Test Loss: 0.00160 | Valid Loss: 0.00160\n",
      "Epoch: 4200 | Loss: 0.00144 | Test Loss: 0.00145 | Valid Loss: 0.00146\n",
      "Epoch: 4400 | Loss: 0.00132 | Test Loss: 0.00133 | Valid Loss: 0.00134\n",
      "Epoch: 4600 | Loss: 0.00122 | Test Loss: 0.00122 | Valid Loss: 0.00123\n",
      "Epoch: 4800 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00113\n",
      "Epoch: 5000 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00104\n",
      "Epoch: 5200 | Loss: 0.00095 | Test Loss: 0.00094 | Valid Loss: 0.00096\n",
      "Epoch: 5400 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00089\n",
      "Epoch: 5600 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00083\n",
      "Epoch: 5800 | Loss: 0.00077 | Test Loss: 0.00075 | Valid Loss: 0.00078\n",
      "Epoch: 6000 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00074\n",
      "Epoch: 6200 | Loss: 0.00068 | Test Loss: 0.00067 | Valid Loss: 0.00070\n",
      "Epoch: 6400 | Loss: 0.00065 | Test Loss: 0.00063 | Valid Loss: 0.00066\n",
      "Epoch: 6600 | Loss: 0.00061 | Test Loss: 0.00059 | Valid Loss: 0.00063\n",
      "Epoch: 6800 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00060\n",
      "Epoch: 7000 | Loss: 0.00066 | Test Loss: 0.00054 | Valid Loss: 0.00057\n",
      "Epoch: 7200 | Loss: 0.00053 | Test Loss: 0.00051 | Valid Loss: 0.00055\n",
      "Epoch: 7400 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00053\n",
      "Epoch: 7600 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00051\n",
      "Epoch: 7800 | Loss: 0.00047 | Test Loss: 0.00045 | Valid Loss: 0.00049\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 0.95858 | Test Loss: 0.83005 | Valid Loss: 0.83716\n",
      "Epoch: 200 | Loss: 0.00509 | Test Loss: 0.00510 | Valid Loss: 0.00529\n",
      "Epoch: 400 | Loss: 0.00468 | Test Loss: 0.00472 | Valid Loss: 0.00489\n",
      "Epoch: 600 | Loss: 0.00454 | Test Loss: 0.00460 | Valid Loss: 0.00475\n",
      "Epoch: 800 | Loss: 0.00440 | Test Loss: 0.00447 | Valid Loss: 0.00461\n",
      "Epoch: 1000 | Loss: 0.00426 | Test Loss: 0.00432 | Valid Loss: 0.00445\n",
      "Epoch: 1200 | Loss: 0.00410 | Test Loss: 0.00416 | Valid Loss: 0.00428\n",
      "Epoch: 1400 | Loss: 0.00392 | Test Loss: 0.00398 | Valid Loss: 0.00410\n",
      "Epoch: 1600 | Loss: 0.00373 | Test Loss: 0.00379 | Valid Loss: 0.00390\n",
      "Epoch: 1800 | Loss: 0.00352 | Test Loss: 0.00358 | Valid Loss: 0.00368\n",
      "Epoch: 2000 | Loss: 0.00330 | Test Loss: 0.00336 | Valid Loss: 0.00344\n",
      "Epoch: 2200 | Loss: 0.00306 | Test Loss: 0.00312 | Valid Loss: 0.00318\n",
      "Epoch: 2400 | Loss: 0.00281 | Test Loss: 0.00287 | Valid Loss: 0.00292\n",
      "Epoch: 2600 | Loss: 0.00257 | Test Loss: 0.00262 | Valid Loss: 0.00265\n",
      "Epoch: 2800 | Loss: 0.00234 | Test Loss: 0.00238 | Valid Loss: 0.00240\n",
      "Epoch: 3000 | Loss: 0.00213 | Test Loss: 0.00216 | Valid Loss: 0.00217\n",
      "Epoch: 3200 | Loss: 0.00194 | Test Loss: 0.00197 | Valid Loss: 0.00198\n",
      "Epoch: 3400 | Loss: 0.00178 | Test Loss: 0.00180 | Valid Loss: 0.00180\n",
      "Epoch: 3600 | Loss: 0.00162 | Test Loss: 0.00164 | Valid Loss: 0.00163\n",
      "Epoch: 3800 | Loss: 0.00150 | Test Loss: 0.00152 | Valid Loss: 0.00150\n",
      "Epoch: 4000 | Loss: 0.00140 | Test Loss: 0.00142 | Valid Loss: 0.00140\n",
      "Epoch: 4200 | Loss: 0.00130 | Test Loss: 0.00132 | Valid Loss: 0.00130\n",
      "Epoch: 4400 | Loss: 0.00121 | Test Loss: 0.00123 | Valid Loss: 0.00121\n",
      "Epoch: 4600 | Loss: 0.00112 | Test Loss: 0.00114 | Valid Loss: 0.00112\n",
      "Epoch: 4800 | Loss: 0.00103 | Test Loss: 0.00105 | Valid Loss: 0.00103\n",
      "Epoch: 5000 | Loss: 0.00097 | Test Loss: 0.00097 | Valid Loss: 0.00097\n",
      "Epoch: 5200 | Loss: 0.00088 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 5400 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00083\n",
      "Epoch: 5600 | Loss: 0.00076 | Test Loss: 0.00077 | Valid Loss: 0.00077\n",
      "Epoch: 5800 | Loss: 0.00071 | Test Loss: 0.00072 | Valid Loss: 0.00073\n",
      "Epoch: 6000 | Loss: 0.00067 | Test Loss: 0.00068 | Valid Loss: 0.00069\n",
      "Epoch: 6200 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00065\n",
      "Epoch: 6400 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "Epoch: 6600 | Loss: 0.00065 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 6800 | Loss: 0.00054 | Test Loss: 0.00054 | Valid Loss: 0.00056\n",
      "Epoch: 7000 | Loss: 0.00052 | Test Loss: 0.00052 | Valid Loss: 0.00054\n",
      "Epoch: 7200 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Early stopping at epoch: 7363\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 2.06685 | Test Loss: 1.86460 | Valid Loss: 1.87019\n",
      "Epoch: 200 | Loss: 0.00686 | Test Loss: 0.00675 | Valid Loss: 0.00679\n",
      "Epoch: 400 | Loss: 0.00509 | Test Loss: 0.00510 | Valid Loss: 0.00513\n",
      "Epoch: 600 | Loss: 0.00477 | Test Loss: 0.00483 | Valid Loss: 0.00483\n",
      "Epoch: 800 | Loss: 0.00464 | Test Loss: 0.00473 | Valid Loss: 0.00471\n",
      "Epoch: 1000 | Loss: 0.00454 | Test Loss: 0.00464 | Valid Loss: 0.00461\n",
      "Epoch: 1200 | Loss: 0.00443 | Test Loss: 0.00452 | Valid Loss: 0.00449\n",
      "Epoch: 1400 | Loss: 0.00430 | Test Loss: 0.00440 | Valid Loss: 0.00437\n",
      "Epoch: 1600 | Loss: 0.00416 | Test Loss: 0.00425 | Valid Loss: 0.00422\n",
      "Epoch: 1800 | Loss: 0.00400 | Test Loss: 0.00408 | Valid Loss: 0.00406\n",
      "Epoch: 2000 | Loss: 0.00382 | Test Loss: 0.00390 | Valid Loss: 0.00387\n",
      "Epoch: 2200 | Loss: 0.00362 | Test Loss: 0.00369 | Valid Loss: 0.00367\n",
      "Epoch: 2400 | Loss: 0.00341 | Test Loss: 0.00346 | Valid Loss: 0.00345\n",
      "Epoch: 2600 | Loss: 0.00318 | Test Loss: 0.00322 | Valid Loss: 0.00321\n",
      "Epoch: 2800 | Loss: 0.00294 | Test Loss: 0.00298 | Valid Loss: 0.00297\n",
      "Epoch: 3000 | Loss: 0.00270 | Test Loss: 0.00273 | Valid Loss: 0.00272\n",
      "Epoch: 3200 | Loss: 0.00247 | Test Loss: 0.00249 | Valid Loss: 0.00249\n",
      "Epoch: 3400 | Loss: 0.00227 | Test Loss: 0.00227 | Valid Loss: 0.00228\n",
      "Epoch: 3600 | Loss: 0.00208 | Test Loss: 0.00209 | Valid Loss: 0.00209\n",
      "Epoch: 3800 | Loss: 0.00192 | Test Loss: 0.00192 | Valid Loss: 0.00193\n",
      "Epoch: 4000 | Loss: 0.00179 | Test Loss: 0.00179 | Valid Loss: 0.00181\n",
      "Epoch: 4200 | Loss: 0.00169 | Test Loss: 0.00168 | Valid Loss: 0.00170\n",
      "Epoch: 4400 | Loss: 0.00160 | Test Loss: 0.00160 | Valid Loss: 0.00162\n",
      "Epoch: 4600 | Loss: 0.00152 | Test Loss: 0.00152 | Valid Loss: 0.00154\n",
      "Epoch: 4800 | Loss: 0.00145 | Test Loss: 0.00145 | Valid Loss: 0.00147\n",
      "Epoch: 5000 | Loss: 0.00138 | Test Loss: 0.00138 | Valid Loss: 0.00140\n",
      "Epoch: 5200 | Loss: 0.00131 | Test Loss: 0.00130 | Valid Loss: 0.00133\n",
      "Epoch: 5400 | Loss: 0.00123 | Test Loss: 0.00123 | Valid Loss: 0.00125\n",
      "Epoch: 5600 | Loss: 0.00115 | Test Loss: 0.00115 | Valid Loss: 0.00117\n",
      "Epoch: 5800 | Loss: 0.00107 | Test Loss: 0.00107 | Valid Loss: 0.00109\n",
      "Epoch: 6000 | Loss: 0.00099 | Test Loss: 0.00099 | Valid Loss: 0.00101\n",
      "Epoch: 6200 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00093\n",
      "Epoch: 6400 | Loss: 0.00085 | Test Loss: 0.00084 | Valid Loss: 0.00089\n",
      "Epoch: 6600 | Loss: 0.00078 | Test Loss: 0.00077 | Valid Loss: 0.00080\n",
      "Epoch: 6800 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00074\n",
      "Epoch: 7000 | Loss: 0.00067 | Test Loss: 0.00066 | Valid Loss: 0.00069\n",
      "Epoch: 7200 | Loss: 0.00063 | Test Loss: 0.00062 | Valid Loss: 0.00065\n",
      "Epoch: 7400 | Loss: 0.00059 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Early stopping at epoch: 7553\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 3.26310 | Test Loss: 2.99526 | Valid Loss: 3.00579\n",
      "Epoch: 200 | Loss: 0.00968 | Test Loss: 0.00926 | Valid Loss: 0.00975\n",
      "Epoch: 400 | Loss: 0.00625 | Test Loss: 0.00606 | Valid Loss: 0.00637\n",
      "Epoch: 600 | Loss: 0.00534 | Test Loss: 0.00524 | Valid Loss: 0.00547\n",
      "Epoch: 800 | Loss: 0.00500 | Test Loss: 0.00496 | Valid Loss: 0.00513\n",
      "Epoch: 1000 | Loss: 0.00482 | Test Loss: 0.00481 | Valid Loss: 0.00495\n",
      "Epoch: 1200 | Loss: 0.00466 | Test Loss: 0.00466 | Valid Loss: 0.00478\n",
      "Epoch: 1400 | Loss: 0.00448 | Test Loss: 0.00448 | Valid Loss: 0.00459\n",
      "Epoch: 1600 | Loss: 0.00428 | Test Loss: 0.00428 | Valid Loss: 0.00438\n",
      "Epoch: 1800 | Loss: 0.00406 | Test Loss: 0.00406 | Valid Loss: 0.00415\n",
      "Epoch: 2000 | Loss: 0.00383 | Test Loss: 0.00382 | Valid Loss: 0.00391\n",
      "Epoch: 2200 | Loss: 0.00359 | Test Loss: 0.00357 | Valid Loss: 0.00365\n",
      "Epoch: 2400 | Loss: 0.00334 | Test Loss: 0.00332 | Valid Loss: 0.00340\n",
      "Epoch: 2600 | Loss: 0.00311 | Test Loss: 0.00308 | Valid Loss: 0.00316\n",
      "Epoch: 2800 | Loss: 0.00289 | Test Loss: 0.00285 | Valid Loss: 0.00293\n",
      "Epoch: 3000 | Loss: 0.00268 | Test Loss: 0.00263 | Valid Loss: 0.00272\n",
      "Epoch: 3200 | Loss: 0.00249 | Test Loss: 0.00243 | Valid Loss: 0.00252\n",
      "Epoch: 3400 | Loss: 0.00232 | Test Loss: 0.00226 | Valid Loss: 0.00235\n",
      "Epoch: 3600 | Loss: 0.00217 | Test Loss: 0.00211 | Valid Loss: 0.00221\n",
      "Epoch: 3800 | Loss: 0.00205 | Test Loss: 0.00198 | Valid Loss: 0.00209\n",
      "Epoch: 4000 | Loss: 0.00195 | Test Loss: 0.00188 | Valid Loss: 0.00198\n",
      "Epoch: 4200 | Loss: 0.00187 | Test Loss: 0.00180 | Valid Loss: 0.00190\n",
      "Epoch: 4400 | Loss: 0.00179 | Test Loss: 0.00172 | Valid Loss: 0.00182\n",
      "Epoch: 4600 | Loss: 0.00171 | Test Loss: 0.00165 | Valid Loss: 0.00174\n",
      "Epoch: 4800 | Loss: 0.00164 | Test Loss: 0.00157 | Valid Loss: 0.00167\n",
      "Epoch: 5000 | Loss: 0.00156 | Test Loss: 0.00150 | Valid Loss: 0.00160\n",
      "Epoch: 5200 | Loss: 0.00148 | Test Loss: 0.00143 | Valid Loss: 0.00152\n",
      "Epoch: 5400 | Loss: 0.00140 | Test Loss: 0.00135 | Valid Loss: 0.00143\n",
      "Epoch: 5600 | Loss: 0.00131 | Test Loss: 0.00127 | Valid Loss: 0.00135\n",
      "Epoch: 5800 | Loss: 0.00122 | Test Loss: 0.00118 | Valid Loss: 0.00125\n",
      "Epoch: 6000 | Loss: 0.00113 | Test Loss: 0.00109 | Valid Loss: 0.00116\n",
      "Epoch: 6200 | Loss: 0.00104 | Test Loss: 0.00100 | Valid Loss: 0.00107\n",
      "Epoch: 6400 | Loss: 0.00096 | Test Loss: 0.00093 | Valid Loss: 0.00099\n",
      "Epoch: 6600 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00091\n",
      "Epoch: 6800 | Loss: 0.00082 | Test Loss: 0.00080 | Valid Loss: 0.00085\n",
      "Epoch: 7000 | Loss: 0.00077 | Test Loss: 0.00074 | Valid Loss: 0.00079\n",
      "Epoch: 7200 | Loss: 0.00072 | Test Loss: 0.00070 | Valid Loss: 0.00074\n",
      "Epoch: 7400 | Loss: 0.00068 | Test Loss: 0.00066 | Valid Loss: 0.00070\n",
      "Epoch: 7600 | Loss: 0.00064 | Test Loss: 0.00062 | Valid Loss: 0.00067\n",
      "Epoch: 7800 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00064\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.12312 | Test Loss: 1.91066 | Valid Loss: 1.90721\n",
      "Epoch: 200 | Loss: 0.00842 | Test Loss: 0.00789 | Valid Loss: 0.00861\n",
      "Epoch: 400 | Loss: 0.00576 | Test Loss: 0.00542 | Valid Loss: 0.00588\n",
      "Epoch: 600 | Loss: 0.00530 | Test Loss: 0.00507 | Valid Loss: 0.00540\n",
      "Epoch: 800 | Loss: 0.00508 | Test Loss: 0.00489 | Valid Loss: 0.00516\n",
      "Epoch: 1000 | Loss: 0.00489 | Test Loss: 0.00471 | Valid Loss: 0.00495\n",
      "Epoch: 1200 | Loss: 0.00468 | Test Loss: 0.00451 | Valid Loss: 0.00473\n",
      "Epoch: 1400 | Loss: 0.00447 | Test Loss: 0.00431 | Valid Loss: 0.00451\n",
      "Epoch: 1600 | Loss: 0.00426 | Test Loss: 0.00411 | Valid Loss: 0.00428\n",
      "Epoch: 1800 | Loss: 0.00405 | Test Loss: 0.00391 | Valid Loss: 0.00406\n",
      "Epoch: 2000 | Loss: 0.00385 | Test Loss: 0.00372 | Valid Loss: 0.00385\n",
      "Epoch: 2200 | Loss: 0.00366 | Test Loss: 0.00353 | Valid Loss: 0.00365\n",
      "Epoch: 2400 | Loss: 0.00348 | Test Loss: 0.00336 | Valid Loss: 0.00346\n",
      "Epoch: 2600 | Loss: 0.00331 | Test Loss: 0.00320 | Valid Loss: 0.00328\n",
      "Epoch: 2800 | Loss: 0.00315 | Test Loss: 0.00304 | Valid Loss: 0.00312\n",
      "Epoch: 3000 | Loss: 0.00299 | Test Loss: 0.00289 | Valid Loss: 0.00296\n",
      "Epoch: 3200 | Loss: 0.00284 | Test Loss: 0.00275 | Valid Loss: 0.00281\n",
      "Epoch: 3400 | Loss: 0.00269 | Test Loss: 0.00261 | Valid Loss: 0.00266\n",
      "Epoch: 3600 | Loss: 0.00254 | Test Loss: 0.00248 | Valid Loss: 0.00252\n",
      "Epoch: 3800 | Loss: 0.00241 | Test Loss: 0.00235 | Valid Loss: 0.00239\n",
      "Epoch: 4000 | Loss: 0.00229 | Test Loss: 0.00224 | Valid Loss: 0.00227\n",
      "Epoch: 4200 | Loss: 0.00217 | Test Loss: 0.00214 | Valid Loss: 0.00215\n",
      "Epoch: 4400 | Loss: 0.00206 | Test Loss: 0.00204 | Valid Loss: 0.00204\n",
      "Epoch: 4600 | Loss: 0.00195 | Test Loss: 0.00193 | Valid Loss: 0.00193\n",
      "Epoch: 4800 | Loss: 0.00183 | Test Loss: 0.00182 | Valid Loss: 0.00181\n",
      "Epoch: 5000 | Loss: 0.00171 | Test Loss: 0.00171 | Valid Loss: 0.00169\n",
      "Epoch: 5200 | Loss: 0.00159 | Test Loss: 0.00159 | Valid Loss: 0.00157\n",
      "Epoch: 5400 | Loss: 0.00146 | Test Loss: 0.00146 | Valid Loss: 0.00144\n",
      "Epoch: 5600 | Loss: 0.00132 | Test Loss: 0.00133 | Valid Loss: 0.00131\n",
      "Epoch: 5800 | Loss: 0.00119 | Test Loss: 0.00121 | Valid Loss: 0.00118\n",
      "Epoch: 6000 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00107\n",
      "Epoch: 6200 | Loss: 0.00097 | Test Loss: 0.00099 | Valid Loss: 0.00097\n",
      "Epoch: 6400 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 6600 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00081\n",
      "Epoch: 6800 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 7000 | Loss: 0.00070 | Test Loss: 0.00073 | Valid Loss: 0.00070\n",
      "Epoch: 7200 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Epoch: 7400 | Loss: 0.00063 | Test Loss: 0.00065 | Valid Loss: 0.00062\n",
      "Epoch: 7600 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 7800 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Early stopping at epoch: 7923\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.66623 | Test Loss: 1.47978 | Valid Loss: 1.46984\n",
      "Epoch: 200 | Loss: 0.00801 | Test Loss: 0.00758 | Valid Loss: 0.00786\n",
      "Epoch: 400 | Loss: 0.00480 | Test Loss: 0.00463 | Valid Loss: 0.00473\n",
      "Epoch: 600 | Loss: 0.00426 | Test Loss: 0.00414 | Valid Loss: 0.00419\n",
      "Epoch: 800 | Loss: 0.00410 | Test Loss: 0.00399 | Valid Loss: 0.00403\n",
      "Epoch: 1000 | Loss: 0.00396 | Test Loss: 0.00386 | Valid Loss: 0.00389\n",
      "Epoch: 1200 | Loss: 0.00383 | Test Loss: 0.00373 | Valid Loss: 0.00377\n",
      "Epoch: 1400 | Loss: 0.00369 | Test Loss: 0.00361 | Valid Loss: 0.00364\n",
      "Epoch: 1600 | Loss: 0.00356 | Test Loss: 0.00348 | Valid Loss: 0.00352\n",
      "Epoch: 1800 | Loss: 0.00343 | Test Loss: 0.00336 | Valid Loss: 0.00339\n",
      "Epoch: 2000 | Loss: 0.00330 | Test Loss: 0.00323 | Valid Loss: 0.00326\n",
      "Epoch: 2200 | Loss: 0.00316 | Test Loss: 0.00310 | Valid Loss: 0.00313\n",
      "Epoch: 2400 | Loss: 0.00303 | Test Loss: 0.00297 | Valid Loss: 0.00300\n",
      "Epoch: 2600 | Loss: 0.00289 | Test Loss: 0.00284 | Valid Loss: 0.00286\n",
      "Epoch: 2800 | Loss: 0.00275 | Test Loss: 0.00271 | Valid Loss: 0.00273\n",
      "Epoch: 3000 | Loss: 0.00262 | Test Loss: 0.00259 | Valid Loss: 0.00260\n",
      "Epoch: 3200 | Loss: 0.00249 | Test Loss: 0.00247 | Valid Loss: 0.00247\n",
      "Epoch: 3400 | Loss: 0.00237 | Test Loss: 0.00235 | Valid Loss: 0.00235\n",
      "Epoch: 3600 | Loss: 0.00225 | Test Loss: 0.00224 | Valid Loss: 0.00223\n",
      "Epoch: 3800 | Loss: 0.00213 | Test Loss: 0.00213 | Valid Loss: 0.00211\n",
      "Epoch: 4000 | Loss: 0.00201 | Test Loss: 0.00202 | Valid Loss: 0.00200\n",
      "Epoch: 4200 | Loss: 0.00188 | Test Loss: 0.00190 | Valid Loss: 0.00187\n",
      "Epoch: 4400 | Loss: 0.00175 | Test Loss: 0.00177 | Valid Loss: 0.00174\n",
      "Epoch: 4600 | Loss: 0.00161 | Test Loss: 0.00163 | Valid Loss: 0.00161\n",
      "Epoch: 4800 | Loss: 0.00147 | Test Loss: 0.00149 | Valid Loss: 0.00146\n",
      "Epoch: 5000 | Loss: 0.00132 | Test Loss: 0.00135 | Valid Loss: 0.00132\n",
      "Epoch: 5200 | Loss: 0.00118 | Test Loss: 0.00120 | Valid Loss: 0.00118\n",
      "Epoch: 5400 | Loss: 0.00103 | Test Loss: 0.00106 | Valid Loss: 0.00104\n",
      "Epoch: 5600 | Loss: 0.00091 | Test Loss: 0.00093 | Valid Loss: 0.00091\n",
      "Epoch: 5800 | Loss: 0.00081 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 6000 | Loss: 0.00073 | Test Loss: 0.00075 | Valid Loss: 0.00074\n",
      "Epoch: 6200 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 6400 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 6600 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6800 | Loss: 0.00053 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 7000 | Loss: 0.00051 | Test Loss: 0.00052 | Valid Loss: 0.00052\n",
      "Epoch: 7200 | Loss: 0.00048 | Test Loss: 0.00050 | Valid Loss: 0.00049\n",
      "Epoch: 7400 | Loss: 0.00085 | Test Loss: 0.00065 | Valid Loss: 0.00062\n",
      "Early stopping at epoch: 7464\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 8000\n",
    "learning_rate = 0.002\n",
    "momentum = 0.8\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(M_MT_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_M_MT_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_M_MT_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_M_MT_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8c7997dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 5.1031 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(M_MT_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_M_MT_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "M_MT_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(M_MT_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45f3f",
   "metadata": {},
   "source": [
    "# Integration - G + M + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b3810d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 124\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "\n",
    "num_models = 15\n",
    "G_M_MT_models = []\n",
    "\n",
    "for i in range(num_models):\n",
    "    G_M_MT_models.append(Initial_Model_V0(input_dim, hidden_units, output_dim).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2df78441",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_G_M_MT_train, y_train = X_G_M_MT_train.to(device), y_train.to(device)\n",
    "X_G_M_MT_test, y_test = X_G_M_MT_test.to(device), y_test.to(device)\n",
    "X_G_M_MT_valid, y_valid = X_G_M_MT_valid.to(device), y_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a6aa7ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 0.86603 | Test Loss: 0.78196 | Valid Loss: 0.76145\n",
      "Epoch: 200 | Loss: 0.00284 | Test Loss: 0.00282 | Valid Loss: 0.00282\n",
      "Epoch: 400 | Loss: 0.00230 | Test Loss: 0.00235 | Valid Loss: 0.00227\n",
      "Epoch: 600 | Loss: 0.00207 | Test Loss: 0.00213 | Valid Loss: 0.00205\n",
      "Epoch: 800 | Loss: 0.00191 | Test Loss: 0.00195 | Valid Loss: 0.00188\n",
      "Epoch: 1000 | Loss: 0.00177 | Test Loss: 0.00181 | Valid Loss: 0.00174\n",
      "Epoch: 1200 | Loss: 0.00166 | Test Loss: 0.00170 | Valid Loss: 0.00163\n",
      "Epoch: 1400 | Loss: 0.00156 | Test Loss: 0.00160 | Valid Loss: 0.00154\n",
      "Epoch: 1600 | Loss: 0.00148 | Test Loss: 0.00152 | Valid Loss: 0.00146\n",
      "Epoch: 1800 | Loss: 0.00140 | Test Loss: 0.00144 | Valid Loss: 0.00138\n",
      "Epoch: 2000 | Loss: 0.00133 | Test Loss: 0.00136 | Valid Loss: 0.00131\n",
      "Epoch: 2200 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00124\n",
      "Epoch: 2400 | Loss: 0.00118 | Test Loss: 0.00121 | Valid Loss: 0.00116\n",
      "Epoch: 2600 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00109\n",
      "Epoch: 2800 | Loss: 0.00104 | Test Loss: 0.00106 | Valid Loss: 0.00102\n",
      "Epoch: 3000 | Loss: 0.00097 | Test Loss: 0.00099 | Valid Loss: 0.00096\n",
      "Epoch: 3200 | Loss: 0.00091 | Test Loss: 0.00093 | Valid Loss: 0.00090\n",
      "Epoch: 3400 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00085\n",
      "Epoch: 3600 | Loss: 0.00081 | Test Loss: 0.00083 | Valid Loss: 0.00080\n",
      "Epoch: 3800 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00076\n",
      "Epoch: 4000 | Loss: 0.00073 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 4200 | Loss: 0.00069 | Test Loss: 0.00071 | Valid Loss: 0.00069\n",
      "Epoch: 4400 | Loss: 0.00066 | Test Loss: 0.00067 | Valid Loss: 0.00066\n",
      "Epoch: 4600 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 4800 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 5000 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 5200 | Loss: 0.00052 | Test Loss: 0.00054 | Valid Loss: 0.00052\n",
      "Epoch: 5400 | Loss: 0.00049 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 5600 | Loss: 0.00046 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 5800 | Loss: 0.00044 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 6000 | Loss: 0.00041 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 6200 | Loss: 0.00039 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 6400 | Loss: 0.00044 | Test Loss: 0.00041 | Valid Loss: 0.00041\n",
      "Epoch: 6600 | Loss: 0.00035 | Test Loss: 0.00037 | Valid Loss: 0.00036\n",
      "Early stopping at epoch: 6650\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.40534 | Test Loss: 1.27794 | Valid Loss: 1.25202\n",
      "Epoch: 200 | Loss: 0.00325 | Test Loss: 0.00319 | Valid Loss: 0.00324\n",
      "Epoch: 400 | Loss: 0.00232 | Test Loss: 0.00229 | Valid Loss: 0.00231\n",
      "Epoch: 600 | Loss: 0.00199 | Test Loss: 0.00198 | Valid Loss: 0.00198\n",
      "Epoch: 800 | Loss: 0.00175 | Test Loss: 0.00175 | Valid Loss: 0.00175\n",
      "Epoch: 1000 | Loss: 0.00156 | Test Loss: 0.00156 | Valid Loss: 0.00156\n",
      "Epoch: 1200 | Loss: 0.00139 | Test Loss: 0.00140 | Valid Loss: 0.00139\n",
      "Epoch: 1400 | Loss: 0.00125 | Test Loss: 0.00126 | Valid Loss: 0.00125\n",
      "Epoch: 1600 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00113\n",
      "Epoch: 1800 | Loss: 0.00103 | Test Loss: 0.00105 | Valid Loss: 0.00104\n",
      "Epoch: 2000 | Loss: 0.00096 | Test Loss: 0.00097 | Valid Loss: 0.00096\n",
      "Epoch: 2200 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00091\n",
      "Epoch: 2400 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 2600 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00083\n",
      "Epoch: 2800 | Loss: 0.00078 | Test Loss: 0.00080 | Valid Loss: 0.00079\n",
      "Epoch: 3000 | Loss: 0.00076 | Test Loss: 0.00077 | Valid Loss: 0.00076\n",
      "Epoch: 3200 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 3400 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 3600 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 3800 | Loss: 0.00066 | Test Loss: 0.00067 | Valid Loss: 0.00067\n",
      "Epoch: 4000 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 4200 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 4400 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 4600 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00060\n",
      "Epoch: 4800 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 5000 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 5200 | Loss: 0.00054 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 5400 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00053\n",
      "Epoch: 5600 | Loss: 0.00050 | Test Loss: 0.00051 | Valid Loss: 0.00052\n",
      "Epoch: 5800 | Loss: 0.00048 | Test Loss: 0.00049 | Valid Loss: 0.00049\n",
      "Epoch: 6000 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00047\n",
      "Epoch: 6200 | Loss: 0.00044 | Test Loss: 0.00045 | Valid Loss: 0.00045\n",
      "Epoch: 6400 | Loss: 0.00042 | Test Loss: 0.00043 | Valid Loss: 0.00043\n",
      "Epoch: 6600 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 6800 | Loss: 0.00039 | Test Loss: 0.00040 | Valid Loss: 0.00040\n",
      "Epoch: 7000 | Loss: 0.00037 | Test Loss: 0.00038 | Valid Loss: 0.00038\n",
      "Epoch: 7200 | Loss: 0.00036 | Test Loss: 0.00037 | Valid Loss: 0.00037\n",
      "Epoch: 7400 | Loss: 0.00035 | Test Loss: 0.00035 | Valid Loss: 0.00036\n",
      "Epoch: 7600 | Loss: 0.00033 | Test Loss: 0.00034 | Valid Loss: 0.00034\n",
      "Epoch: 7800 | Loss: 0.00032 | Test Loss: 0.00033 | Valid Loss: 0.00033\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 0.81726 | Test Loss: 0.70590 | Valid Loss: 0.69955\n",
      "Epoch: 200 | Loss: 0.00294 | Test Loss: 0.00284 | Valid Loss: 0.00286\n",
      "Epoch: 400 | Loss: 0.00208 | Test Loss: 0.00201 | Valid Loss: 0.00205\n",
      "Epoch: 600 | Loss: 0.00164 | Test Loss: 0.00158 | Valid Loss: 0.00162\n",
      "Epoch: 800 | Loss: 0.00132 | Test Loss: 0.00127 | Valid Loss: 0.00131\n",
      "Epoch: 1000 | Loss: 0.00111 | Test Loss: 0.00106 | Valid Loss: 0.00110\n",
      "Epoch: 1200 | Loss: 0.00097 | Test Loss: 0.00093 | Valid Loss: 0.00097\n",
      "Epoch: 1400 | Loss: 0.00088 | Test Loss: 0.00085 | Valid Loss: 0.00088\n",
      "Epoch: 1600 | Loss: 0.00082 | Test Loss: 0.00079 | Valid Loss: 0.00083\n",
      "Epoch: 1800 | Loss: 0.00078 | Test Loss: 0.00075 | Valid Loss: 0.00078\n",
      "Epoch: 2000 | Loss: 0.00074 | Test Loss: 0.00071 | Valid Loss: 0.00075\n",
      "Epoch: 2200 | Loss: 0.00071 | Test Loss: 0.00068 | Valid Loss: 0.00071\n",
      "Epoch: 2400 | Loss: 0.00067 | Test Loss: 0.00065 | Valid Loss: 0.00068\n",
      "Epoch: 2600 | Loss: 0.00064 | Test Loss: 0.00062 | Valid Loss: 0.00065\n",
      "Epoch: 2800 | Loss: 0.00062 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "Epoch: 3000 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00060\n",
      "Epoch: 3200 | Loss: 0.00057 | Test Loss: 0.00055 | Valid Loss: 0.00058\n",
      "Epoch: 3400 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00055\n",
      "Epoch: 3600 | Loss: 0.00052 | Test Loss: 0.00051 | Valid Loss: 0.00053\n",
      "Epoch: 3800 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 4000 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00049\n",
      "Epoch: 4200 | Loss: 0.00047 | Test Loss: 0.00045 | Valid Loss: 0.00047\n",
      "Epoch: 4400 | Loss: 0.00045 | Test Loss: 0.00043 | Valid Loss: 0.00046\n",
      "Epoch: 4600 | Loss: 0.00043 | Test Loss: 0.00042 | Valid Loss: 0.00044\n",
      "Epoch: 4800 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00043\n",
      "Epoch: 5000 | Loss: 0.00039 | Test Loss: 0.00038 | Valid Loss: 0.00040\n",
      "Epoch: 5200 | Loss: 0.00037 | Test Loss: 0.00036 | Valid Loss: 0.00038\n",
      "Epoch: 5400 | Loss: 0.00035 | Test Loss: 0.00034 | Valid Loss: 0.00036\n",
      "Epoch: 5600 | Loss: 0.00033 | Test Loss: 0.00033 | Valid Loss: 0.00034\n",
      "Epoch: 5800 | Loss: 0.00032 | Test Loss: 0.00031 | Valid Loss: 0.00033\n",
      "Epoch: 6000 | Loss: 0.00060 | Test Loss: 0.00066 | Valid Loss: 0.00069\n",
      "Epoch: 6200 | Loss: 0.00029 | Test Loss: 0.00029 | Valid Loss: 0.00030\n",
      "Epoch: 6400 | Loss: 0.00028 | Test Loss: 0.00028 | Valid Loss: 0.00029\n",
      "Epoch: 6600 | Loss: 0.00027 | Test Loss: 0.00028 | Valid Loss: 0.00029\n",
      "Epoch: 6800 | Loss: 0.00026 | Test Loss: 0.00026 | Valid Loss: 0.00027\n",
      "Early stopping at epoch: 6845\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 1.32138 | Test Loss: 1.20017 | Valid Loss: 1.19084\n",
      "Epoch: 200 | Loss: 0.00349 | Test Loss: 0.00361 | Valid Loss: 0.00343\n",
      "Epoch: 400 | Loss: 0.00232 | Test Loss: 0.00236 | Valid Loss: 0.00228\n",
      "Epoch: 600 | Loss: 0.00191 | Test Loss: 0.00193 | Valid Loss: 0.00188\n",
      "Epoch: 800 | Loss: 0.00163 | Test Loss: 0.00163 | Valid Loss: 0.00160\n",
      "Epoch: 1000 | Loss: 0.00139 | Test Loss: 0.00139 | Valid Loss: 0.00138\n",
      "Epoch: 1200 | Loss: 0.00121 | Test Loss: 0.00120 | Valid Loss: 0.00119\n",
      "Epoch: 1400 | Loss: 0.00106 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 1600 | Loss: 0.00094 | Test Loss: 0.00093 | Valid Loss: 0.00094\n",
      "Epoch: 1800 | Loss: 0.00086 | Test Loss: 0.00085 | Valid Loss: 0.00086\n",
      "Epoch: 2000 | Loss: 0.00080 | Test Loss: 0.00078 | Valid Loss: 0.00080\n",
      "Epoch: 2200 | Loss: 0.00075 | Test Loss: 0.00073 | Valid Loss: 0.00075\n",
      "Epoch: 2400 | Loss: 0.00071 | Test Loss: 0.00069 | Valid Loss: 0.00071\n",
      "Epoch: 2600 | Loss: 0.00068 | Test Loss: 0.00066 | Valid Loss: 0.00068\n",
      "Epoch: 2800 | Loss: 0.00065 | Test Loss: 0.00062 | Valid Loss: 0.00065\n",
      "Epoch: 3000 | Loss: 0.00062 | Test Loss: 0.00060 | Valid Loss: 0.00062\n",
      "Epoch: 3200 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00060\n",
      "Epoch: 3400 | Loss: 0.00057 | Test Loss: 0.00055 | Valid Loss: 0.00057\n",
      "Epoch: 3600 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00055\n",
      "Epoch: 3800 | Loss: 0.00053 | Test Loss: 0.00051 | Valid Loss: 0.00053\n",
      "Epoch: 4000 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 4200 | Loss: 0.00049 | Test Loss: 0.00047 | Valid Loss: 0.00050\n",
      "Epoch: 4400 | Loss: 0.00048 | Test Loss: 0.00046 | Valid Loss: 0.00048\n",
      "Epoch: 4600 | Loss: 0.00046 | Test Loss: 0.00044 | Valid Loss: 0.00047\n",
      "Epoch: 4800 | Loss: 0.00045 | Test Loss: 0.00043 | Valid Loss: 0.00045\n",
      "Epoch: 5000 | Loss: 0.00043 | Test Loss: 0.00041 | Valid Loss: 0.00044\n",
      "Epoch: 5200 | Loss: 0.00041 | Test Loss: 0.00040 | Valid Loss: 0.00042\n",
      "Epoch: 5400 | Loss: 0.00039 | Test Loss: 0.00038 | Valid Loss: 0.00040\n",
      "Epoch: 5600 | Loss: 0.00038 | Test Loss: 0.00037 | Valid Loss: 0.00038\n",
      "Epoch: 5800 | Loss: 0.00036 | Test Loss: 0.00035 | Valid Loss: 0.00037\n",
      "Epoch: 6000 | Loss: 0.00034 | Test Loss: 0.00033 | Valid Loss: 0.00035\n",
      "Epoch: 6200 | Loss: 0.00032 | Test Loss: 0.00031 | Valid Loss: 0.00033\n",
      "Epoch: 6400 | Loss: 0.00030 | Test Loss: 0.00030 | Valid Loss: 0.00031\n",
      "Epoch: 6600 | Loss: 0.00029 | Test Loss: 0.00028 | Valid Loss: 0.00030\n",
      "Epoch: 6800 | Loss: 0.00027 | Test Loss: 0.00027 | Valid Loss: 0.00028\n",
      "Epoch: 7000 | Loss: 0.00028 | Test Loss: 0.00027 | Valid Loss: 0.00029\n",
      "Epoch: 7200 | Loss: 0.00024 | Test Loss: 0.00024 | Valid Loss: 0.00025\n",
      "Epoch: 7400 | Loss: 0.00023 | Test Loss: 0.00023 | Valid Loss: 0.00024\n",
      "Epoch: 7600 | Loss: 0.00022 | Test Loss: 0.00022 | Valid Loss: 0.00023\n",
      "Epoch: 7800 | Loss: 0.00021 | Test Loss: 0.00022 | Valid Loss: 0.00023\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 1.09833 | Test Loss: 0.97423 | Valid Loss: 0.97649\n",
      "Epoch: 200 | Loss: 0.00288 | Test Loss: 0.00293 | Valid Loss: 0.00282\n",
      "Epoch: 400 | Loss: 0.00208 | Test Loss: 0.00216 | Valid Loss: 0.00209\n",
      "Epoch: 600 | Loss: 0.00177 | Test Loss: 0.00184 | Valid Loss: 0.00178\n",
      "Epoch: 800 | Loss: 0.00151 | Test Loss: 0.00156 | Valid Loss: 0.00152\n",
      "Epoch: 1000 | Loss: 0.00128 | Test Loss: 0.00132 | Valid Loss: 0.00129\n",
      "Epoch: 1200 | Loss: 0.00109 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 1400 | Loss: 0.00094 | Test Loss: 0.00096 | Valid Loss: 0.00095\n",
      "Epoch: 1600 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 1800 | Loss: 0.00076 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 2000 | Loss: 0.00070 | Test Loss: 0.00070 | Valid Loss: 0.00071\n",
      "Epoch: 2200 | Loss: 0.00066 | Test Loss: 0.00066 | Valid Loss: 0.00067\n",
      "Epoch: 2400 | Loss: 0.00063 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 2600 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 2800 | Loss: 0.00059 | Test Loss: 0.00058 | Valid Loss: 0.00059\n",
      "Epoch: 3000 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 3200 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00055\n",
      "Epoch: 3400 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 3600 | Loss: 0.00051 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 3800 | Loss: 0.00050 | Test Loss: 0.00048 | Valid Loss: 0.00050\n",
      "Epoch: 4000 | Loss: 0.00048 | Test Loss: 0.00046 | Valid Loss: 0.00048\n",
      "Epoch: 4200 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 4400 | Loss: 0.00044 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 4600 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 4800 | Loss: 0.00040 | Test Loss: 0.00039 | Valid Loss: 0.00040\n",
      "Epoch: 5000 | Loss: 0.00038 | Test Loss: 0.00036 | Valid Loss: 0.00038\n",
      "Epoch: 5200 | Loss: 0.00035 | Test Loss: 0.00034 | Valid Loss: 0.00036\n",
      "Epoch: 5400 | Loss: 0.00033 | Test Loss: 0.00032 | Valid Loss: 0.00033\n",
      "Epoch: 5600 | Loss: 0.00031 | Test Loss: 0.00030 | Valid Loss: 0.00031\n",
      "Epoch: 5800 | Loss: 0.00029 | Test Loss: 0.00028 | Valid Loss: 0.00029\n",
      "Epoch: 6000 | Loss: 0.00027 | Test Loss: 0.00026 | Valid Loss: 0.00028\n",
      "Epoch: 6200 | Loss: 0.00025 | Test Loss: 0.00025 | Valid Loss: 0.00026\n",
      "Epoch: 6400 | Loss: 0.00024 | Test Loss: 0.00023 | Valid Loss: 0.00025\n",
      "Epoch: 6600 | Loss: 0.00053 | Test Loss: 0.00049 | Valid Loss: 0.00051\n",
      "Epoch: 6800 | Loss: 0.00022 | Test Loss: 0.00021 | Valid Loss: 0.00022\n",
      "Epoch: 7000 | Loss: 0.00021 | Test Loss: 0.00020 | Valid Loss: 0.00021\n",
      "Epoch: 7200 | Loss: 0.00020 | Test Loss: 0.00021 | Valid Loss: 0.00022\n",
      "Epoch: 7400 | Loss: 0.00019 | Test Loss: 0.00019 | Valid Loss: 0.00019\n",
      "Epoch: 7600 | Loss: 0.00018 | Test Loss: 0.00018 | Valid Loss: 0.00019\n",
      "Early stopping at epoch: 7718\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 1.84101 | Test Loss: 1.68784 | Valid Loss: 1.66869\n",
      "Epoch: 200 | Loss: 0.00410 | Test Loss: 0.00407 | Valid Loss: 0.00405\n",
      "Epoch: 400 | Loss: 0.00270 | Test Loss: 0.00266 | Valid Loss: 0.00267\n",
      "Epoch: 600 | Loss: 0.00230 | Test Loss: 0.00226 | Valid Loss: 0.00226\n",
      "Epoch: 800 | Loss: 0.00202 | Test Loss: 0.00200 | Valid Loss: 0.00199\n",
      "Epoch: 1000 | Loss: 0.00178 | Test Loss: 0.00177 | Valid Loss: 0.00175\n",
      "Epoch: 1200 | Loss: 0.00155 | Test Loss: 0.00155 | Valid Loss: 0.00153\n",
      "Epoch: 1400 | Loss: 0.00135 | Test Loss: 0.00134 | Valid Loss: 0.00133\n",
      "Epoch: 1600 | Loss: 0.00116 | Test Loss: 0.00116 | Valid Loss: 0.00114\n",
      "Epoch: 1800 | Loss: 0.00099 | Test Loss: 0.00099 | Valid Loss: 0.00097\n",
      "Epoch: 2000 | Loss: 0.00086 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 2200 | Loss: 0.00076 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 2400 | Loss: 0.00069 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Epoch: 2600 | Loss: 0.00064 | Test Loss: 0.00063 | Valid Loss: 0.00061\n",
      "Epoch: 2800 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 3000 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 3200 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "Epoch: 3400 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00052\n",
      "Epoch: 3600 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 3800 | Loss: 0.00052 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 4000 | Loss: 0.00051 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 4200 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 4400 | Loss: 0.00048 | Test Loss: 0.00047 | Valid Loss: 0.00046\n",
      "Epoch: 4600 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00044\n",
      "Epoch: 4800 | Loss: 0.00045 | Test Loss: 0.00044 | Valid Loss: 0.00043\n",
      "Epoch: 5000 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00041\n",
      "Epoch: 5200 | Loss: 0.00042 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 5400 | Loss: 0.00040 | Test Loss: 0.00039 | Valid Loss: 0.00038\n",
      "Epoch: 5600 | Loss: 0.00038 | Test Loss: 0.00037 | Valid Loss: 0.00037\n",
      "Epoch: 5800 | Loss: 0.00036 | Test Loss: 0.00035 | Valid Loss: 0.00035\n",
      "Epoch: 6000 | Loss: 0.00034 | Test Loss: 0.00033 | Valid Loss: 0.00033\n",
      "Epoch: 6200 | Loss: 0.00032 | Test Loss: 0.00031 | Valid Loss: 0.00031\n",
      "Epoch: 6400 | Loss: 0.00030 | Test Loss: 0.00030 | Valid Loss: 0.00029\n",
      "Epoch: 6600 | Loss: 0.00033 | Test Loss: 0.00033 | Valid Loss: 0.00033\n",
      "Epoch: 6800 | Loss: 0.00029 | Test Loss: 0.00027 | Valid Loss: 0.00027\n",
      "Epoch: 7000 | Loss: 0.00026 | Test Loss: 0.00027 | Valid Loss: 0.00027\n",
      "Epoch: 7200 | Loss: 0.00023 | Test Loss: 0.00023 | Valid Loss: 0.00023\n",
      "Epoch: 7400 | Loss: 0.00022 | Test Loss: 0.00022 | Valid Loss: 0.00021\n",
      "Epoch: 7600 | Loss: 0.00021 | Test Loss: 0.00021 | Valid Loss: 0.00020\n",
      "Epoch: 7800 | Loss: 0.00020 | Test Loss: 0.00020 | Valid Loss: 0.00019\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 1.74056 | Test Loss: 1.62777 | Valid Loss: 1.60429\n",
      "Epoch: 200 | Loss: 0.00330 | Test Loss: 0.00327 | Valid Loss: 0.00316\n",
      "Epoch: 400 | Loss: 0.00239 | Test Loss: 0.00242 | Valid Loss: 0.00230\n",
      "Epoch: 600 | Loss: 0.00207 | Test Loss: 0.00211 | Valid Loss: 0.00200\n",
      "Epoch: 800 | Loss: 0.00185 | Test Loss: 0.00190 | Valid Loss: 0.00180\n",
      "Epoch: 1000 | Loss: 0.00167 | Test Loss: 0.00172 | Valid Loss: 0.00162\n",
      "Epoch: 1200 | Loss: 0.00150 | Test Loss: 0.00155 | Valid Loss: 0.00146\n",
      "Epoch: 1400 | Loss: 0.00135 | Test Loss: 0.00140 | Valid Loss: 0.00130\n",
      "Epoch: 1600 | Loss: 0.00120 | Test Loss: 0.00125 | Valid Loss: 0.00116\n",
      "Epoch: 1800 | Loss: 0.00107 | Test Loss: 0.00112 | Valid Loss: 0.00103\n",
      "Epoch: 2000 | Loss: 0.00095 | Test Loss: 0.00099 | Valid Loss: 0.00091\n",
      "Epoch: 2200 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00081\n",
      "Epoch: 2400 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00073\n",
      "Epoch: 2600 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00066\n",
      "Epoch: 2800 | Loss: 0.00065 | Test Loss: 0.00067 | Valid Loss: 0.00062\n",
      "Epoch: 3000 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00058\n",
      "Epoch: 3200 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00056\n",
      "Epoch: 3400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00054\n",
      "Epoch: 3600 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00053\n",
      "Epoch: 3800 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00051\n",
      "Epoch: 4000 | Loss: 0.00053 | Test Loss: 0.00054 | Valid Loss: 0.00050\n",
      "Epoch: 4200 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00049\n",
      "Epoch: 4400 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00048\n",
      "Epoch: 4600 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00047\n",
      "Epoch: 4800 | Loss: 0.00048 | Test Loss: 0.00049 | Valid Loss: 0.00046\n",
      "Epoch: 5000 | Loss: 0.00047 | Test Loss: 0.00048 | Valid Loss: 0.00045\n",
      "Epoch: 5200 | Loss: 0.00046 | Test Loss: 0.00047 | Valid Loss: 0.00044\n",
      "Epoch: 5400 | Loss: 0.00045 | Test Loss: 0.00046 | Valid Loss: 0.00043\n",
      "Epoch: 5600 | Loss: 0.00043 | Test Loss: 0.00044 | Valid Loss: 0.00041\n",
      "Epoch: 5800 | Loss: 0.00042 | Test Loss: 0.00043 | Valid Loss: 0.00040\n",
      "Epoch: 6000 | Loss: 0.00040 | Test Loss: 0.00041 | Valid Loss: 0.00038\n",
      "Epoch: 6200 | Loss: 0.00038 | Test Loss: 0.00039 | Valid Loss: 0.00037\n",
      "Epoch: 6400 | Loss: 0.00037 | Test Loss: 0.00037 | Valid Loss: 0.00035\n",
      "Epoch: 6600 | Loss: 0.00035 | Test Loss: 0.00035 | Valid Loss: 0.00033\n",
      "Epoch: 6800 | Loss: 0.00034 | Test Loss: 0.00035 | Valid Loss: 0.00033\n",
      "Epoch: 7000 | Loss: 0.00031 | Test Loss: 0.00032 | Valid Loss: 0.00030\n",
      "Epoch: 7200 | Loss: 0.00029 | Test Loss: 0.00030 | Valid Loss: 0.00028\n",
      "Epoch: 7400 | Loss: 0.00027 | Test Loss: 0.00028 | Valid Loss: 0.00026\n",
      "Epoch: 7600 | Loss: 0.00025 | Test Loss: 0.00026 | Valid Loss: 0.00025\n",
      "Epoch: 7800 | Loss: 0.00024 | Test Loss: 0.00024 | Valid Loss: 0.00023\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 0.89204 | Test Loss: 0.79047 | Valid Loss: 0.79103\n",
      "Epoch: 200 | Loss: 0.00296 | Test Loss: 0.00305 | Valid Loss: 0.00284\n",
      "Epoch: 400 | Loss: 0.00215 | Test Loss: 0.00220 | Valid Loss: 0.00209\n",
      "Epoch: 600 | Loss: 0.00174 | Test Loss: 0.00178 | Valid Loss: 0.00169\n",
      "Epoch: 800 | Loss: 0.00141 | Test Loss: 0.00144 | Valid Loss: 0.00137\n",
      "Epoch: 1000 | Loss: 0.00114 | Test Loss: 0.00117 | Valid Loss: 0.00111\n",
      "Epoch: 1200 | Loss: 0.00093 | Test Loss: 0.00096 | Valid Loss: 0.00092\n",
      "Epoch: 1400 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Epoch: 1600 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00069\n",
      "Epoch: 1800 | Loss: 0.00064 | Test Loss: 0.00066 | Valid Loss: 0.00064\n",
      "Epoch: 2000 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 2200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 2400 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 2600 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00053\n",
      "Epoch: 2800 | Loss: 0.00052 | Test Loss: 0.00053 | Valid Loss: 0.00051\n",
      "Epoch: 3000 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 3200 | Loss: 0.00049 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 3400 | Loss: 0.00047 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 3600 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 3800 | Loss: 0.00044 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 4000 | Loss: 0.00042 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 4200 | Loss: 0.00040 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 4400 | Loss: 0.00039 | Test Loss: 0.00039 | Valid Loss: 0.00038\n",
      "Epoch: 4600 | Loss: 0.00036 | Test Loss: 0.00037 | Valid Loss: 0.00036\n",
      "Epoch: 4800 | Loss: 0.00034 | Test Loss: 0.00035 | Valid Loss: 0.00034\n",
      "Epoch: 5000 | Loss: 0.00032 | Test Loss: 0.00033 | Valid Loss: 0.00032\n",
      "Epoch: 5200 | Loss: 0.00031 | Test Loss: 0.00032 | Valid Loss: 0.00031\n",
      "Epoch: 5400 | Loss: 0.00029 | Test Loss: 0.00030 | Valid Loss: 0.00029\n",
      "Epoch: 5600 | Loss: 0.00027 | Test Loss: 0.00028 | Valid Loss: 0.00027\n",
      "Epoch: 5800 | Loss: 0.00026 | Test Loss: 0.00027 | Valid Loss: 0.00026\n",
      "Epoch: 6000 | Loss: 0.00024 | Test Loss: 0.00025 | Valid Loss: 0.00025\n",
      "Epoch: 6200 | Loss: 0.00023 | Test Loss: 0.00024 | Valid Loss: 0.00023\n",
      "Epoch: 6400 | Loss: 0.00022 | Test Loss: 0.00023 | Valid Loss: 0.00022\n",
      "Early stopping at epoch: 6406\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.68102 | Test Loss: 1.53875 | Valid Loss: 1.53408\n",
      "Epoch: 200 | Loss: 0.00330 | Test Loss: 0.00332 | Valid Loss: 0.00328\n",
      "Epoch: 400 | Loss: 0.00236 | Test Loss: 0.00235 | Valid Loss: 0.00238\n",
      "Epoch: 600 | Loss: 0.00202 | Test Loss: 0.00202 | Valid Loss: 0.00205\n",
      "Epoch: 800 | Loss: 0.00177 | Test Loss: 0.00177 | Valid Loss: 0.00179\n",
      "Epoch: 1000 | Loss: 0.00154 | Test Loss: 0.00154 | Valid Loss: 0.00156\n",
      "Epoch: 1200 | Loss: 0.00133 | Test Loss: 0.00134 | Valid Loss: 0.00134\n",
      "Epoch: 1400 | Loss: 0.00114 | Test Loss: 0.00116 | Valid Loss: 0.00115\n",
      "Epoch: 1600 | Loss: 0.00099 | Test Loss: 0.00100 | Valid Loss: 0.00100\n",
      "Epoch: 1800 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 2000 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 2200 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 2400 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 2600 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 2800 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 3000 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 3200 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 3400 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 3600 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00053\n",
      "Epoch: 3800 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 4000 | Loss: 0.00052 | Test Loss: 0.00052 | Valid Loss: 0.00051\n",
      "Epoch: 4200 | Loss: 0.00050 | Test Loss: 0.00051 | Valid Loss: 0.00049\n",
      "Epoch: 4400 | Loss: 0.00049 | Test Loss: 0.00049 | Valid Loss: 0.00048\n",
      "Epoch: 4600 | Loss: 0.00048 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 4800 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 5000 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 5200 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 5400 | Loss: 0.00041 | Test Loss: 0.00042 | Valid Loss: 0.00041\n",
      "Epoch: 5600 | Loss: 0.00039 | Test Loss: 0.00040 | Valid Loss: 0.00039\n",
      "Epoch: 5800 | Loss: 0.00038 | Test Loss: 0.00038 | Valid Loss: 0.00037\n",
      "Epoch: 6000 | Loss: 0.00036 | Test Loss: 0.00037 | Valid Loss: 0.00036\n",
      "Epoch: 6200 | Loss: 0.00034 | Test Loss: 0.00034 | Valid Loss: 0.00033\n",
      "Epoch: 6400 | Loss: 0.00032 | Test Loss: 0.00032 | Valid Loss: 0.00032\n",
      "Epoch: 6600 | Loss: 0.00030 | Test Loss: 0.00030 | Valid Loss: 0.00030\n",
      "Epoch: 6800 | Loss: 0.00028 | Test Loss: 0.00029 | Valid Loss: 0.00028\n",
      "Epoch: 7000 | Loss: 0.00026 | Test Loss: 0.00027 | Valid Loss: 0.00026\n",
      "Epoch: 7200 | Loss: 0.00025 | Test Loss: 0.00025 | Valid Loss: 0.00025\n",
      "Epoch: 7400 | Loss: 0.00023 | Test Loss: 0.00024 | Valid Loss: 0.00023\n",
      "Epoch: 7600 | Loss: 0.00022 | Test Loss: 0.00023 | Valid Loss: 0.00022\n",
      "Early stopping at epoch: 7779\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 2.59099 | Test Loss: 2.38823 | Valid Loss: 2.40922\n",
      "Epoch: 200 | Loss: 0.00397 | Test Loss: 0.00403 | Valid Loss: 0.00396\n",
      "Epoch: 400 | Loss: 0.00292 | Test Loss: 0.00293 | Valid Loss: 0.00293\n",
      "Epoch: 600 | Loss: 0.00250 | Test Loss: 0.00251 | Valid Loss: 0.00252\n",
      "Epoch: 800 | Loss: 0.00222 | Test Loss: 0.00223 | Valid Loss: 0.00224\n",
      "Epoch: 1000 | Loss: 0.00199 | Test Loss: 0.00200 | Valid Loss: 0.00200\n",
      "Epoch: 1200 | Loss: 0.00178 | Test Loss: 0.00179 | Valid Loss: 0.00179\n",
      "Epoch: 1400 | Loss: 0.00158 | Test Loss: 0.00159 | Valid Loss: 0.00159\n",
      "Epoch: 1600 | Loss: 0.00139 | Test Loss: 0.00139 | Valid Loss: 0.00139\n",
      "Epoch: 1800 | Loss: 0.00121 | Test Loss: 0.00121 | Valid Loss: 0.00120\n",
      "Epoch: 2000 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00104\n",
      "Epoch: 2200 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00090\n",
      "Epoch: 2400 | Loss: 0.00081 | Test Loss: 0.00080 | Valid Loss: 0.00080\n",
      "Epoch: 2600 | Loss: 0.00074 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 2800 | Loss: 0.00069 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 3000 | Loss: 0.00065 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 3200 | Loss: 0.00063 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 3400 | Loss: 0.00061 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 3600 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 3800 | Loss: 0.00057 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 4000 | Loss: 0.00055 | Test Loss: 0.00054 | Valid Loss: 0.00054\n",
      "Epoch: 4200 | Loss: 0.00054 | Test Loss: 0.00052 | Valid Loss: 0.00053\n",
      "Epoch: 4400 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00052\n",
      "Epoch: 4600 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Epoch: 4800 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00049\n",
      "Epoch: 5000 | Loss: 0.00048 | Test Loss: 0.00046 | Valid Loss: 0.00047\n",
      "Epoch: 5200 | Loss: 0.00046 | Test Loss: 0.00044 | Valid Loss: 0.00046\n",
      "Epoch: 5400 | Loss: 0.00044 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 5600 | Loss: 0.00043 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 5800 | Loss: 0.00041 | Test Loss: 0.00039 | Valid Loss: 0.00041\n",
      "Epoch: 6000 | Loss: 0.00039 | Test Loss: 0.00037 | Valid Loss: 0.00039\n",
      "Epoch: 6200 | Loss: 0.00037 | Test Loss: 0.00035 | Valid Loss: 0.00037\n",
      "Epoch: 6400 | Loss: 0.00034 | Test Loss: 0.00033 | Valid Loss: 0.00035\n",
      "Epoch: 6600 | Loss: 0.00036 | Test Loss: 0.00034 | Valid Loss: 0.00035\n",
      "Epoch: 6800 | Loss: 0.00030 | Test Loss: 0.00029 | Valid Loss: 0.00031\n",
      "Epoch: 7000 | Loss: 0.00028 | Test Loss: 0.00027 | Valid Loss: 0.00029\n",
      "Epoch: 7200 | Loss: 0.00026 | Test Loss: 0.00026 | Valid Loss: 0.00027\n",
      "Epoch: 7400 | Loss: 0.00024 | Test Loss: 0.00024 | Valid Loss: 0.00025\n",
      "Epoch: 7600 | Loss: 0.00023 | Test Loss: 0.00023 | Valid Loss: 0.00024\n",
      "Epoch: 7800 | Loss: 0.00022 | Test Loss: 0.00021 | Valid Loss: 0.00023\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 1.76348 | Test Loss: 1.57281 | Valid Loss: 1.59032\n",
      "Epoch: 200 | Loss: 0.00347 | Test Loss: 0.00365 | Valid Loss: 0.00350\n",
      "Epoch: 400 | Loss: 0.00242 | Test Loss: 0.00251 | Valid Loss: 0.00248\n",
      "Epoch: 600 | Loss: 0.00204 | Test Loss: 0.00209 | Valid Loss: 0.00210\n",
      "Epoch: 800 | Loss: 0.00176 | Test Loss: 0.00180 | Valid Loss: 0.00182\n",
      "Epoch: 1000 | Loss: 0.00152 | Test Loss: 0.00156 | Valid Loss: 0.00158\n",
      "Epoch: 1200 | Loss: 0.00132 | Test Loss: 0.00135 | Valid Loss: 0.00136\n",
      "Epoch: 1400 | Loss: 0.00115 | Test Loss: 0.00117 | Valid Loss: 0.00118\n",
      "Epoch: 1600 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 1800 | Loss: 0.00089 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 2000 | Loss: 0.00081 | Test Loss: 0.00081 | Valid Loss: 0.00081\n",
      "Epoch: 2200 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 2400 | Loss: 0.00070 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 2600 | Loss: 0.00066 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 2800 | Loss: 0.00064 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 3000 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 3200 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 3400 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 3600 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 3800 | Loss: 0.00055 | Test Loss: 0.00053 | Valid Loss: 0.00054\n",
      "Epoch: 4000 | Loss: 0.00053 | Test Loss: 0.00052 | Valid Loss: 0.00052\n",
      "Epoch: 4200 | Loss: 0.00052 | Test Loss: 0.00050 | Valid Loss: 0.00051\n",
      "Epoch: 4400 | Loss: 0.00051 | Test Loss: 0.00049 | Valid Loss: 0.00050\n",
      "Epoch: 4600 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00048\n",
      "Epoch: 4800 | Loss: 0.00048 | Test Loss: 0.00046 | Valid Loss: 0.00047\n",
      "Epoch: 5000 | Loss: 0.00046 | Test Loss: 0.00045 | Valid Loss: 0.00046\n",
      "Epoch: 5200 | Loss: 0.00045 | Test Loss: 0.00043 | Valid Loss: 0.00044\n",
      "Epoch: 5400 | Loss: 0.00043 | Test Loss: 0.00041 | Valid Loss: 0.00042\n",
      "Epoch: 5600 | Loss: 0.00041 | Test Loss: 0.00040 | Valid Loss: 0.00041\n",
      "Epoch: 5800 | Loss: 0.00039 | Test Loss: 0.00038 | Valid Loss: 0.00039\n",
      "Epoch: 6000 | Loss: 0.00037 | Test Loss: 0.00036 | Valid Loss: 0.00037\n",
      "Epoch: 6200 | Loss: 0.00035 | Test Loss: 0.00034 | Valid Loss: 0.00035\n",
      "Epoch: 6400 | Loss: 0.00033 | Test Loss: 0.00032 | Valid Loss: 0.00033\n",
      "Epoch: 6600 | Loss: 0.00031 | Test Loss: 0.00030 | Valid Loss: 0.00031\n",
      "Epoch: 6800 | Loss: 0.00029 | Test Loss: 0.00028 | Valid Loss: 0.00029\n",
      "Epoch: 7000 | Loss: 0.00027 | Test Loss: 0.00027 | Valid Loss: 0.00028\n",
      "Epoch: 7200 | Loss: 0.00026 | Test Loss: 0.00025 | Valid Loss: 0.00026\n",
      "Epoch: 7400 | Loss: 0.00036 | Test Loss: 0.00025 | Valid Loss: 0.00027\n",
      "Epoch: 7600 | Loss: 0.00023 | Test Loss: 0.00023 | Valid Loss: 0.00023\n",
      "Epoch: 7800 | Loss: 0.00022 | Test Loss: 0.00021 | Valid Loss: 0.00022\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 2.79220 | Test Loss: 2.58466 | Valid Loss: 2.59940\n",
      "Epoch: 200 | Loss: 0.00494 | Test Loss: 0.00496 | Valid Loss: 0.00490\n",
      "Epoch: 400 | Loss: 0.00295 | Test Loss: 0.00291 | Valid Loss: 0.00296\n",
      "Epoch: 600 | Loss: 0.00246 | Test Loss: 0.00243 | Valid Loss: 0.00246\n",
      "Epoch: 800 | Loss: 0.00217 | Test Loss: 0.00215 | Valid Loss: 0.00217\n",
      "Epoch: 1000 | Loss: 0.00192 | Test Loss: 0.00191 | Valid Loss: 0.00192\n",
      "Epoch: 1200 | Loss: 0.00170 | Test Loss: 0.00168 | Valid Loss: 0.00169\n",
      "Epoch: 1400 | Loss: 0.00149 | Test Loss: 0.00148 | Valid Loss: 0.00148\n",
      "Epoch: 1600 | Loss: 0.00130 | Test Loss: 0.00129 | Valid Loss: 0.00129\n",
      "Epoch: 1800 | Loss: 0.00114 | Test Loss: 0.00113 | Valid Loss: 0.00113\n",
      "Epoch: 2000 | Loss: 0.00101 | Test Loss: 0.00099 | Valid Loss: 0.00099\n",
      "Epoch: 2200 | Loss: 0.00091 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 2400 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 2600 | Loss: 0.00078 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 2800 | Loss: 0.00074 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 3000 | Loss: 0.00070 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 3200 | Loss: 0.00067 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00064 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 3600 | Loss: 0.00062 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 3800 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4000 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 4200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00054\n",
      "Epoch: 4400 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 4600 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00050\n",
      "Epoch: 4800 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 5000 | Loss: 0.00048 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 5200 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00045\n",
      "Epoch: 5400 | Loss: 0.00045 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 5600 | Loss: 0.00043 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 5800 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00040\n",
      "Epoch: 6000 | Loss: 0.00039 | Test Loss: 0.00039 | Valid Loss: 0.00038\n",
      "Epoch: 6200 | Loss: 0.00037 | Test Loss: 0.00037 | Valid Loss: 0.00036\n",
      "Epoch: 6400 | Loss: 0.00035 | Test Loss: 0.00035 | Valid Loss: 0.00034\n",
      "Epoch: 6600 | Loss: 0.00033 | Test Loss: 0.00033 | Valid Loss: 0.00032\n",
      "Epoch: 6800 | Loss: 0.00031 | Test Loss: 0.00031 | Valid Loss: 0.00030\n",
      "Epoch: 7000 | Loss: 0.00029 | Test Loss: 0.00029 | Valid Loss: 0.00029\n",
      "Epoch: 7200 | Loss: 0.00027 | Test Loss: 0.00028 | Valid Loss: 0.00027\n",
      "Epoch: 7400 | Loss: 0.00026 | Test Loss: 0.00026 | Valid Loss: 0.00025\n",
      "Epoch: 7600 | Loss: 0.00024 | Test Loss: 0.00025 | Valid Loss: 0.00024\n",
      "Epoch: 7800 | Loss: 0.00023 | Test Loss: 0.00024 | Valid Loss: 0.00023\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 1.70738 | Test Loss: 1.56939 | Valid Loss: 1.57321\n",
      "Epoch: 200 | Loss: 0.00385 | Test Loss: 0.00386 | Valid Loss: 0.00384\n",
      "Epoch: 400 | Loss: 0.00247 | Test Loss: 0.00245 | Valid Loss: 0.00248\n",
      "Epoch: 600 | Loss: 0.00205 | Test Loss: 0.00204 | Valid Loss: 0.00207\n",
      "Epoch: 800 | Loss: 0.00178 | Test Loss: 0.00176 | Valid Loss: 0.00179\n",
      "Epoch: 1000 | Loss: 0.00155 | Test Loss: 0.00154 | Valid Loss: 0.00156\n",
      "Epoch: 1200 | Loss: 0.00135 | Test Loss: 0.00134 | Valid Loss: 0.00135\n",
      "Epoch: 1400 | Loss: 0.00118 | Test Loss: 0.00117 | Valid Loss: 0.00117\n",
      "Epoch: 1600 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00103\n",
      "Epoch: 1800 | Loss: 0.00094 | Test Loss: 0.00094 | Valid Loss: 0.00093\n",
      "Epoch: 2000 | Loss: 0.00087 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 2200 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 2400 | Loss: 0.00078 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 2600 | Loss: 0.00075 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 2800 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00070\n",
      "Epoch: 3000 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 3200 | Loss: 0.00066 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 3600 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 3800 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4000 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00056\n",
      "Epoch: 4200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00054\n",
      "Epoch: 4400 | Loss: 0.00053 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 4600 | Loss: 0.00051 | Test Loss: 0.00051 | Valid Loss: 0.00051\n",
      "Epoch: 4800 | Loss: 0.00050 | Test Loss: 0.00050 | Valid Loss: 0.00049\n",
      "Epoch: 5000 | Loss: 0.00048 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 5200 | Loss: 0.00046 | Test Loss: 0.00046 | Valid Loss: 0.00046\n",
      "Epoch: 5400 | Loss: 0.00044 | Test Loss: 0.00045 | Valid Loss: 0.00044\n",
      "Epoch: 5600 | Loss: 0.00042 | Test Loss: 0.00043 | Valid Loss: 0.00042\n",
      "Epoch: 5800 | Loss: 0.00041 | Test Loss: 0.00041 | Valid Loss: 0.00041\n",
      "Epoch: 6000 | Loss: 0.00039 | Test Loss: 0.00039 | Valid Loss: 0.00039\n",
      "Epoch: 6200 | Loss: 0.00037 | Test Loss: 0.00037 | Valid Loss: 0.00037\n",
      "Epoch: 6400 | Loss: 0.00035 | Test Loss: 0.00036 | Valid Loss: 0.00036\n",
      "Epoch: 6600 | Loss: 0.00034 | Test Loss: 0.00034 | Valid Loss: 0.00034\n",
      "Epoch: 6800 | Loss: 0.00032 | Test Loss: 0.00032 | Valid Loss: 0.00032\n",
      "Epoch: 7000 | Loss: 0.00030 | Test Loss: 0.00031 | Valid Loss: 0.00031\n",
      "Epoch: 7200 | Loss: 0.00029 | Test Loss: 0.00030 | Valid Loss: 0.00030\n",
      "Epoch: 7400 | Loss: 0.00028 | Test Loss: 0.00028 | Valid Loss: 0.00028\n",
      "Epoch: 7600 | Loss: 0.00027 | Test Loss: 0.00027 | Valid Loss: 0.00027\n",
      "Epoch: 7800 | Loss: 0.00026 | Test Loss: 0.00026 | Valid Loss: 0.00026\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 0.64703 | Test Loss: 0.54564 | Valid Loss: 0.53921\n",
      "Epoch: 200 | Loss: 0.00265 | Test Loss: 0.00253 | Valid Loss: 0.00265\n",
      "Epoch: 400 | Loss: 0.00192 | Test Loss: 0.00187 | Valid Loss: 0.00192\n",
      "Epoch: 600 | Loss: 0.00152 | Test Loss: 0.00149 | Valid Loss: 0.00151\n",
      "Epoch: 800 | Loss: 0.00123 | Test Loss: 0.00122 | Valid Loss: 0.00122\n",
      "Epoch: 1000 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00103\n",
      "Epoch: 1200 | Loss: 0.00093 | Test Loss: 0.00094 | Valid Loss: 0.00091\n",
      "Epoch: 1400 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00083\n",
      "Epoch: 1600 | Loss: 0.00080 | Test Loss: 0.00082 | Valid Loss: 0.00078\n",
      "Epoch: 1800 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00074\n",
      "Epoch: 2000 | Loss: 0.00072 | Test Loss: 0.00075 | Valid Loss: 0.00070\n",
      "Epoch: 2200 | Loss: 0.00069 | Test Loss: 0.00072 | Valid Loss: 0.00067\n",
      "Epoch: 2400 | Loss: 0.00067 | Test Loss: 0.00070 | Valid Loss: 0.00065\n",
      "Epoch: 2600 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00063\n",
      "Epoch: 2800 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00061\n",
      "Epoch: 3000 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00059\n",
      "Epoch: 3200 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00058\n",
      "Epoch: 3400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00056\n",
      "Epoch: 3600 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00055\n",
      "Epoch: 3800 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00053\n",
      "Epoch: 4000 | Loss: 0.00053 | Test Loss: 0.00056 | Valid Loss: 0.00052\n",
      "Epoch: 4200 | Loss: 0.00051 | Test Loss: 0.00054 | Valid Loss: 0.00050\n",
      "Epoch: 4400 | Loss: 0.00050 | Test Loss: 0.00052 | Valid Loss: 0.00049\n",
      "Epoch: 4600 | Loss: 0.00048 | Test Loss: 0.00051 | Valid Loss: 0.00047\n",
      "Epoch: 4800 | Loss: 0.00047 | Test Loss: 0.00049 | Valid Loss: 0.00046\n",
      "Epoch: 5000 | Loss: 0.00050 | Test Loss: 0.00047 | Valid Loss: 0.00044\n",
      "Epoch: 5200 | Loss: 0.00043 | Test Loss: 0.00046 | Valid Loss: 0.00043\n",
      "Epoch: 5400 | Loss: 0.00042 | Test Loss: 0.00044 | Valid Loss: 0.00041\n",
      "Epoch: 5600 | Loss: 0.00040 | Test Loss: 0.00043 | Valid Loss: 0.00040\n",
      "Epoch: 5800 | Loss: 0.00040 | Test Loss: 0.00042 | Valid Loss: 0.00039\n",
      "Epoch: 6000 | Loss: 0.00038 | Test Loss: 0.00040 | Valid Loss: 0.00038\n",
      "Epoch: 6200 | Loss: 0.00037 | Test Loss: 0.00039 | Valid Loss: 0.00036\n",
      "Epoch: 6400 | Loss: 0.00035 | Test Loss: 0.00038 | Valid Loss: 0.00035\n",
      "Epoch: 6600 | Loss: 0.00034 | Test Loss: 0.00037 | Valid Loss: 0.00035\n",
      "Epoch: 6800 | Loss: 0.00033 | Test Loss: 0.00035 | Valid Loss: 0.00033\n",
      "Epoch: 7000 | Loss: 0.00032 | Test Loss: 0.00034 | Valid Loss: 0.00032\n",
      "Early stopping at epoch: 7115\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.15843 | Test Loss: 1.02603 | Valid Loss: 1.03354\n",
      "Epoch: 200 | Loss: 0.00306 | Test Loss: 0.00307 | Valid Loss: 0.00298\n",
      "Epoch: 400 | Loss: 0.00241 | Test Loss: 0.00243 | Valid Loss: 0.00237\n",
      "Epoch: 600 | Loss: 0.00209 | Test Loss: 0.00212 | Valid Loss: 0.00207\n",
      "Epoch: 800 | Loss: 0.00187 | Test Loss: 0.00191 | Valid Loss: 0.00186\n",
      "Epoch: 1000 | Loss: 0.00172 | Test Loss: 0.00175 | Valid Loss: 0.00170\n",
      "Epoch: 1200 | Loss: 0.00160 | Test Loss: 0.00164 | Valid Loss: 0.00159\n",
      "Epoch: 1400 | Loss: 0.00151 | Test Loss: 0.00155 | Valid Loss: 0.00151\n",
      "Epoch: 1600 | Loss: 0.00143 | Test Loss: 0.00148 | Valid Loss: 0.00144\n",
      "Epoch: 1800 | Loss: 0.00137 | Test Loss: 0.00142 | Valid Loss: 0.00139\n",
      "Epoch: 2000 | Loss: 0.00132 | Test Loss: 0.00137 | Valid Loss: 0.00134\n",
      "Epoch: 2200 | Loss: 0.00127 | Test Loss: 0.00132 | Valid Loss: 0.00129\n",
      "Epoch: 2400 | Loss: 0.00122 | Test Loss: 0.00127 | Valid Loss: 0.00124\n",
      "Epoch: 2600 | Loss: 0.00117 | Test Loss: 0.00122 | Valid Loss: 0.00119\n",
      "Epoch: 2800 | Loss: 0.00112 | Test Loss: 0.00117 | Valid Loss: 0.00114\n",
      "Epoch: 3000 | Loss: 0.00107 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 3200 | Loss: 0.00102 | Test Loss: 0.00107 | Valid Loss: 0.00105\n",
      "Epoch: 3400 | Loss: 0.00097 | Test Loss: 0.00102 | Valid Loss: 0.00100\n",
      "Epoch: 3600 | Loss: 0.00093 | Test Loss: 0.00097 | Valid Loss: 0.00096\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00093 | Valid Loss: 0.00091\n",
      "Epoch: 4000 | Loss: 0.00085 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 4200 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 4400 | Loss: 0.00077 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 4600 | Loss: 0.00073 | Test Loss: 0.00076 | Valid Loss: 0.00075\n",
      "Epoch: 4800 | Loss: 0.00069 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5000 | Loss: 0.00065 | Test Loss: 0.00069 | Valid Loss: 0.00067\n",
      "Epoch: 5200 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 5400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 5600 | Loss: 0.00054 | Test Loss: 0.00056 | Valid Loss: 0.00055\n",
      "Epoch: 5800 | Loss: 0.00051 | Test Loss: 0.00053 | Valid Loss: 0.00052\n",
      "Epoch: 6000 | Loss: 0.00047 | Test Loss: 0.00050 | Valid Loss: 0.00048\n",
      "Epoch: 6200 | Loss: 0.00049 | Test Loss: 0.00048 | Valid Loss: 0.00047\n",
      "Epoch: 6400 | Loss: 0.00042 | Test Loss: 0.00044 | Valid Loss: 0.00043\n",
      "Epoch: 6600 | Loss: 0.00040 | Test Loss: 0.00042 | Valid Loss: 0.00041\n",
      "Epoch: 6800 | Loss: 0.00038 | Test Loss: 0.00040 | Valid Loss: 0.00039\n",
      "Epoch: 7000 | Loss: 0.00036 | Test Loss: 0.00038 | Valid Loss: 0.00037\n",
      "Epoch: 7200 | Loss: 0.00034 | Test Loss: 0.00036 | Valid Loss: 0.00035\n",
      "Early stopping at epoch: 7260\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 8000\n",
    "learning_rate = 0.001\n",
    "momentum = 1.0\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_M_MT_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_M_MT_train).squeeze()\n",
    "    \n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_M_MT_valid).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_M_MT_test).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f58b1ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 3.6485 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_M_MT_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_M_MT_test).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_M_MT_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_M_MT_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3192920c",
   "metadata": {},
   "source": [
    "# Stacking - G + M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e37957a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim_1 = 31\n",
    "input_dim_2 = 31\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "num_methods = 2\n",
    "\n",
    "meta_input_dim = 10\n",
    "\n",
    "num_models = 15\n",
    "G_M_stacking_models = []\n",
    "    \n",
    "for i in range(num_models):\n",
    "    G_M_stacking_models.append(Initial_Model_V0(meta_input_dim, hidden_units, output_dim).to(device))\n",
    "    \n",
    "next(G_M_stacking_models[1].parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7770e6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 31])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_G_train, y_train = X_G_train.to(device), y_train.to(device)\n",
    "X_G_test, y_test = X_G_test.to(device), y_test.to(device)\n",
    "X_G_valid, y_valid = X_G_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_M_train = X_M_train.to(device)\n",
    "X_M_test = X_M_test.to(device)\n",
    "X_M_valid = X_M_valid.to(device)\n",
    "\n",
    "X_G_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f714b6f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-6853472df699>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnum_in_set\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_NN_in_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G_models' is not defined"
     ]
    }
   ],
   "source": [
    "num_NN_in_set = 5\n",
    "\n",
    "# Training\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_train)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_train)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "\n",
    "X_G_M_stack_train = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd03c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_valid)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_valid)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "\n",
    "X_G_M_stack_valid = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_test)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_test)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "\n",
    "X_G_M_stack_test = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2)), 2)\n",
    "\n",
    "X_G_M_stack_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2b8e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 2.12102 | Test Loss: 2.05732 | Valid Loss: 2.03532\n",
      "Epoch: 200 | Loss: 0.00748 | Test Loss: 0.00759 | Valid Loss: 0.00744\n",
      "Epoch: 400 | Loss: 0.00704 | Test Loss: 0.00714 | Valid Loss: 0.00700\n",
      "Epoch: 600 | Loss: 0.00650 | Test Loss: 0.00659 | Valid Loss: 0.00645\n",
      "Epoch: 800 | Loss: 0.00591 | Test Loss: 0.00600 | Valid Loss: 0.00586\n",
      "Epoch: 1000 | Loss: 0.00531 | Test Loss: 0.00539 | Valid Loss: 0.00526\n",
      "Epoch: 1200 | Loss: 0.00471 | Test Loss: 0.00478 | Valid Loss: 0.00466\n",
      "Epoch: 1400 | Loss: 0.00415 | Test Loss: 0.00421 | Valid Loss: 0.00410\n",
      "Epoch: 1600 | Loss: 0.00362 | Test Loss: 0.00368 | Valid Loss: 0.00357\n",
      "Epoch: 1800 | Loss: 0.00314 | Test Loss: 0.00319 | Valid Loss: 0.00309\n",
      "Epoch: 2000 | Loss: 0.00272 | Test Loss: 0.00276 | Valid Loss: 0.00267\n",
      "Epoch: 2200 | Loss: 0.00235 | Test Loss: 0.00239 | Valid Loss: 0.00231\n",
      "Epoch: 2400 | Loss: 0.00205 | Test Loss: 0.00209 | Valid Loss: 0.00200\n",
      "Epoch: 2600 | Loss: 0.00181 | Test Loss: 0.00184 | Valid Loss: 0.00176\n",
      "Epoch: 2800 | Loss: 0.00162 | Test Loss: 0.00165 | Valid Loss: 0.00158\n",
      "Epoch: 3000 | Loss: 0.00148 | Test Loss: 0.00152 | Valid Loss: 0.00145\n",
      "Epoch: 3200 | Loss: 0.00139 | Test Loss: 0.00142 | Valid Loss: 0.00135\n",
      "Epoch: 3400 | Loss: 0.00133 | Test Loss: 0.00136 | Valid Loss: 0.00130\n",
      "Epoch: 3600 | Loss: 0.00129 | Test Loss: 0.00132 | Valid Loss: 0.00126\n",
      "Epoch: 3800 | Loss: 0.00127 | Test Loss: 0.00130 | Valid Loss: 0.00124\n",
      "Epoch: 4000 | Loss: 0.00126 | Test Loss: 0.00129 | Valid Loss: 0.00123\n",
      "Epoch: 4200 | Loss: 0.00125 | Test Loss: 0.00128 | Valid Loss: 0.00122\n",
      "Epoch: 4400 | Loss: 0.00124 | Test Loss: 0.00127 | Valid Loss: 0.00122\n",
      "Epoch: 4600 | Loss: 0.00124 | Test Loss: 0.00127 | Valid Loss: 0.00121\n",
      "Epoch: 4800 | Loss: 0.00123 | Test Loss: 0.00126 | Valid Loss: 0.00121\n",
      "Epoch: 5000 | Loss: 0.00123 | Test Loss: 0.00126 | Valid Loss: 0.00121\n",
      "Epoch: 5200 | Loss: 0.00122 | Test Loss: 0.00126 | Valid Loss: 0.00120\n",
      "Epoch: 5400 | Loss: 0.00122 | Test Loss: 0.00125 | Valid Loss: 0.00120\n",
      "Epoch: 5600 | Loss: 0.00121 | Test Loss: 0.00125 | Valid Loss: 0.00120\n",
      "Epoch: 5800 | Loss: 0.00121 | Test Loss: 0.00124 | Valid Loss: 0.00119\n",
      "Epoch: 6000 | Loss: 0.00121 | Test Loss: 0.00124 | Valid Loss: 0.00119\n",
      "Epoch: 6200 | Loss: 0.00120 | Test Loss: 0.00123 | Valid Loss: 0.00118\n",
      "Epoch: 6400 | Loss: 0.00120 | Test Loss: 0.00123 | Valid Loss: 0.00118\n",
      "Epoch: 6600 | Loss: 0.00119 | Test Loss: 0.00122 | Valid Loss: 0.00118\n",
      "Epoch: 6800 | Loss: 0.00119 | Test Loss: 0.00122 | Valid Loss: 0.00118\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.48271 | Test Loss: 1.42525 | Valid Loss: 1.40453\n",
      "Epoch: 200 | Loss: 0.01019 | Test Loss: 0.01056 | Valid Loss: 0.01008\n",
      "Epoch: 400 | Loss: 0.00944 | Test Loss: 0.00978 | Valid Loss: 0.00934\n",
      "Epoch: 600 | Loss: 0.00854 | Test Loss: 0.00884 | Valid Loss: 0.00844\n",
      "Epoch: 800 | Loss: 0.00757 | Test Loss: 0.00784 | Valid Loss: 0.00748\n",
      "Epoch: 1000 | Loss: 0.00662 | Test Loss: 0.00685 | Valid Loss: 0.00653\n",
      "Epoch: 1200 | Loss: 0.00572 | Test Loss: 0.00591 | Valid Loss: 0.00564\n",
      "Epoch: 1400 | Loss: 0.00490 | Test Loss: 0.00507 | Valid Loss: 0.00483\n",
      "Epoch: 1600 | Loss: 0.00418 | Test Loss: 0.00432 | Valid Loss: 0.00412\n",
      "Epoch: 1800 | Loss: 0.00356 | Test Loss: 0.00367 | Valid Loss: 0.00351\n",
      "Epoch: 2000 | Loss: 0.00302 | Test Loss: 0.00312 | Valid Loss: 0.00298\n",
      "Epoch: 2200 | Loss: 0.00257 | Test Loss: 0.00264 | Valid Loss: 0.00253\n",
      "Epoch: 2400 | Loss: 0.00219 | Test Loss: 0.00225 | Valid Loss: 0.00216\n",
      "Epoch: 2600 | Loss: 0.00188 | Test Loss: 0.00193 | Valid Loss: 0.00185\n",
      "Epoch: 2800 | Loss: 0.00164 | Test Loss: 0.00168 | Valid Loss: 0.00162\n",
      "Epoch: 3000 | Loss: 0.00147 | Test Loss: 0.00150 | Valid Loss: 0.00145\n",
      "Epoch: 3200 | Loss: 0.00134 | Test Loss: 0.00137 | Valid Loss: 0.00133\n",
      "Epoch: 3400 | Loss: 0.00126 | Test Loss: 0.00128 | Valid Loss: 0.00125\n",
      "Epoch: 3600 | Loss: 0.00121 | Test Loss: 0.00123 | Valid Loss: 0.00121\n",
      "Epoch: 3800 | Loss: 0.00118 | Test Loss: 0.00120 | Valid Loss: 0.00118\n",
      "Epoch: 4000 | Loss: 0.00117 | Test Loss: 0.00119 | Valid Loss: 0.00117\n",
      "Epoch: 4200 | Loss: 0.00116 | Test Loss: 0.00118 | Valid Loss: 0.00116\n",
      "Epoch: 4400 | Loss: 0.00115 | Test Loss: 0.00117 | Valid Loss: 0.00116\n",
      "Epoch: 4600 | Loss: 0.00115 | Test Loss: 0.00116 | Valid Loss: 0.00115\n",
      "Epoch: 4800 | Loss: 0.00114 | Test Loss: 0.00116 | Valid Loss: 0.00115\n",
      "Epoch: 5000 | Loss: 0.00114 | Test Loss: 0.00115 | Valid Loss: 0.00114\n",
      "Epoch: 5200 | Loss: 0.00113 | Test Loss: 0.00115 | Valid Loss: 0.00114\n",
      "Epoch: 5400 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00113\n",
      "Epoch: 5600 | Loss: 0.00112 | Test Loss: 0.00114 | Valid Loss: 0.00113\n",
      "Epoch: 5800 | Loss: 0.00112 | Test Loss: 0.00114 | Valid Loss: 0.00113\n",
      "Epoch: 6000 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00112\n",
      "Epoch: 6200 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00112\n",
      "Epoch: 6400 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00112\n",
      "Epoch: 6600 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00112\n",
      "Epoch: 6800 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00111\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 1.30244 | Test Loss: 1.24369 | Valid Loss: 1.23495\n",
      "Epoch: 200 | Loss: 0.00893 | Test Loss: 0.00888 | Valid Loss: 0.00891\n",
      "Epoch: 400 | Loss: 0.00836 | Test Loss: 0.00830 | Valid Loss: 0.00833\n",
      "Epoch: 600 | Loss: 0.00767 | Test Loss: 0.00762 | Valid Loss: 0.00764\n",
      "Epoch: 800 | Loss: 0.00696 | Test Loss: 0.00690 | Valid Loss: 0.00692\n",
      "Epoch: 1000 | Loss: 0.00625 | Test Loss: 0.00619 | Valid Loss: 0.00621\n",
      "Epoch: 1200 | Loss: 0.00556 | Test Loss: 0.00551 | Valid Loss: 0.00552\n",
      "Epoch: 1400 | Loss: 0.00491 | Test Loss: 0.00486 | Valid Loss: 0.00487\n",
      "Epoch: 1600 | Loss: 0.00430 | Test Loss: 0.00425 | Valid Loss: 0.00426\n",
      "Epoch: 1800 | Loss: 0.00373 | Test Loss: 0.00368 | Valid Loss: 0.00369\n",
      "Epoch: 2000 | Loss: 0.00319 | Test Loss: 0.00315 | Valid Loss: 0.00316\n",
      "Epoch: 2200 | Loss: 0.00271 | Test Loss: 0.00267 | Valid Loss: 0.00267\n",
      "Epoch: 2400 | Loss: 0.00228 | Test Loss: 0.00225 | Valid Loss: 0.00224\n",
      "Epoch: 2600 | Loss: 0.00192 | Test Loss: 0.00189 | Valid Loss: 0.00189\n",
      "Epoch: 2800 | Loss: 0.00163 | Test Loss: 0.00161 | Valid Loss: 0.00160\n",
      "Epoch: 3000 | Loss: 0.00142 | Test Loss: 0.00141 | Valid Loss: 0.00139\n",
      "Epoch: 3200 | Loss: 0.00128 | Test Loss: 0.00127 | Valid Loss: 0.00125\n",
      "Epoch: 3400 | Loss: 0.00118 | Test Loss: 0.00118 | Valid Loss: 0.00116\n",
      "Epoch: 3600 | Loss: 0.00113 | Test Loss: 0.00112 | Valid Loss: 0.00111\n",
      "Epoch: 3800 | Loss: 0.00109 | Test Loss: 0.00109 | Valid Loss: 0.00107\n",
      "Epoch: 4000 | Loss: 0.00107 | Test Loss: 0.00108 | Valid Loss: 0.00106\n",
      "Epoch: 4200 | Loss: 0.00106 | Test Loss: 0.00107 | Valid Loss: 0.00105\n",
      "Epoch: 4400 | Loss: 0.00105 | Test Loss: 0.00106 | Valid Loss: 0.00104\n",
      "Epoch: 4600 | Loss: 0.00104 | Test Loss: 0.00105 | Valid Loss: 0.00103\n",
      "Epoch: 4800 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00102\n",
      "Epoch: 5000 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00102\n",
      "Epoch: 5200 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00101\n",
      "Epoch: 5400 | Loss: 0.00101 | Test Loss: 0.00101 | Valid Loss: 0.00100\n",
      "Epoch: 5600 | Loss: 0.00100 | Test Loss: 0.00101 | Valid Loss: 0.00099\n",
      "Epoch: 5800 | Loss: 0.00100 | Test Loss: 0.00100 | Valid Loss: 0.00099\n",
      "Epoch: 6000 | Loss: 0.00099 | Test Loss: 0.00099 | Valid Loss: 0.00098\n",
      "Epoch: 6200 | Loss: 0.00098 | Test Loss: 0.00098 | Valid Loss: 0.00097\n",
      "Epoch: 6400 | Loss: 0.00098 | Test Loss: 0.00097 | Valid Loss: 0.00097\n",
      "Epoch: 6600 | Loss: 0.00097 | Test Loss: 0.00097 | Valid Loss: 0.00096\n",
      "Epoch: 6800 | Loss: 0.00097 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 0.81218 | Test Loss: 0.76830 | Valid Loss: 0.76597\n",
      "Epoch: 200 | Loss: 0.00859 | Test Loss: 0.00883 | Valid Loss: 0.00864\n",
      "Epoch: 400 | Loss: 0.00770 | Test Loss: 0.00790 | Valid Loss: 0.00774\n",
      "Epoch: 600 | Loss: 0.00669 | Test Loss: 0.00686 | Valid Loss: 0.00672\n",
      "Epoch: 800 | Loss: 0.00569 | Test Loss: 0.00583 | Valid Loss: 0.00571\n",
      "Epoch: 1000 | Loss: 0.00477 | Test Loss: 0.00487 | Valid Loss: 0.00478\n",
      "Epoch: 1200 | Loss: 0.00395 | Test Loss: 0.00403 | Valid Loss: 0.00395\n",
      "Epoch: 1400 | Loss: 0.00325 | Test Loss: 0.00331 | Valid Loss: 0.00325\n",
      "Epoch: 1600 | Loss: 0.00267 | Test Loss: 0.00271 | Valid Loss: 0.00266\n",
      "Epoch: 1800 | Loss: 0.00219 | Test Loss: 0.00221 | Valid Loss: 0.00218\n",
      "Epoch: 2000 | Loss: 0.00180 | Test Loss: 0.00182 | Valid Loss: 0.00179\n",
      "Epoch: 2200 | Loss: 0.00151 | Test Loss: 0.00152 | Valid Loss: 0.00150\n",
      "Epoch: 2400 | Loss: 0.00130 | Test Loss: 0.00130 | Valid Loss: 0.00129\n",
      "Epoch: 2600 | Loss: 0.00116 | Test Loss: 0.00115 | Valid Loss: 0.00115\n",
      "Epoch: 2800 | Loss: 0.00107 | Test Loss: 0.00106 | Valid Loss: 0.00106\n",
      "Epoch: 3000 | Loss: 0.00101 | Test Loss: 0.00101 | Valid Loss: 0.00101\n",
      "Epoch: 3200 | Loss: 0.00098 | Test Loss: 0.00098 | Valid Loss: 0.00098\n",
      "Epoch: 3400 | Loss: 0.00096 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 3600 | Loss: 0.00095 | Test Loss: 0.00095 | Valid Loss: 0.00095\n",
      "Epoch: 3800 | Loss: 0.00094 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 4000 | Loss: 0.00093 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 4200 | Loss: 0.00092 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 4400 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00091\n",
      "Epoch: 4600 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 4800 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 5000 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 5200 | Loss: 0.00087 | Test Loss: 0.00087 | Valid Loss: 0.00087\n",
      "Epoch: 5400 | Loss: 0.00086 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 5600 | Loss: 0.00085 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 5800 | Loss: 0.00084 | Test Loss: 0.00084 | Valid Loss: 0.00084\n",
      "Epoch: 6000 | Loss: 0.00084 | Test Loss: 0.00084 | Valid Loss: 0.00084\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00083\n",
      "Epoch: 6400 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00083\n",
      "Epoch: 6600 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00083\n",
      "Epoch: 6800 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00082\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 1.43821 | Test Loss: 1.38421 | Valid Loss: 1.38581\n",
      "Epoch: 200 | Loss: 0.00967 | Test Loss: 0.00973 | Valid Loss: 0.00990\n",
      "Epoch: 400 | Loss: 0.00896 | Test Loss: 0.00902 | Valid Loss: 0.00918\n",
      "Epoch: 600 | Loss: 0.00812 | Test Loss: 0.00817 | Valid Loss: 0.00832\n",
      "Epoch: 800 | Loss: 0.00723 | Test Loss: 0.00727 | Valid Loss: 0.00740\n",
      "Epoch: 1000 | Loss: 0.00635 | Test Loss: 0.00639 | Valid Loss: 0.00651\n",
      "Epoch: 1200 | Loss: 0.00553 | Test Loss: 0.00556 | Valid Loss: 0.00567\n",
      "Epoch: 1400 | Loss: 0.00477 | Test Loss: 0.00479 | Valid Loss: 0.00489\n",
      "Epoch: 1600 | Loss: 0.00407 | Test Loss: 0.00409 | Valid Loss: 0.00418\n",
      "Epoch: 1800 | Loss: 0.00345 | Test Loss: 0.00346 | Valid Loss: 0.00354\n",
      "Epoch: 2000 | Loss: 0.00290 | Test Loss: 0.00291 | Valid Loss: 0.00298\n",
      "Epoch: 2200 | Loss: 0.00242 | Test Loss: 0.00243 | Valid Loss: 0.00249\n",
      "Epoch: 2400 | Loss: 0.00201 | Test Loss: 0.00202 | Valid Loss: 0.00207\n",
      "Epoch: 2600 | Loss: 0.00167 | Test Loss: 0.00168 | Valid Loss: 0.00172\n",
      "Epoch: 2800 | Loss: 0.00141 | Test Loss: 0.00141 | Valid Loss: 0.00145\n",
      "Epoch: 3000 | Loss: 0.00121 | Test Loss: 0.00121 | Valid Loss: 0.00125\n",
      "Epoch: 3200 | Loss: 0.00108 | Test Loss: 0.00108 | Valid Loss: 0.00111\n",
      "Epoch: 3400 | Loss: 0.00100 | Test Loss: 0.00100 | Valid Loss: 0.00103\n",
      "Epoch: 3600 | Loss: 0.00096 | Test Loss: 0.00095 | Valid Loss: 0.00098\n",
      "Epoch: 3800 | Loss: 0.00093 | Test Loss: 0.00093 | Valid Loss: 0.00095\n",
      "Epoch: 4000 | Loss: 0.00092 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 4200 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00093\n",
      "Epoch: 4400 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00092\n",
      "Epoch: 4600 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00091\n",
      "Epoch: 4800 | Loss: 0.00088 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 5000 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00090\n",
      "Epoch: 5200 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 5400 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00088\n",
      "Epoch: 5600 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00088\n",
      "Epoch: 5800 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00087\n",
      "Epoch: 6000 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00087\n",
      "Epoch: 6200 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 6400 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 6600 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 6800 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 0.82638 | Test Loss: 0.78736 | Valid Loss: 0.78424\n",
      "Epoch: 200 | Loss: 0.01036 | Test Loss: 0.01064 | Valid Loss: 0.01046\n",
      "Epoch: 400 | Loss: 0.00935 | Test Loss: 0.00959 | Valid Loss: 0.00944\n",
      "Epoch: 600 | Loss: 0.00821 | Test Loss: 0.00842 | Valid Loss: 0.00829\n",
      "Epoch: 800 | Loss: 0.00707 | Test Loss: 0.00725 | Valid Loss: 0.00714\n",
      "Epoch: 1000 | Loss: 0.00598 | Test Loss: 0.00614 | Valid Loss: 0.00605\n",
      "Epoch: 1200 | Loss: 0.00497 | Test Loss: 0.00510 | Valid Loss: 0.00503\n",
      "Epoch: 1400 | Loss: 0.00404 | Test Loss: 0.00414 | Valid Loss: 0.00409\n",
      "Epoch: 1600 | Loss: 0.00321 | Test Loss: 0.00330 | Valid Loss: 0.00326\n",
      "Epoch: 1800 | Loss: 0.00251 | Test Loss: 0.00257 | Valid Loss: 0.00255\n",
      "Epoch: 2000 | Loss: 0.00195 | Test Loss: 0.00199 | Valid Loss: 0.00198\n",
      "Epoch: 2200 | Loss: 0.00152 | Test Loss: 0.00155 | Valid Loss: 0.00154\n",
      "Epoch: 2400 | Loss: 0.00122 | Test Loss: 0.00125 | Valid Loss: 0.00124\n",
      "Epoch: 2600 | Loss: 0.00104 | Test Loss: 0.00106 | Valid Loss: 0.00105\n",
      "Epoch: 2800 | Loss: 0.00094 | Test Loss: 0.00095 | Valid Loss: 0.00095\n",
      "Epoch: 3000 | Loss: 0.00089 | Test Loss: 0.00090 | Valid Loss: 0.00089\n",
      "Epoch: 3200 | Loss: 0.00086 | Test Loss: 0.00087 | Valid Loss: 0.00087\n",
      "Epoch: 3400 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 3600 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 3800 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 4000 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 4400 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 4600 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 4800 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 5000 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 5200 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 5400 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 5600 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 5800 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 6000 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 6200 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 6400 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 6600 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 6800 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 0.96519 | Test Loss: 0.92728 | Valid Loss: 0.91632\n",
      "Epoch: 200 | Loss: 0.00809 | Test Loss: 0.00835 | Valid Loss: 0.00808\n",
      "Epoch: 400 | Loss: 0.00741 | Test Loss: 0.00766 | Valid Loss: 0.00740\n",
      "Epoch: 600 | Loss: 0.00663 | Test Loss: 0.00686 | Valid Loss: 0.00662\n",
      "Epoch: 800 | Loss: 0.00584 | Test Loss: 0.00604 | Valid Loss: 0.00583\n",
      "Epoch: 1000 | Loss: 0.00507 | Test Loss: 0.00525 | Valid Loss: 0.00505\n",
      "Epoch: 1200 | Loss: 0.00432 | Test Loss: 0.00447 | Valid Loss: 0.00430\n",
      "Epoch: 1400 | Loss: 0.00360 | Test Loss: 0.00374 | Valid Loss: 0.00358\n",
      "Epoch: 1600 | Loss: 0.00294 | Test Loss: 0.00306 | Valid Loss: 0.00292\n",
      "Epoch: 1800 | Loss: 0.00236 | Test Loss: 0.00245 | Valid Loss: 0.00233\n",
      "Epoch: 2000 | Loss: 0.00186 | Test Loss: 0.00193 | Valid Loss: 0.00183\n",
      "Epoch: 2200 | Loss: 0.00145 | Test Loss: 0.00152 | Valid Loss: 0.00142\n",
      "Epoch: 2400 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00113\n",
      "Epoch: 2600 | Loss: 0.00097 | Test Loss: 0.00101 | Valid Loss: 0.00094\n",
      "Epoch: 2800 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00083\n",
      "Epoch: 3000 | Loss: 0.00080 | Test Loss: 0.00084 | Valid Loss: 0.00078\n",
      "Epoch: 3200 | Loss: 0.00078 | Test Loss: 0.00081 | Valid Loss: 0.00075\n",
      "Epoch: 3400 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00074\n",
      "Epoch: 3600 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00074\n",
      "Epoch: 3800 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00074\n",
      "Epoch: 4000 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00074\n",
      "Epoch: 4200 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00074\n",
      "Epoch: 4400 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00073\n",
      "Epoch: 4600 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00073\n",
      "Epoch: 4800 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00073\n",
      "Epoch: 5000 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00073\n",
      "Epoch: 5200 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 5400 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 5600 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 5800 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 6000 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00073\n",
      "Epoch: 6200 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "Epoch: 6400 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "Epoch: 6600 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "Epoch: 6800 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00072\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.75032 | Test Loss: 2.69136 | Valid Loss: 2.68355\n",
      "Epoch: 200 | Loss: 0.00738 | Test Loss: 0.00752 | Valid Loss: 0.00742\n",
      "Epoch: 400 | Loss: 0.00703 | Test Loss: 0.00716 | Valid Loss: 0.00708\n",
      "Epoch: 600 | Loss: 0.00660 | Test Loss: 0.00672 | Valid Loss: 0.00664\n",
      "Epoch: 800 | Loss: 0.00611 | Test Loss: 0.00623 | Valid Loss: 0.00615\n",
      "Epoch: 1000 | Loss: 0.00559 | Test Loss: 0.00571 | Valid Loss: 0.00563\n",
      "Epoch: 1200 | Loss: 0.00506 | Test Loss: 0.00518 | Valid Loss: 0.00511\n",
      "Epoch: 1400 | Loss: 0.00455 | Test Loss: 0.00466 | Valid Loss: 0.00460\n",
      "Epoch: 1600 | Loss: 0.00406 | Test Loss: 0.00416 | Valid Loss: 0.00410\n",
      "Epoch: 1800 | Loss: 0.00359 | Test Loss: 0.00369 | Valid Loss: 0.00364\n",
      "Epoch: 2000 | Loss: 0.00316 | Test Loss: 0.00326 | Valid Loss: 0.00320\n",
      "Epoch: 2200 | Loss: 0.00276 | Test Loss: 0.00285 | Valid Loss: 0.00281\n",
      "Epoch: 2400 | Loss: 0.00240 | Test Loss: 0.00248 | Valid Loss: 0.00244\n",
      "Epoch: 2600 | Loss: 0.00207 | Test Loss: 0.00215 | Valid Loss: 0.00211\n",
      "Epoch: 2800 | Loss: 0.00179 | Test Loss: 0.00186 | Valid Loss: 0.00182\n",
      "Epoch: 3000 | Loss: 0.00154 | Test Loss: 0.00161 | Valid Loss: 0.00157\n",
      "Epoch: 3200 | Loss: 0.00133 | Test Loss: 0.00139 | Valid Loss: 0.00136\n",
      "Epoch: 3400 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00118\n",
      "Epoch: 3600 | Loss: 0.00102 | Test Loss: 0.00108 | Valid Loss: 0.00105\n",
      "Epoch: 3800 | Loss: 0.00092 | Test Loss: 0.00097 | Valid Loss: 0.00094\n",
      "Epoch: 4000 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 4200 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00082\n",
      "Epoch: 4400 | Loss: 0.00078 | Test Loss: 0.00082 | Valid Loss: 0.00079\n",
      "Epoch: 4600 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00078\n",
      "Epoch: 4800 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 5000 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00076\n",
      "Epoch: 5200 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00076\n",
      "Epoch: 5400 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00076\n",
      "Epoch: 5600 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00076\n",
      "Epoch: 5800 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 6000 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 6200 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 6400 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 6600 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "Epoch: 6800 | Loss: 0.00075 | Test Loss: 0.00078 | Valid Loss: 0.00075\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 2.23154 | Test Loss: 2.17333 | Valid Loss: 2.16794\n",
      "Epoch: 200 | Loss: 0.00617 | Test Loss: 0.00613 | Valid Loss: 0.00626\n",
      "Epoch: 400 | Loss: 0.00591 | Test Loss: 0.00587 | Valid Loss: 0.00599\n",
      "Epoch: 600 | Loss: 0.00558 | Test Loss: 0.00555 | Valid Loss: 0.00566\n",
      "Epoch: 800 | Loss: 0.00521 | Test Loss: 0.00519 | Valid Loss: 0.00529\n",
      "Epoch: 1000 | Loss: 0.00482 | Test Loss: 0.00481 | Valid Loss: 0.00490\n",
      "Epoch: 1200 | Loss: 0.00442 | Test Loss: 0.00442 | Valid Loss: 0.00450\n",
      "Epoch: 1400 | Loss: 0.00402 | Test Loss: 0.00403 | Valid Loss: 0.00409\n",
      "Epoch: 1600 | Loss: 0.00363 | Test Loss: 0.00364 | Valid Loss: 0.00370\n",
      "Epoch: 1800 | Loss: 0.00324 | Test Loss: 0.00326 | Valid Loss: 0.00331\n",
      "Epoch: 2000 | Loss: 0.00287 | Test Loss: 0.00290 | Valid Loss: 0.00293\n",
      "Epoch: 2200 | Loss: 0.00252 | Test Loss: 0.00255 | Valid Loss: 0.00258\n",
      "Epoch: 2400 | Loss: 0.00219 | Test Loss: 0.00223 | Valid Loss: 0.00225\n",
      "Epoch: 2600 | Loss: 0.00190 | Test Loss: 0.00194 | Valid Loss: 0.00194\n",
      "Epoch: 2800 | Loss: 0.00163 | Test Loss: 0.00168 | Valid Loss: 0.00168\n",
      "Epoch: 3000 | Loss: 0.00141 | Test Loss: 0.00146 | Valid Loss: 0.00145\n",
      "Epoch: 3200 | Loss: 0.00122 | Test Loss: 0.00127 | Valid Loss: 0.00126\n",
      "Epoch: 3400 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00111\n",
      "Epoch: 3600 | Loss: 0.00097 | Test Loss: 0.00101 | Valid Loss: 0.00099\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00093 | Valid Loss: 0.00091\n",
      "Epoch: 4000 | Loss: 0.00084 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4200 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 4400 | Loss: 0.00079 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 4600 | Loss: 0.00078 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 4800 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00078\n",
      "Epoch: 5000 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00078\n",
      "Epoch: 5200 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 5400 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 5600 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 5800 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 6000 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 6200 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00077\n",
      "Epoch: 6400 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 6600 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 6800 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00076\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.31071 | Test Loss: 1.25693 | Valid Loss: 1.26300\n",
      "Epoch: 200 | Loss: 0.00886 | Test Loss: 0.00906 | Valid Loss: 0.00907\n",
      "Epoch: 400 | Loss: 0.00835 | Test Loss: 0.00854 | Valid Loss: 0.00855\n",
      "Epoch: 600 | Loss: 0.00773 | Test Loss: 0.00791 | Valid Loss: 0.00792\n",
      "Epoch: 800 | Loss: 0.00706 | Test Loss: 0.00723 | Valid Loss: 0.00724\n",
      "Epoch: 1000 | Loss: 0.00637 | Test Loss: 0.00653 | Valid Loss: 0.00654\n",
      "Epoch: 1200 | Loss: 0.00569 | Test Loss: 0.00583 | Valid Loss: 0.00584\n",
      "Epoch: 1400 | Loss: 0.00503 | Test Loss: 0.00515 | Valid Loss: 0.00516\n",
      "Epoch: 1600 | Loss: 0.00440 | Test Loss: 0.00451 | Valid Loss: 0.00452\n",
      "Epoch: 1800 | Loss: 0.00380 | Test Loss: 0.00390 | Valid Loss: 0.00391\n",
      "Epoch: 2000 | Loss: 0.00325 | Test Loss: 0.00333 | Valid Loss: 0.00334\n",
      "Epoch: 2200 | Loss: 0.00275 | Test Loss: 0.00282 | Valid Loss: 0.00283\n",
      "Epoch: 2400 | Loss: 0.00230 | Test Loss: 0.00236 | Valid Loss: 0.00237\n",
      "Epoch: 2600 | Loss: 0.00191 | Test Loss: 0.00196 | Valid Loss: 0.00197\n",
      "Epoch: 2800 | Loss: 0.00158 | Test Loss: 0.00162 | Valid Loss: 0.00163\n",
      "Epoch: 3000 | Loss: 0.00133 | Test Loss: 0.00136 | Valid Loss: 0.00137\n",
      "Epoch: 3200 | Loss: 0.00114 | Test Loss: 0.00116 | Valid Loss: 0.00117\n",
      "Epoch: 3400 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 3600 | Loss: 0.00092 | Test Loss: 0.00093 | Valid Loss: 0.00094\n",
      "Epoch: 3800 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 4000 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00084\n",
      "Epoch: 4400 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00083\n",
      "Epoch: 4600 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00083\n",
      "Epoch: 4800 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00082\n",
      "Epoch: 5000 | Loss: 0.00081 | Test Loss: 0.00081 | Valid Loss: 0.00082\n",
      "Epoch: 5200 | Loss: 0.00081 | Test Loss: 0.00081 | Valid Loss: 0.00081\n",
      "Epoch: 5400 | Loss: 0.00080 | Test Loss: 0.00080 | Valid Loss: 0.00081\n",
      "Epoch: 5600 | Loss: 0.00080 | Test Loss: 0.00080 | Valid Loss: 0.00081\n",
      "Epoch: 5800 | Loss: 0.00080 | Test Loss: 0.00080 | Valid Loss: 0.00080\n",
      "Epoch: 6000 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00080\n",
      "Epoch: 6200 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00079\n",
      "Epoch: 6400 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00079\n",
      "Epoch: 6600 | Loss: 0.00078 | Test Loss: 0.00078 | Valid Loss: 0.00079\n",
      "Epoch: 6800 | Loss: 0.00078 | Test Loss: 0.00078 | Valid Loss: 0.00079\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 0.86278 | Test Loss: 0.80759 | Valid Loss: 0.81489\n",
      "Epoch: 200 | Loss: 0.00925 | Test Loss: 0.00918 | Valid Loss: 0.00947\n",
      "Epoch: 400 | Loss: 0.00846 | Test Loss: 0.00840 | Valid Loss: 0.00867\n",
      "Epoch: 600 | Loss: 0.00758 | Test Loss: 0.00753 | Valid Loss: 0.00777\n",
      "Epoch: 800 | Loss: 0.00670 | Test Loss: 0.00666 | Valid Loss: 0.00688\n",
      "Epoch: 1000 | Loss: 0.00588 | Test Loss: 0.00585 | Valid Loss: 0.00604\n",
      "Epoch: 1200 | Loss: 0.00510 | Test Loss: 0.00508 | Valid Loss: 0.00525\n",
      "Epoch: 1400 | Loss: 0.00438 | Test Loss: 0.00437 | Valid Loss: 0.00452\n",
      "Epoch: 1600 | Loss: 0.00371 | Test Loss: 0.00370 | Valid Loss: 0.00383\n",
      "Epoch: 1800 | Loss: 0.00308 | Test Loss: 0.00308 | Valid Loss: 0.00319\n",
      "Epoch: 2000 | Loss: 0.00250 | Test Loss: 0.00251 | Valid Loss: 0.00259\n",
      "Epoch: 2200 | Loss: 0.00199 | Test Loss: 0.00201 | Valid Loss: 0.00207\n",
      "Epoch: 2400 | Loss: 0.00159 | Test Loss: 0.00161 | Valid Loss: 0.00165\n",
      "Epoch: 2600 | Loss: 0.00129 | Test Loss: 0.00131 | Valid Loss: 0.00134\n",
      "Epoch: 2800 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00114\n",
      "Epoch: 3000 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00102\n",
      "Epoch: 3200 | Loss: 0.00093 | Test Loss: 0.00095 | Valid Loss: 0.00096\n",
      "Epoch: 3400 | Loss: 0.00090 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 3600 | Loss: 0.00089 | Test Loss: 0.00091 | Valid Loss: 0.00091\n",
      "Epoch: 3800 | Loss: 0.00088 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 4000 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 4200 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00088\n",
      "Epoch: 4400 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 4600 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00087\n",
      "Epoch: 5000 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00087\n",
      "Epoch: 5200 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 5400 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 5600 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00086\n",
      "Epoch: 5800 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00086\n",
      "Epoch: 6000 | Loss: 0.00084 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 6400 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 6600 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00085\n",
      "Epoch: 6800 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00085\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 1.68765 | Test Loss: 1.61418 | Valid Loss: 1.62228\n",
      "Epoch: 200 | Loss: 0.00922 | Test Loss: 0.00923 | Valid Loss: 0.00906\n",
      "Epoch: 400 | Loss: 0.00868 | Test Loss: 0.00869 | Valid Loss: 0.00853\n",
      "Epoch: 600 | Loss: 0.00803 | Test Loss: 0.00804 | Valid Loss: 0.00789\n",
      "Epoch: 800 | Loss: 0.00734 | Test Loss: 0.00734 | Valid Loss: 0.00721\n",
      "Epoch: 1000 | Loss: 0.00663 | Test Loss: 0.00664 | Valid Loss: 0.00651\n",
      "Epoch: 1200 | Loss: 0.00595 | Test Loss: 0.00595 | Valid Loss: 0.00584\n",
      "Epoch: 1400 | Loss: 0.00529 | Test Loss: 0.00529 | Valid Loss: 0.00519\n",
      "Epoch: 1600 | Loss: 0.00467 | Test Loss: 0.00467 | Valid Loss: 0.00458\n",
      "Epoch: 1800 | Loss: 0.00408 | Test Loss: 0.00408 | Valid Loss: 0.00401\n",
      "Epoch: 2000 | Loss: 0.00354 | Test Loss: 0.00354 | Valid Loss: 0.00348\n",
      "Epoch: 2200 | Loss: 0.00305 | Test Loss: 0.00305 | Valid Loss: 0.00299\n",
      "Epoch: 2400 | Loss: 0.00260 | Test Loss: 0.00260 | Valid Loss: 0.00255\n",
      "Epoch: 2600 | Loss: 0.00221 | Test Loss: 0.00221 | Valid Loss: 0.00217\n",
      "Epoch: 2800 | Loss: 0.00187 | Test Loss: 0.00187 | Valid Loss: 0.00184\n",
      "Epoch: 3000 | Loss: 0.00160 | Test Loss: 0.00159 | Valid Loss: 0.00157\n",
      "Epoch: 3200 | Loss: 0.00138 | Test Loss: 0.00137 | Valid Loss: 0.00136\n",
      "Epoch: 3400 | Loss: 0.00122 | Test Loss: 0.00121 | Valid Loss: 0.00121\n",
      "Epoch: 3600 | Loss: 0.00110 | Test Loss: 0.00110 | Valid Loss: 0.00110\n",
      "Epoch: 3800 | Loss: 0.00103 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 4000 | Loss: 0.00098 | Test Loss: 0.00098 | Valid Loss: 0.00098\n",
      "Epoch: 4200 | Loss: 0.00096 | Test Loss: 0.00095 | Valid Loss: 0.00096\n",
      "Epoch: 4400 | Loss: 0.00094 | Test Loss: 0.00093 | Valid Loss: 0.00094\n",
      "Epoch: 4600 | Loss: 0.00093 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 4800 | Loss: 0.00092 | Test Loss: 0.00091 | Valid Loss: 0.00093\n",
      "Epoch: 5000 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00092\n",
      "Epoch: 5200 | Loss: 0.00091 | Test Loss: 0.00090 | Valid Loss: 0.00091\n",
      "Epoch: 5400 | Loss: 0.00090 | Test Loss: 0.00089 | Valid Loss: 0.00091\n",
      "Epoch: 5600 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 5800 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 6000 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00088\n",
      "Epoch: 6200 | Loss: 0.00087 | Test Loss: 0.00086 | Valid Loss: 0.00087\n",
      "Epoch: 6400 | Loss: 0.00086 | Test Loss: 0.00086 | Valid Loss: 0.00087\n",
      "Epoch: 6600 | Loss: 0.00085 | Test Loss: 0.00085 | Valid Loss: 0.00086\n",
      "Epoch: 6800 | Loss: 0.00085 | Test Loss: 0.00084 | Valid Loss: 0.00085\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 1.01798 | Test Loss: 0.96688 | Valid Loss: 0.97384\n",
      "Epoch: 200 | Loss: 0.00933 | Test Loss: 0.00917 | Valid Loss: 0.00949\n",
      "Epoch: 400 | Loss: 0.00841 | Test Loss: 0.00826 | Valid Loss: 0.00856\n",
      "Epoch: 600 | Loss: 0.00737 | Test Loss: 0.00724 | Valid Loss: 0.00750\n",
      "Epoch: 800 | Loss: 0.00633 | Test Loss: 0.00621 | Valid Loss: 0.00645\n",
      "Epoch: 1000 | Loss: 0.00535 | Test Loss: 0.00525 | Valid Loss: 0.00546\n",
      "Epoch: 1200 | Loss: 0.00447 | Test Loss: 0.00438 | Valid Loss: 0.00457\n",
      "Epoch: 1400 | Loss: 0.00369 | Test Loss: 0.00361 | Valid Loss: 0.00378\n",
      "Epoch: 1600 | Loss: 0.00302 | Test Loss: 0.00295 | Valid Loss: 0.00309\n",
      "Epoch: 1800 | Loss: 0.00245 | Test Loss: 0.00240 | Valid Loss: 0.00252\n",
      "Epoch: 2000 | Loss: 0.00199 | Test Loss: 0.00195 | Valid Loss: 0.00205\n",
      "Epoch: 2200 | Loss: 0.00164 | Test Loss: 0.00160 | Valid Loss: 0.00169\n",
      "Epoch: 2400 | Loss: 0.00139 | Test Loss: 0.00136 | Valid Loss: 0.00144\n",
      "Epoch: 2600 | Loss: 0.00123 | Test Loss: 0.00120 | Valid Loss: 0.00127\n",
      "Epoch: 2800 | Loss: 0.00113 | Test Loss: 0.00111 | Valid Loss: 0.00117\n",
      "Epoch: 3000 | Loss: 0.00108 | Test Loss: 0.00105 | Valid Loss: 0.00111\n",
      "Epoch: 3200 | Loss: 0.00105 | Test Loss: 0.00103 | Valid Loss: 0.00108\n",
      "Epoch: 3400 | Loss: 0.00103 | Test Loss: 0.00101 | Valid Loss: 0.00106\n",
      "Epoch: 3600 | Loss: 0.00102 | Test Loss: 0.00099 | Valid Loss: 0.00104\n",
      "Epoch: 3800 | Loss: 0.00100 | Test Loss: 0.00098 | Valid Loss: 0.00103\n",
      "Epoch: 4000 | Loss: 0.00099 | Test Loss: 0.00097 | Valid Loss: 0.00101\n",
      "Epoch: 4200 | Loss: 0.00098 | Test Loss: 0.00095 | Valid Loss: 0.00099\n",
      "Epoch: 4400 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00098\n",
      "Epoch: 4600 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00096\n",
      "Epoch: 4800 | Loss: 0.00093 | Test Loss: 0.00091 | Valid Loss: 0.00095\n",
      "Epoch: 5000 | Loss: 0.00092 | Test Loss: 0.00090 | Valid Loss: 0.00093\n",
      "Epoch: 5200 | Loss: 0.00091 | Test Loss: 0.00089 | Valid Loss: 0.00092\n",
      "Epoch: 5400 | Loss: 0.00090 | Test Loss: 0.00088 | Valid Loss: 0.00091\n",
      "Epoch: 5600 | Loss: 0.00090 | Test Loss: 0.00088 | Valid Loss: 0.00090\n",
      "Epoch: 5800 | Loss: 0.00089 | Test Loss: 0.00087 | Valid Loss: 0.00089\n",
      "Epoch: 6000 | Loss: 0.00089 | Test Loss: 0.00087 | Valid Loss: 0.00089\n",
      "Epoch: 6200 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 6400 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 6600 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 6800 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 0.93165 | Test Loss: 0.87536 | Valid Loss: 0.87256\n",
      "Epoch: 200 | Loss: 0.00945 | Test Loss: 0.00929 | Valid Loss: 0.00970\n",
      "Epoch: 400 | Loss: 0.00875 | Test Loss: 0.00860 | Valid Loss: 0.00898\n",
      "Epoch: 600 | Loss: 0.00795 | Test Loss: 0.00782 | Valid Loss: 0.00816\n",
      "Epoch: 800 | Loss: 0.00714 | Test Loss: 0.00702 | Valid Loss: 0.00733\n",
      "Epoch: 1000 | Loss: 0.00637 | Test Loss: 0.00626 | Valid Loss: 0.00653\n",
      "Epoch: 1200 | Loss: 0.00564 | Test Loss: 0.00554 | Valid Loss: 0.00578\n",
      "Epoch: 1400 | Loss: 0.00497 | Test Loss: 0.00488 | Valid Loss: 0.00509\n",
      "Epoch: 1600 | Loss: 0.00434 | Test Loss: 0.00426 | Valid Loss: 0.00444\n",
      "Epoch: 1800 | Loss: 0.00377 | Test Loss: 0.00370 | Valid Loss: 0.00385\n",
      "Epoch: 2000 | Loss: 0.00325 | Test Loss: 0.00319 | Valid Loss: 0.00332\n",
      "Epoch: 2200 | Loss: 0.00279 | Test Loss: 0.00274 | Valid Loss: 0.00284\n",
      "Epoch: 2400 | Loss: 0.00239 | Test Loss: 0.00234 | Valid Loss: 0.00242\n",
      "Epoch: 2600 | Loss: 0.00204 | Test Loss: 0.00200 | Valid Loss: 0.00206\n",
      "Epoch: 2800 | Loss: 0.00176 | Test Loss: 0.00173 | Valid Loss: 0.00177\n",
      "Epoch: 3000 | Loss: 0.00154 | Test Loss: 0.00151 | Valid Loss: 0.00154\n",
      "Epoch: 3200 | Loss: 0.00138 | Test Loss: 0.00136 | Valid Loss: 0.00137\n",
      "Epoch: 3400 | Loss: 0.00127 | Test Loss: 0.00125 | Valid Loss: 0.00125\n",
      "Epoch: 3600 | Loss: 0.00120 | Test Loss: 0.00119 | Valid Loss: 0.00118\n",
      "Epoch: 3800 | Loss: 0.00116 | Test Loss: 0.00115 | Valid Loss: 0.00114\n",
      "Epoch: 4000 | Loss: 0.00114 | Test Loss: 0.00113 | Valid Loss: 0.00112\n",
      "Epoch: 4200 | Loss: 0.00113 | Test Loss: 0.00113 | Valid Loss: 0.00111\n",
      "Epoch: 4400 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 4600 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 4800 | Loss: 0.00111 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 5000 | Loss: 0.00111 | Test Loss: 0.00111 | Valid Loss: 0.00108\n",
      "Epoch: 5200 | Loss: 0.00111 | Test Loss: 0.00111 | Valid Loss: 0.00108\n",
      "Epoch: 5400 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00108\n",
      "Epoch: 5600 | Loss: 0.00110 | Test Loss: 0.00110 | Valid Loss: 0.00107\n",
      "Epoch: 5800 | Loss: 0.00110 | Test Loss: 0.00110 | Valid Loss: 0.00107\n",
      "Epoch: 6000 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00107\n",
      "Epoch: 6200 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00106\n",
      "Epoch: 6400 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00106\n",
      "Epoch: 6600 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00106\n",
      "Epoch: 6800 | Loss: 0.00109 | Test Loss: 0.00109 | Valid Loss: 0.00106\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.12733 | Test Loss: 1.08287 | Valid Loss: 1.07174\n",
      "Epoch: 200 | Loss: 0.00962 | Test Loss: 0.00919 | Valid Loss: 0.00944\n",
      "Epoch: 400 | Loss: 0.00886 | Test Loss: 0.00847 | Valid Loss: 0.00870\n",
      "Epoch: 600 | Loss: 0.00797 | Test Loss: 0.00763 | Valid Loss: 0.00783\n",
      "Epoch: 800 | Loss: 0.00705 | Test Loss: 0.00675 | Valid Loss: 0.00693\n",
      "Epoch: 1000 | Loss: 0.00614 | Test Loss: 0.00589 | Valid Loss: 0.00604\n",
      "Epoch: 1200 | Loss: 0.00529 | Test Loss: 0.00508 | Valid Loss: 0.00520\n",
      "Epoch: 1400 | Loss: 0.00451 | Test Loss: 0.00434 | Valid Loss: 0.00443\n",
      "Epoch: 1600 | Loss: 0.00378 | Test Loss: 0.00365 | Valid Loss: 0.00372\n",
      "Epoch: 1800 | Loss: 0.00313 | Test Loss: 0.00303 | Valid Loss: 0.00308\n",
      "Epoch: 2000 | Loss: 0.00256 | Test Loss: 0.00249 | Valid Loss: 0.00252\n",
      "Epoch: 2200 | Loss: 0.00208 | Test Loss: 0.00204 | Valid Loss: 0.00205\n",
      "Epoch: 2400 | Loss: 0.00171 | Test Loss: 0.00169 | Valid Loss: 0.00168\n",
      "Epoch: 2600 | Loss: 0.00145 | Test Loss: 0.00144 | Valid Loss: 0.00143\n",
      "Epoch: 2800 | Loss: 0.00129 | Test Loss: 0.00129 | Valid Loss: 0.00127\n",
      "Epoch: 3000 | Loss: 0.00120 | Test Loss: 0.00120 | Valid Loss: 0.00118\n",
      "Epoch: 3200 | Loss: 0.00116 | Test Loss: 0.00116 | Valid Loss: 0.00114\n",
      "Epoch: 3400 | Loss: 0.00114 | Test Loss: 0.00115 | Valid Loss: 0.00112\n",
      "Epoch: 3600 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00111\n",
      "Epoch: 3800 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00111\n",
      "Epoch: 4000 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00111\n",
      "Epoch: 4200 | Loss: 0.00113 | Test Loss: 0.00114 | Valid Loss: 0.00111\n",
      "Epoch: 4400 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 4600 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 4800 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 5000 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 5200 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 5400 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 5600 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 5800 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 6000 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 6200 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 6400 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 6600 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 6800 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 7000\n",
    "learning_rate = 0.001\n",
    "momentum = 1.2\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_M_stacking_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_M_stack_train[:, num, :]).squeeze() # беру результаты сетей для num столбца\n",
    "                                                                # по смыслу - результаты G M на num координате слоя\n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_M_stack_valid[:, num, :]).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_M_stack_test[:, num, :]).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8c1857ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 6.5036 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_M_stacking_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_M_stack_test[:, num, :]).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_M_stack_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_M_stack_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bdc71b",
   "metadata": {},
   "source": [
    "# Stacking - G + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "da609112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim_1 = 31\n",
    "input_dim_2 = 62\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "num_methods = 2\n",
    "\n",
    "meta_input_dim = 10\n",
    "\n",
    "num_models = 15\n",
    "G_MT_stacking_models = []\n",
    "    \n",
    "for i in range(num_models):\n",
    "    G_MT_stacking_models.append(Initial_Model_V0(meta_input_dim, hidden_units, output_dim).to(device))\n",
    "    \n",
    "next(G_MT_stacking_models[1].parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "795fbc91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 31])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_G_train, y_train = X_G_train.to(device), y_train.to(device)\n",
    "X_G_test, y_test = X_G_test.to(device), y_test.to(device)\n",
    "X_G_valid, y_valid = X_G_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_MT_train = X_MT_train.to(device)\n",
    "X_MT_test = X_MT_test.to(device)\n",
    "X_MT_valid = X_MT_valid.to(device)\n",
    "\n",
    "X_G_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2e1fa982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 15, 10])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_NN_in_set = 5\n",
    "\n",
    "# Training\n",
    "\n",
    "G_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_train)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_train)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_MT_stack_train = torch.cat((torch.cat(G_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_MT_stack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d7bc5dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 15, 10])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "G_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_valid)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_valid)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_MT_stack_valid = torch.cat((torch.cat(G_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_MT_stack_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32755e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 15, 10])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "G_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_test)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_test)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_MT_stack_test = torch.cat((torch.cat(G_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_MT_stack_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d04922a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 2.12298 | Test Loss: 2.05964 | Valid Loss: 2.03762\n",
      "Epoch: 200 | Loss: 0.00811 | Test Loss: 0.00825 | Valid Loss: 0.00811\n",
      "Epoch: 400 | Loss: 0.00767 | Test Loss: 0.00781 | Valid Loss: 0.00767\n",
      "Epoch: 600 | Loss: 0.00713 | Test Loss: 0.00726 | Valid Loss: 0.00713\n",
      "Epoch: 800 | Loss: 0.00654 | Test Loss: 0.00666 | Valid Loss: 0.00653\n",
      "Epoch: 1000 | Loss: 0.00591 | Test Loss: 0.00602 | Valid Loss: 0.00591\n",
      "Epoch: 1200 | Loss: 0.00528 | Test Loss: 0.00539 | Valid Loss: 0.00528\n",
      "Epoch: 1400 | Loss: 0.00467 | Test Loss: 0.00476 | Valid Loss: 0.00466\n",
      "Epoch: 1600 | Loss: 0.00408 | Test Loss: 0.00416 | Valid Loss: 0.00407\n",
      "Epoch: 1800 | Loss: 0.00352 | Test Loss: 0.00360 | Valid Loss: 0.00351\n",
      "Epoch: 2000 | Loss: 0.00301 | Test Loss: 0.00308 | Valid Loss: 0.00300\n",
      "Epoch: 2200 | Loss: 0.00255 | Test Loss: 0.00262 | Valid Loss: 0.00254\n",
      "Epoch: 2400 | Loss: 0.00215 | Test Loss: 0.00221 | Valid Loss: 0.00214\n",
      "Epoch: 2600 | Loss: 0.00181 | Test Loss: 0.00187 | Valid Loss: 0.00180\n",
      "Epoch: 2800 | Loss: 0.00154 | Test Loss: 0.00158 | Valid Loss: 0.00153\n",
      "Epoch: 3000 | Loss: 0.00132 | Test Loss: 0.00136 | Valid Loss: 0.00131\n",
      "Epoch: 3200 | Loss: 0.00116 | Test Loss: 0.00120 | Valid Loss: 0.00115\n",
      "Epoch: 3400 | Loss: 0.00104 | Test Loss: 0.00108 | Valid Loss: 0.00104\n",
      "Epoch: 3600 | Loss: 0.00097 | Test Loss: 0.00100 | Valid Loss: 0.00096\n",
      "Epoch: 3800 | Loss: 0.00092 | Test Loss: 0.00095 | Valid Loss: 0.00092\n",
      "Epoch: 4000 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 4200 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 4400 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00086\n",
      "Epoch: 4600 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 5000 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00085\n",
      "Epoch: 5200 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00085\n",
      "Epoch: 5400 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00085\n",
      "Epoch: 5600 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5800 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6000 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6200 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6400 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6600 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6800 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00084\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.48290 | Test Loss: 1.42549 | Valid Loss: 1.40478\n",
      "Epoch: 200 | Loss: 0.01055 | Test Loss: 0.01095 | Valid Loss: 0.01046\n",
      "Epoch: 400 | Loss: 0.00985 | Test Loss: 0.01023 | Valid Loss: 0.00977\n",
      "Epoch: 600 | Loss: 0.00900 | Test Loss: 0.00934 | Valid Loss: 0.00892\n",
      "Epoch: 800 | Loss: 0.00806 | Test Loss: 0.00837 | Valid Loss: 0.00799\n",
      "Epoch: 1000 | Loss: 0.00711 | Test Loss: 0.00738 | Valid Loss: 0.00705\n",
      "Epoch: 1200 | Loss: 0.00619 | Test Loss: 0.00643 | Valid Loss: 0.00614\n",
      "Epoch: 1400 | Loss: 0.00533 | Test Loss: 0.00553 | Valid Loss: 0.00528\n",
      "Epoch: 1600 | Loss: 0.00454 | Test Loss: 0.00472 | Valid Loss: 0.00450\n",
      "Epoch: 1800 | Loss: 0.00384 | Test Loss: 0.00398 | Valid Loss: 0.00381\n",
      "Epoch: 2000 | Loss: 0.00321 | Test Loss: 0.00333 | Valid Loss: 0.00318\n",
      "Epoch: 2200 | Loss: 0.00265 | Test Loss: 0.00275 | Valid Loss: 0.00263\n",
      "Epoch: 2400 | Loss: 0.00216 | Test Loss: 0.00225 | Valid Loss: 0.00215\n",
      "Epoch: 2600 | Loss: 0.00175 | Test Loss: 0.00182 | Valid Loss: 0.00174\n",
      "Epoch: 2800 | Loss: 0.00142 | Test Loss: 0.00147 | Valid Loss: 0.00141\n",
      "Epoch: 3000 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00116\n",
      "Epoch: 3200 | Loss: 0.00098 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 3400 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 3600 | Loss: 0.00080 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 3800 | Loss: 0.00076 | Test Loss: 0.00078 | Valid Loss: 0.00077\n",
      "Epoch: 4000 | Loss: 0.00074 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 4200 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 4400 | Loss: 0.00073 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 4600 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00075\n",
      "Epoch: 4800 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00075\n",
      "Epoch: 5000 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 5200 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 5400 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 5600 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 5800 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 6000 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 6200 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 6400 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 6600 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 6800 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 1.30229 | Test Loss: 1.24343 | Valid Loss: 1.23477\n",
      "Epoch: 200 | Loss: 0.00974 | Test Loss: 0.00971 | Valid Loss: 0.00976\n",
      "Epoch: 400 | Loss: 0.00921 | Test Loss: 0.00918 | Valid Loss: 0.00923\n",
      "Epoch: 600 | Loss: 0.00857 | Test Loss: 0.00854 | Valid Loss: 0.00859\n",
      "Epoch: 800 | Loss: 0.00788 | Test Loss: 0.00785 | Valid Loss: 0.00790\n",
      "Epoch: 1000 | Loss: 0.00717 | Test Loss: 0.00714 | Valid Loss: 0.00719\n",
      "Epoch: 1200 | Loss: 0.00647 | Test Loss: 0.00643 | Valid Loss: 0.00648\n",
      "Epoch: 1400 | Loss: 0.00577 | Test Loss: 0.00574 | Valid Loss: 0.00578\n",
      "Epoch: 1600 | Loss: 0.00508 | Test Loss: 0.00505 | Valid Loss: 0.00509\n",
      "Epoch: 1800 | Loss: 0.00441 | Test Loss: 0.00438 | Valid Loss: 0.00441\n",
      "Epoch: 2000 | Loss: 0.00375 | Test Loss: 0.00372 | Valid Loss: 0.00375\n",
      "Epoch: 2200 | Loss: 0.00313 | Test Loss: 0.00310 | Valid Loss: 0.00312\n",
      "Epoch: 2400 | Loss: 0.00255 | Test Loss: 0.00253 | Valid Loss: 0.00255\n",
      "Epoch: 2600 | Loss: 0.00205 | Test Loss: 0.00203 | Valid Loss: 0.00204\n",
      "Epoch: 2800 | Loss: 0.00162 | Test Loss: 0.00160 | Valid Loss: 0.00161\n",
      "Epoch: 3000 | Loss: 0.00129 | Test Loss: 0.00127 | Valid Loss: 0.00127\n",
      "Epoch: 3200 | Loss: 0.00104 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 3400 | Loss: 0.00089 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 3600 | Loss: 0.00079 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 3800 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00073\n",
      "Epoch: 4000 | Loss: 0.00071 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 4200 | Loss: 0.00070 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 4400 | Loss: 0.00070 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 4600 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 4800 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 5000 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 5200 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 5400 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 5600 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 5800 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6000 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6200 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6400 | Loss: 0.00069 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6600 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6800 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 0.81236 | Test Loss: 0.76839 | Valid Loss: 0.76627\n",
      "Epoch: 200 | Loss: 0.00939 | Test Loss: 0.00970 | Valid Loss: 0.00946\n",
      "Epoch: 400 | Loss: 0.00859 | Test Loss: 0.00887 | Valid Loss: 0.00865\n",
      "Epoch: 600 | Loss: 0.00764 | Test Loss: 0.00790 | Valid Loss: 0.00769\n",
      "Epoch: 800 | Loss: 0.00667 | Test Loss: 0.00689 | Valid Loss: 0.00671\n",
      "Epoch: 1000 | Loss: 0.00572 | Test Loss: 0.00591 | Valid Loss: 0.00574\n",
      "Epoch: 1200 | Loss: 0.00482 | Test Loss: 0.00498 | Valid Loss: 0.00484\n",
      "Epoch: 1400 | Loss: 0.00401 | Test Loss: 0.00414 | Valid Loss: 0.00402\n",
      "Epoch: 1600 | Loss: 0.00327 | Test Loss: 0.00338 | Valid Loss: 0.00327\n",
      "Epoch: 1800 | Loss: 0.00261 | Test Loss: 0.00270 | Valid Loss: 0.00261\n",
      "Epoch: 2000 | Loss: 0.00204 | Test Loss: 0.00211 | Valid Loss: 0.00203\n",
      "Epoch: 2200 | Loss: 0.00156 | Test Loss: 0.00161 | Valid Loss: 0.00155\n",
      "Epoch: 2400 | Loss: 0.00119 | Test Loss: 0.00123 | Valid Loss: 0.00118\n",
      "Epoch: 2600 | Loss: 0.00093 | Test Loss: 0.00095 | Valid Loss: 0.00091\n",
      "Epoch: 2800 | Loss: 0.00075 | Test Loss: 0.00077 | Valid Loss: 0.00074\n",
      "Epoch: 3000 | Loss: 0.00066 | Test Loss: 0.00067 | Valid Loss: 0.00065\n",
      "Epoch: 3200 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 3400 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 3600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 3800 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5800 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 6000 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 6200 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 1.43805 | Test Loss: 1.38377 | Valid Loss: 1.38592\n",
      "Epoch: 200 | Loss: 0.01083 | Test Loss: 0.01088 | Valid Loss: 0.01111\n",
      "Epoch: 400 | Loss: 0.01021 | Test Loss: 0.01025 | Valid Loss: 0.01047\n",
      "Epoch: 600 | Loss: 0.00944 | Test Loss: 0.00947 | Valid Loss: 0.00968\n",
      "Epoch: 800 | Loss: 0.00859 | Test Loss: 0.00862 | Valid Loss: 0.00881\n",
      "Epoch: 1000 | Loss: 0.00774 | Test Loss: 0.00776 | Valid Loss: 0.00794\n",
      "Epoch: 1200 | Loss: 0.00690 | Test Loss: 0.00691 | Valid Loss: 0.00708\n",
      "Epoch: 1400 | Loss: 0.00609 | Test Loss: 0.00610 | Valid Loss: 0.00625\n",
      "Epoch: 1600 | Loss: 0.00531 | Test Loss: 0.00532 | Valid Loss: 0.00545\n",
      "Epoch: 1800 | Loss: 0.00456 | Test Loss: 0.00456 | Valid Loss: 0.00468\n",
      "Epoch: 2000 | Loss: 0.00385 | Test Loss: 0.00385 | Valid Loss: 0.00395\n",
      "Epoch: 2200 | Loss: 0.00318 | Test Loss: 0.00318 | Valid Loss: 0.00326\n",
      "Epoch: 2400 | Loss: 0.00255 | Test Loss: 0.00255 | Valid Loss: 0.00262\n",
      "Epoch: 2600 | Loss: 0.00199 | Test Loss: 0.00198 | Valid Loss: 0.00204\n",
      "Epoch: 2800 | Loss: 0.00150 | Test Loss: 0.00150 | Valid Loss: 0.00154\n",
      "Epoch: 3000 | Loss: 0.00112 | Test Loss: 0.00112 | Valid Loss: 0.00114\n",
      "Epoch: 3200 | Loss: 0.00086 | Test Loss: 0.00086 | Valid Loss: 0.00088\n",
      "Epoch: 3400 | Loss: 0.00071 | Test Loss: 0.00072 | Valid Loss: 0.00072\n",
      "Epoch: 3600 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 3800 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 4000 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 4200 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 4400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 4600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4800 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 5000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 5200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 5400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 5800 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6200 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6400 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6600 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6800 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 0.82619 | Test Loss: 0.78723 | Valid Loss: 0.78410\n",
      "Epoch: 200 | Loss: 0.01080 | Test Loss: 0.01108 | Valid Loss: 0.01091\n",
      "Epoch: 400 | Loss: 0.00992 | Test Loss: 0.01018 | Valid Loss: 0.01002\n",
      "Epoch: 600 | Loss: 0.00890 | Test Loss: 0.00913 | Valid Loss: 0.00900\n",
      "Epoch: 800 | Loss: 0.00785 | Test Loss: 0.00805 | Valid Loss: 0.00794\n",
      "Epoch: 1000 | Loss: 0.00681 | Test Loss: 0.00698 | Valid Loss: 0.00689\n",
      "Epoch: 1200 | Loss: 0.00579 | Test Loss: 0.00594 | Valid Loss: 0.00587\n",
      "Epoch: 1400 | Loss: 0.00482 | Test Loss: 0.00494 | Valid Loss: 0.00488\n",
      "Epoch: 1600 | Loss: 0.00390 | Test Loss: 0.00400 | Valid Loss: 0.00395\n",
      "Epoch: 1800 | Loss: 0.00305 | Test Loss: 0.00313 | Valid Loss: 0.00310\n",
      "Epoch: 2000 | Loss: 0.00231 | Test Loss: 0.00237 | Valid Loss: 0.00235\n",
      "Epoch: 2200 | Loss: 0.00169 | Test Loss: 0.00174 | Valid Loss: 0.00172\n",
      "Epoch: 2400 | Loss: 0.00123 | Test Loss: 0.00126 | Valid Loss: 0.00125\n",
      "Epoch: 2600 | Loss: 0.00092 | Test Loss: 0.00095 | Valid Loss: 0.00094\n",
      "Epoch: 2800 | Loss: 0.00074 | Test Loss: 0.00077 | Valid Loss: 0.00075\n",
      "Epoch: 3000 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Epoch: 3200 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 3400 | Loss: 0.00061 | Test Loss: 0.00063 | Valid Loss: 0.00061\n",
      "Epoch: 3600 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 3800 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4000 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4200 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4400 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4600 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4800 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5000 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5200 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 5400 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 5600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 5800 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6000 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6400 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6800 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 0.96516 | Test Loss: 0.92719 | Valid Loss: 0.91630\n",
      "Epoch: 200 | Loss: 0.00860 | Test Loss: 0.00887 | Valid Loss: 0.00862\n",
      "Epoch: 400 | Loss: 0.00798 | Test Loss: 0.00823 | Valid Loss: 0.00799\n",
      "Epoch: 600 | Loss: 0.00724 | Test Loss: 0.00747 | Valid Loss: 0.00725\n",
      "Epoch: 800 | Loss: 0.00647 | Test Loss: 0.00668 | Valid Loss: 0.00648\n",
      "Epoch: 1000 | Loss: 0.00569 | Test Loss: 0.00587 | Valid Loss: 0.00569\n",
      "Epoch: 1200 | Loss: 0.00491 | Test Loss: 0.00507 | Valid Loss: 0.00491\n",
      "Epoch: 1400 | Loss: 0.00415 | Test Loss: 0.00428 | Valid Loss: 0.00414\n",
      "Epoch: 1600 | Loss: 0.00341 | Test Loss: 0.00353 | Valid Loss: 0.00340\n",
      "Epoch: 1800 | Loss: 0.00272 | Test Loss: 0.00282 | Valid Loss: 0.00271\n",
      "Epoch: 2000 | Loss: 0.00210 | Test Loss: 0.00218 | Valid Loss: 0.00208\n",
      "Epoch: 2200 | Loss: 0.00157 | Test Loss: 0.00164 | Valid Loss: 0.00155\n",
      "Epoch: 2400 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00114\n",
      "Epoch: 2600 | Loss: 0.00087 | Test Loss: 0.00092 | Valid Loss: 0.00085\n",
      "Epoch: 2800 | Loss: 0.00071 | Test Loss: 0.00074 | Valid Loss: 0.00068\n",
      "Epoch: 3000 | Loss: 0.00062 | Test Loss: 0.00066 | Valid Loss: 0.00060\n",
      "Epoch: 3200 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00056\n",
      "Epoch: 3400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00055\n",
      "Epoch: 3600 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 3800 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 4000 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 4200 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 4400 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 4600 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 4800 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00053\n",
      "Epoch: 5000 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00053\n",
      "Epoch: 5200 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00053\n",
      "Epoch: 5400 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 5600 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 6000 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 6800 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.74993 | Test Loss: 2.69083 | Valid Loss: 2.68305\n",
      "Epoch: 200 | Loss: 0.00801 | Test Loss: 0.00813 | Valid Loss: 0.00805\n",
      "Epoch: 400 | Loss: 0.00770 | Test Loss: 0.00781 | Valid Loss: 0.00773\n",
      "Epoch: 600 | Loss: 0.00730 | Test Loss: 0.00741 | Valid Loss: 0.00734\n",
      "Epoch: 800 | Loss: 0.00685 | Test Loss: 0.00695 | Valid Loss: 0.00688\n",
      "Epoch: 1000 | Loss: 0.00636 | Test Loss: 0.00646 | Valid Loss: 0.00639\n",
      "Epoch: 1200 | Loss: 0.00584 | Test Loss: 0.00594 | Valid Loss: 0.00588\n",
      "Epoch: 1400 | Loss: 0.00532 | Test Loss: 0.00542 | Valid Loss: 0.00536\n",
      "Epoch: 1600 | Loss: 0.00481 | Test Loss: 0.00490 | Valid Loss: 0.00485\n",
      "Epoch: 1800 | Loss: 0.00431 | Test Loss: 0.00439 | Valid Loss: 0.00435\n",
      "Epoch: 2000 | Loss: 0.00383 | Test Loss: 0.00391 | Valid Loss: 0.00387\n",
      "Epoch: 2200 | Loss: 0.00337 | Test Loss: 0.00344 | Valid Loss: 0.00340\n",
      "Epoch: 2400 | Loss: 0.00293 | Test Loss: 0.00300 | Valid Loss: 0.00297\n",
      "Epoch: 2600 | Loss: 0.00252 | Test Loss: 0.00258 | Valid Loss: 0.00256\n",
      "Epoch: 2800 | Loss: 0.00214 | Test Loss: 0.00220 | Valid Loss: 0.00218\n",
      "Epoch: 3000 | Loss: 0.00180 | Test Loss: 0.00186 | Valid Loss: 0.00183\n",
      "Epoch: 3200 | Loss: 0.00150 | Test Loss: 0.00155 | Valid Loss: 0.00153\n",
      "Epoch: 3400 | Loss: 0.00125 | Test Loss: 0.00129 | Valid Loss: 0.00128\n",
      "Epoch: 3600 | Loss: 0.00104 | Test Loss: 0.00108 | Valid Loss: 0.00107\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00091\n",
      "Epoch: 4000 | Loss: 0.00077 | Test Loss: 0.00080 | Valid Loss: 0.00079\n",
      "Epoch: 4200 | Loss: 0.00070 | Test Loss: 0.00073 | Valid Loss: 0.00071\n",
      "Epoch: 4400 | Loss: 0.00065 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Epoch: 4600 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00063\n",
      "Epoch: 4800 | Loss: 0.00061 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 5000 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 5200 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5400 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 5600 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 5800 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 6000 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 6400 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 6800 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 2.23150 | Test Loss: 2.17320 | Valid Loss: 2.16780\n",
      "Epoch: 200 | Loss: 0.00681 | Test Loss: 0.00675 | Valid Loss: 0.00690\n",
      "Epoch: 400 | Loss: 0.00657 | Test Loss: 0.00652 | Valid Loss: 0.00666\n",
      "Epoch: 600 | Loss: 0.00627 | Test Loss: 0.00622 | Valid Loss: 0.00636\n",
      "Epoch: 800 | Loss: 0.00592 | Test Loss: 0.00588 | Valid Loss: 0.00601\n",
      "Epoch: 1000 | Loss: 0.00555 | Test Loss: 0.00552 | Valid Loss: 0.00563\n",
      "Epoch: 1200 | Loss: 0.00515 | Test Loss: 0.00513 | Valid Loss: 0.00524\n",
      "Epoch: 1400 | Loss: 0.00474 | Test Loss: 0.00473 | Valid Loss: 0.00483\n",
      "Epoch: 1600 | Loss: 0.00433 | Test Loss: 0.00433 | Valid Loss: 0.00441\n",
      "Epoch: 1800 | Loss: 0.00391 | Test Loss: 0.00392 | Valid Loss: 0.00398\n",
      "Epoch: 2000 | Loss: 0.00349 | Test Loss: 0.00351 | Valid Loss: 0.00356\n",
      "Epoch: 2200 | Loss: 0.00308 | Test Loss: 0.00311 | Valid Loss: 0.00315\n",
      "Epoch: 2400 | Loss: 0.00268 | Test Loss: 0.00271 | Valid Loss: 0.00275\n",
      "Epoch: 2600 | Loss: 0.00230 | Test Loss: 0.00234 | Valid Loss: 0.00236\n",
      "Epoch: 2800 | Loss: 0.00195 | Test Loss: 0.00199 | Valid Loss: 0.00201\n",
      "Epoch: 3000 | Loss: 0.00163 | Test Loss: 0.00168 | Valid Loss: 0.00168\n",
      "Epoch: 3200 | Loss: 0.00136 | Test Loss: 0.00140 | Valid Loss: 0.00140\n",
      "Epoch: 3400 | Loss: 0.00113 | Test Loss: 0.00118 | Valid Loss: 0.00117\n",
      "Epoch: 3600 | Loss: 0.00094 | Test Loss: 0.00099 | Valid Loss: 0.00098\n",
      "Epoch: 3800 | Loss: 0.00081 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 4000 | Loss: 0.00071 | Test Loss: 0.00076 | Valid Loss: 0.00074\n",
      "Epoch: 4200 | Loss: 0.00065 | Test Loss: 0.00070 | Valid Loss: 0.00068\n",
      "Epoch: 4400 | Loss: 0.00062 | Test Loss: 0.00066 | Valid Loss: 0.00064\n",
      "Epoch: 4600 | Loss: 0.00060 | Test Loss: 0.00064 | Valid Loss: 0.00062\n",
      "Epoch: 4800 | Loss: 0.00059 | Test Loss: 0.00063 | Valid Loss: 0.00061\n",
      "Epoch: 5000 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5200 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5400 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 5800 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 6000 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 6200 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 6400 | Loss: 0.00058 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 6600 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 6800 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.31088 | Test Loss: 1.25694 | Valid Loss: 1.26324\n",
      "Epoch: 200 | Loss: 0.00960 | Test Loss: 0.00981 | Valid Loss: 0.00982\n",
      "Epoch: 400 | Loss: 0.00916 | Test Loss: 0.00936 | Valid Loss: 0.00938\n",
      "Epoch: 600 | Loss: 0.00862 | Test Loss: 0.00881 | Valid Loss: 0.00883\n",
      "Epoch: 800 | Loss: 0.00802 | Test Loss: 0.00820 | Valid Loss: 0.00821\n",
      "Epoch: 1000 | Loss: 0.00739 | Test Loss: 0.00755 | Valid Loss: 0.00757\n",
      "Epoch: 1200 | Loss: 0.00675 | Test Loss: 0.00689 | Valid Loss: 0.00691\n",
      "Epoch: 1400 | Loss: 0.00609 | Test Loss: 0.00623 | Valid Loss: 0.00625\n",
      "Epoch: 1600 | Loss: 0.00544 | Test Loss: 0.00556 | Valid Loss: 0.00558\n",
      "Epoch: 1800 | Loss: 0.00480 | Test Loss: 0.00491 | Valid Loss: 0.00493\n",
      "Epoch: 2000 | Loss: 0.00417 | Test Loss: 0.00426 | Valid Loss: 0.00428\n",
      "Epoch: 2200 | Loss: 0.00355 | Test Loss: 0.00363 | Valid Loss: 0.00365\n",
      "Epoch: 2400 | Loss: 0.00296 | Test Loss: 0.00303 | Valid Loss: 0.00305\n",
      "Epoch: 2600 | Loss: 0.00241 | Test Loss: 0.00247 | Valid Loss: 0.00249\n",
      "Epoch: 2800 | Loss: 0.00192 | Test Loss: 0.00196 | Valid Loss: 0.00198\n",
      "Epoch: 3000 | Loss: 0.00150 | Test Loss: 0.00153 | Valid Loss: 0.00155\n",
      "Epoch: 3200 | Loss: 0.00116 | Test Loss: 0.00118 | Valid Loss: 0.00120\n",
      "Epoch: 3400 | Loss: 0.00091 | Test Loss: 0.00092 | Valid Loss: 0.00094\n",
      "Epoch: 3600 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00077\n",
      "Epoch: 3800 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00066\n",
      "Epoch: 4000 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 4200 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 4400 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 5000 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 5200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 5400 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 5600 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 6000 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 6800 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 0.86328 | Test Loss: 0.80768 | Valid Loss: 0.81527\n",
      "Epoch: 200 | Loss: 0.01023 | Test Loss: 0.01009 | Valid Loss: 0.01044\n",
      "Epoch: 400 | Loss: 0.00951 | Test Loss: 0.00937 | Valid Loss: 0.00970\n",
      "Epoch: 600 | Loss: 0.00868 | Test Loss: 0.00855 | Valid Loss: 0.00886\n",
      "Epoch: 800 | Loss: 0.00782 | Test Loss: 0.00770 | Valid Loss: 0.00799\n",
      "Epoch: 1000 | Loss: 0.00699 | Test Loss: 0.00688 | Valid Loss: 0.00714\n",
      "Epoch: 1200 | Loss: 0.00618 | Test Loss: 0.00608 | Valid Loss: 0.00632\n",
      "Epoch: 1400 | Loss: 0.00540 | Test Loss: 0.00531 | Valid Loss: 0.00553\n",
      "Epoch: 1600 | Loss: 0.00463 | Test Loss: 0.00455 | Valid Loss: 0.00475\n",
      "Epoch: 1800 | Loss: 0.00386 | Test Loss: 0.00380 | Valid Loss: 0.00396\n",
      "Epoch: 2000 | Loss: 0.00310 | Test Loss: 0.00305 | Valid Loss: 0.00319\n",
      "Epoch: 2200 | Loss: 0.00238 | Test Loss: 0.00235 | Valid Loss: 0.00246\n",
      "Epoch: 2400 | Loss: 0.00175 | Test Loss: 0.00173 | Valid Loss: 0.00182\n",
      "Epoch: 2600 | Loss: 0.00127 | Test Loss: 0.00126 | Valid Loss: 0.00133\n",
      "Epoch: 2800 | Loss: 0.00095 | Test Loss: 0.00095 | Valid Loss: 0.00100\n",
      "Epoch: 3000 | Loss: 0.00077 | Test Loss: 0.00078 | Valid Loss: 0.00082\n",
      "Epoch: 3200 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00072\n",
      "Epoch: 3400 | Loss: 0.00065 | Test Loss: 0.00066 | Valid Loss: 0.00068\n",
      "Epoch: 3600 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00066\n",
      "Epoch: 3800 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00065\n",
      "Epoch: 4000 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 4200 | Loss: 0.00061 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 4400 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00064\n",
      "Epoch: 4600 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 4800 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 5000 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 5200 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 5400 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 5600 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 5800 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 6000 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 6200 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00063\n",
      "Epoch: 6400 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00063\n",
      "Epoch: 6600 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 6800 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 1.68761 | Test Loss: 1.61425 | Valid Loss: 1.62229\n",
      "Epoch: 200 | Loss: 0.01016 | Test Loss: 0.01017 | Valid Loss: 0.00999\n",
      "Epoch: 400 | Loss: 0.00966 | Test Loss: 0.00967 | Valid Loss: 0.00950\n",
      "Epoch: 600 | Loss: 0.00904 | Test Loss: 0.00905 | Valid Loss: 0.00889\n",
      "Epoch: 800 | Loss: 0.00837 | Test Loss: 0.00837 | Valid Loss: 0.00822\n",
      "Epoch: 1000 | Loss: 0.00766 | Test Loss: 0.00767 | Valid Loss: 0.00753\n",
      "Epoch: 1200 | Loss: 0.00695 | Test Loss: 0.00696 | Valid Loss: 0.00683\n",
      "Epoch: 1400 | Loss: 0.00625 | Test Loss: 0.00626 | Valid Loss: 0.00614\n",
      "Epoch: 1600 | Loss: 0.00557 | Test Loss: 0.00557 | Valid Loss: 0.00547\n",
      "Epoch: 1800 | Loss: 0.00490 | Test Loss: 0.00490 | Valid Loss: 0.00481\n",
      "Epoch: 2000 | Loss: 0.00426 | Test Loss: 0.00426 | Valid Loss: 0.00418\n",
      "Epoch: 2200 | Loss: 0.00364 | Test Loss: 0.00363 | Valid Loss: 0.00357\n",
      "Epoch: 2400 | Loss: 0.00305 | Test Loss: 0.00305 | Valid Loss: 0.00299\n",
      "Epoch: 2600 | Loss: 0.00250 | Test Loss: 0.00250 | Valid Loss: 0.00245\n",
      "Epoch: 2800 | Loss: 0.00201 | Test Loss: 0.00201 | Valid Loss: 0.00197\n",
      "Epoch: 3000 | Loss: 0.00159 | Test Loss: 0.00158 | Valid Loss: 0.00155\n",
      "Epoch: 3200 | Loss: 0.00125 | Test Loss: 0.00124 | Valid Loss: 0.00122\n",
      "Epoch: 3400 | Loss: 0.00099 | Test Loss: 0.00099 | Valid Loss: 0.00097\n",
      "Epoch: 3600 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00080\n",
      "Epoch: 3800 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 4000 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 4200 | Loss: 0.00061 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 4400 | Loss: 0.00060 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 4600 | Loss: 0.00059 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 4800 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 5000 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5200 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5800 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6000 | Loss: 0.00058 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6200 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6400 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 1.01868 | Test Loss: 0.96690 | Valid Loss: 0.97456\n",
      "Epoch: 200 | Loss: 0.01011 | Test Loss: 0.00996 | Valid Loss: 0.01029\n",
      "Epoch: 400 | Loss: 0.00928 | Test Loss: 0.00914 | Valid Loss: 0.00944\n",
      "Epoch: 600 | Loss: 0.00831 | Test Loss: 0.00819 | Valid Loss: 0.00847\n",
      "Epoch: 800 | Loss: 0.00731 | Test Loss: 0.00721 | Valid Loss: 0.00745\n",
      "Epoch: 1000 | Loss: 0.00633 | Test Loss: 0.00624 | Valid Loss: 0.00646\n",
      "Epoch: 1200 | Loss: 0.00540 | Test Loss: 0.00532 | Valid Loss: 0.00552\n",
      "Epoch: 1400 | Loss: 0.00452 | Test Loss: 0.00446 | Valid Loss: 0.00463\n",
      "Epoch: 1600 | Loss: 0.00371 | Test Loss: 0.00365 | Valid Loss: 0.00380\n",
      "Epoch: 1800 | Loss: 0.00296 | Test Loss: 0.00291 | Valid Loss: 0.00303\n",
      "Epoch: 2000 | Loss: 0.00227 | Test Loss: 0.00223 | Valid Loss: 0.00234\n",
      "Epoch: 2200 | Loss: 0.00169 | Test Loss: 0.00165 | Valid Loss: 0.00174\n",
      "Epoch: 2400 | Loss: 0.00123 | Test Loss: 0.00120 | Valid Loss: 0.00126\n",
      "Epoch: 2600 | Loss: 0.00091 | Test Loss: 0.00089 | Valid Loss: 0.00094\n",
      "Epoch: 2800 | Loss: 0.00074 | Test Loss: 0.00071 | Valid Loss: 0.00076\n",
      "Epoch: 3000 | Loss: 0.00065 | Test Loss: 0.00063 | Valid Loss: 0.00067\n",
      "Epoch: 3200 | Loss: 0.00062 | Test Loss: 0.00060 | Valid Loss: 0.00063\n",
      "Epoch: 3400 | Loss: 0.00061 | Test Loss: 0.00059 | Valid Loss: 0.00062\n",
      "Epoch: 3600 | Loss: 0.00061 | Test Loss: 0.00058 | Valid Loss: 0.00062\n",
      "Epoch: 3800 | Loss: 0.00061 | Test Loss: 0.00058 | Valid Loss: 0.00062\n",
      "Epoch: 4000 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 4200 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 4400 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 4600 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 4800 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 5000 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00061\n",
      "Epoch: 5200 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 5400 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 5600 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 5800 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6000 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6200 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6400 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6600 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6800 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00060\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 0.93161 | Test Loss: 0.87537 | Valid Loss: 0.87255\n",
      "Epoch: 200 | Loss: 0.00993 | Test Loss: 0.00982 | Valid Loss: 0.01022\n",
      "Epoch: 400 | Loss: 0.00929 | Test Loss: 0.00918 | Valid Loss: 0.00955\n",
      "Epoch: 600 | Loss: 0.00853 | Test Loss: 0.00844 | Valid Loss: 0.00877\n",
      "Epoch: 800 | Loss: 0.00775 | Test Loss: 0.00766 | Valid Loss: 0.00796\n",
      "Epoch: 1000 | Loss: 0.00697 | Test Loss: 0.00690 | Valid Loss: 0.00716\n",
      "Epoch: 1200 | Loss: 0.00623 | Test Loss: 0.00616 | Valid Loss: 0.00640\n",
      "Epoch: 1400 | Loss: 0.00551 | Test Loss: 0.00546 | Valid Loss: 0.00566\n",
      "Epoch: 1600 | Loss: 0.00483 | Test Loss: 0.00479 | Valid Loss: 0.00496\n",
      "Epoch: 1800 | Loss: 0.00419 | Test Loss: 0.00415 | Valid Loss: 0.00430\n",
      "Epoch: 2000 | Loss: 0.00358 | Test Loss: 0.00355 | Valid Loss: 0.00367\n",
      "Epoch: 2200 | Loss: 0.00302 | Test Loss: 0.00299 | Valid Loss: 0.00309\n",
      "Epoch: 2400 | Loss: 0.00250 | Test Loss: 0.00248 | Valid Loss: 0.00255\n",
      "Epoch: 2600 | Loss: 0.00204 | Test Loss: 0.00202 | Valid Loss: 0.00208\n",
      "Epoch: 2800 | Loss: 0.00165 | Test Loss: 0.00164 | Valid Loss: 0.00168\n",
      "Epoch: 3000 | Loss: 0.00134 | Test Loss: 0.00133 | Valid Loss: 0.00135\n",
      "Epoch: 3200 | Loss: 0.00111 | Test Loss: 0.00110 | Valid Loss: 0.00111\n",
      "Epoch: 3400 | Loss: 0.00095 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 3600 | Loss: 0.00084 | Test Loss: 0.00084 | Valid Loss: 0.00084\n",
      "Epoch: 3800 | Loss: 0.00079 | Test Loss: 0.00078 | Valid Loss: 0.00078\n",
      "Epoch: 4000 | Loss: 0.00075 | Test Loss: 0.00075 | Valid Loss: 0.00074\n",
      "Epoch: 4200 | Loss: 0.00074 | Test Loss: 0.00074 | Valid Loss: 0.00073\n",
      "Epoch: 4400 | Loss: 0.00073 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 4600 | Loss: 0.00073 | Test Loss: 0.00072 | Valid Loss: 0.00072\n",
      "Epoch: 4800 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5000 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5400 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5600 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 5800 | Loss: 0.00072 | Test Loss: 0.00072 | Valid Loss: 0.00071\n",
      "Epoch: 6000 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 6200 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 6400 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 6600 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "Epoch: 6800 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00071\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.12742 | Test Loss: 1.08287 | Valid Loss: 1.07169\n",
      "Epoch: 200 | Loss: 0.00976 | Test Loss: 0.00930 | Valid Loss: 0.00957\n",
      "Epoch: 400 | Loss: 0.00905 | Test Loss: 0.00861 | Valid Loss: 0.00887\n",
      "Epoch: 600 | Loss: 0.00821 | Test Loss: 0.00781 | Valid Loss: 0.00805\n",
      "Epoch: 800 | Loss: 0.00732 | Test Loss: 0.00696 | Valid Loss: 0.00717\n",
      "Epoch: 1000 | Loss: 0.00642 | Test Loss: 0.00611 | Valid Loss: 0.00629\n",
      "Epoch: 1200 | Loss: 0.00557 | Test Loss: 0.00529 | Valid Loss: 0.00545\n",
      "Epoch: 1400 | Loss: 0.00476 | Test Loss: 0.00452 | Valid Loss: 0.00466\n",
      "Epoch: 1600 | Loss: 0.00400 | Test Loss: 0.00379 | Valid Loss: 0.00391\n",
      "Epoch: 1800 | Loss: 0.00328 | Test Loss: 0.00312 | Valid Loss: 0.00321\n",
      "Epoch: 2000 | Loss: 0.00264 | Test Loss: 0.00250 | Valid Loss: 0.00258\n",
      "Epoch: 2200 | Loss: 0.00208 | Test Loss: 0.00198 | Valid Loss: 0.00203\n",
      "Epoch: 2400 | Loss: 0.00163 | Test Loss: 0.00156 | Valid Loss: 0.00159\n",
      "Epoch: 2600 | Loss: 0.00131 | Test Loss: 0.00126 | Valid Loss: 0.00127\n",
      "Epoch: 2800 | Loss: 0.00110 | Test Loss: 0.00106 | Valid Loss: 0.00107\n",
      "Epoch: 3000 | Loss: 0.00099 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 3200 | Loss: 0.00093 | Test Loss: 0.00091 | Valid Loss: 0.00090\n",
      "Epoch: 3400 | Loss: 0.00091 | Test Loss: 0.00089 | Valid Loss: 0.00088\n",
      "Epoch: 3600 | Loss: 0.00090 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4000 | Loss: 0.00089 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4200 | Loss: 0.00089 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 4400 | Loss: 0.00089 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 4600 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 4800 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 5000 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5200 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5400 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5600 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5800 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6000 | Loss: 0.00088 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 6200 | Loss: 0.00088 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 6400 | Loss: 0.00087 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 6600 | Loss: 0.00087 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 6800 | Loss: 0.00087 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 7000\n",
    "learning_rate = 0.001\n",
    "momentum = 1.2\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_MT_stacking_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_MT_stack_train[:, num, :]).squeeze() # беру результаты сетей для num столбца\n",
    "                                                                # по смыслу - результаты G M на num координате слоя\n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_MT_stack_valid[:, num, :]).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_MT_stack_test[:, num, :]).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4223719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 5.4873 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_MT_stacking_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_MT_stack_test[:, num, :]).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_MT_stack_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_MT_stack_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094dea45",
   "metadata": {},
   "source": [
    "# Stacking - M + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "03be3474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim_1 = 31\n",
    "input_dim_2 = 62\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "num_methods = 2\n",
    "\n",
    "meta_input_dim = 10\n",
    "\n",
    "num_models = 15\n",
    "M_MT_stacking_models = []\n",
    "    \n",
    "for i in range(num_models):\n",
    "    M_MT_stacking_models.append(Initial_Model_V0(meta_input_dim, hidden_units, output_dim).to(device))\n",
    "    \n",
    "next(M_MT_stacking_models[1].parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bebc9044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 31])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_M_train, y_train = X_M_train.to(device), y_train.to(device)\n",
    "X_M_test, y_test = X_M_test.to(device), y_test.to(device)\n",
    "X_M_valid, y_valid = X_M_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_MT_train = X_MT_train.to(device)\n",
    "X_MT_test = X_MT_test.to(device)\n",
    "X_MT_valid = X_MT_valid.to(device)\n",
    "\n",
    "X_G_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c7f21beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 15, 10])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_NN_in_set = 5\n",
    "\n",
    "# Training\n",
    "\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_train)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_train)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_M_MT_stack_train = torch.cat((torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_M_MT_stack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6c083924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 15, 10])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_valid)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_valid)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_M_MT_stack_valid = torch.cat((torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_M_MT_stack_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "84a18cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 15, 10])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation\n",
    "\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_test)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_test)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_M_MT_stack_test = torch.cat((torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_M_MT_stack_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a9edcf44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 2.12216 | Test Loss: 1.98513 | Valid Loss: 1.96351\n",
      "Epoch: 200 | Loss: 0.00815 | Test Loss: 0.00826 | Valid Loss: 0.00814\n",
      "Epoch: 400 | Loss: 0.00709 | Test Loss: 0.00719 | Valid Loss: 0.00707\n",
      "Epoch: 600 | Loss: 0.00596 | Test Loss: 0.00604 | Valid Loss: 0.00594\n",
      "Epoch: 800 | Loss: 0.00489 | Test Loss: 0.00495 | Valid Loss: 0.00486\n",
      "Epoch: 1000 | Loss: 0.00393 | Test Loss: 0.00399 | Valid Loss: 0.00390\n",
      "Epoch: 1200 | Loss: 0.00313 | Test Loss: 0.00317 | Valid Loss: 0.00309\n",
      "Epoch: 1400 | Loss: 0.00248 | Test Loss: 0.00252 | Valid Loss: 0.00245\n",
      "Epoch: 1600 | Loss: 0.00199 | Test Loss: 0.00203 | Valid Loss: 0.00196\n",
      "Epoch: 1800 | Loss: 0.00165 | Test Loss: 0.00169 | Valid Loss: 0.00161\n",
      "Epoch: 2000 | Loss: 0.00143 | Test Loss: 0.00147 | Valid Loss: 0.00139\n",
      "Epoch: 2200 | Loss: 0.00130 | Test Loss: 0.00135 | Valid Loss: 0.00127\n",
      "Epoch: 2400 | Loss: 0.00123 | Test Loss: 0.00128 | Valid Loss: 0.00120\n",
      "Epoch: 2600 | Loss: 0.00120 | Test Loss: 0.00125 | Valid Loss: 0.00116\n",
      "Epoch: 2800 | Loss: 0.00118 | Test Loss: 0.00123 | Valid Loss: 0.00115\n",
      "Epoch: 3000 | Loss: 0.00117 | Test Loss: 0.00123 | Valid Loss: 0.00114\n",
      "Epoch: 3200 | Loss: 0.00117 | Test Loss: 0.00122 | Valid Loss: 0.00113\n",
      "Epoch: 3400 | Loss: 0.00117 | Test Loss: 0.00122 | Valid Loss: 0.00113\n",
      "Epoch: 3600 | Loss: 0.00116 | Test Loss: 0.00122 | Valid Loss: 0.00113\n",
      "Epoch: 3800 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00113\n",
      "Epoch: 4000 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00112\n",
      "Epoch: 4200 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00112\n",
      "Epoch: 4400 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00112\n",
      "Epoch: 4600 | Loss: 0.00115 | Test Loss: 0.00121 | Valid Loss: 0.00112\n",
      "Epoch: 4800 | Loss: 0.00115 | Test Loss: 0.00121 | Valid Loss: 0.00112\n",
      "Epoch: 5000 | Loss: 0.00115 | Test Loss: 0.00120 | Valid Loss: 0.00111\n",
      "Epoch: 5200 | Loss: 0.00115 | Test Loss: 0.00120 | Valid Loss: 0.00111\n",
      "Epoch: 5400 | Loss: 0.00115 | Test Loss: 0.00120 | Valid Loss: 0.00111\n",
      "Epoch: 5600 | Loss: 0.00114 | Test Loss: 0.00120 | Valid Loss: 0.00111\n",
      "Epoch: 5800 | Loss: 0.00114 | Test Loss: 0.00120 | Valid Loss: 0.00111\n",
      "Epoch: 6000 | Loss: 0.00114 | Test Loss: 0.00119 | Valid Loss: 0.00110\n",
      "Epoch: 6200 | Loss: 0.00114 | Test Loss: 0.00119 | Valid Loss: 0.00110\n",
      "Epoch: 6400 | Loss: 0.00114 | Test Loss: 0.00119 | Valid Loss: 0.00110\n",
      "Epoch: 6600 | Loss: 0.00113 | Test Loss: 0.00119 | Valid Loss: 0.00110\n",
      "Epoch: 6800 | Loss: 0.00113 | Test Loss: 0.00118 | Valid Loss: 0.00110\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.48287 | Test Loss: 1.36401 | Valid Loss: 1.34382\n",
      "Epoch: 200 | Loss: 0.01024 | Test Loss: 0.01059 | Valid Loss: 0.01014\n",
      "Epoch: 400 | Loss: 0.00863 | Test Loss: 0.00890 | Valid Loss: 0.00854\n",
      "Epoch: 600 | Loss: 0.00694 | Test Loss: 0.00714 | Valid Loss: 0.00686\n",
      "Epoch: 800 | Loss: 0.00542 | Test Loss: 0.00556 | Valid Loss: 0.00536\n",
      "Epoch: 1000 | Loss: 0.00417 | Test Loss: 0.00427 | Valid Loss: 0.00412\n",
      "Epoch: 1200 | Loss: 0.00318 | Test Loss: 0.00324 | Valid Loss: 0.00314\n",
      "Epoch: 1400 | Loss: 0.00242 | Test Loss: 0.00245 | Valid Loss: 0.00239\n",
      "Epoch: 1600 | Loss: 0.00187 | Test Loss: 0.00188 | Valid Loss: 0.00184\n",
      "Epoch: 1800 | Loss: 0.00150 | Test Loss: 0.00150 | Valid Loss: 0.00148\n",
      "Epoch: 2000 | Loss: 0.00128 | Test Loss: 0.00129 | Valid Loss: 0.00127\n",
      "Epoch: 2200 | Loss: 0.00118 | Test Loss: 0.00119 | Valid Loss: 0.00116\n",
      "Epoch: 2400 | Loss: 0.00114 | Test Loss: 0.00114 | Valid Loss: 0.00112\n",
      "Epoch: 2600 | Loss: 0.00112 | Test Loss: 0.00113 | Valid Loss: 0.00111\n",
      "Epoch: 2800 | Loss: 0.00111 | Test Loss: 0.00113 | Valid Loss: 0.00110\n",
      "Epoch: 3000 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 3200 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 3400 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 3600 | Loss: 0.00111 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 3800 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 4000 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 4200 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 4400 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 4600 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 4800 | Loss: 0.00110 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 5000 | Loss: 0.00109 | Test Loss: 0.00111 | Valid Loss: 0.00109\n",
      "Epoch: 5200 | Loss: 0.00109 | Test Loss: 0.00111 | Valid Loss: 0.00108\n",
      "Epoch: 5400 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 5600 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 5800 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 6000 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 6200 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00108\n",
      "Epoch: 6400 | Loss: 0.00108 | Test Loss: 0.00110 | Valid Loss: 0.00107\n",
      "Epoch: 6600 | Loss: 0.00108 | Test Loss: 0.00109 | Valid Loss: 0.00107\n",
      "Epoch: 6800 | Loss: 0.00108 | Test Loss: 0.00109 | Valid Loss: 0.00107\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 1.30229 | Test Loss: 1.18649 | Valid Loss: 1.17810\n",
      "Epoch: 200 | Loss: 0.00957 | Test Loss: 0.00953 | Valid Loss: 0.00960\n",
      "Epoch: 400 | Loss: 0.00835 | Test Loss: 0.00830 | Valid Loss: 0.00837\n",
      "Epoch: 600 | Loss: 0.00707 | Test Loss: 0.00702 | Valid Loss: 0.00709\n",
      "Epoch: 800 | Loss: 0.00586 | Test Loss: 0.00581 | Valid Loss: 0.00587\n",
      "Epoch: 1000 | Loss: 0.00474 | Test Loss: 0.00469 | Valid Loss: 0.00475\n",
      "Epoch: 1200 | Loss: 0.00372 | Test Loss: 0.00368 | Valid Loss: 0.00372\n",
      "Epoch: 1400 | Loss: 0.00283 | Test Loss: 0.00280 | Valid Loss: 0.00283\n",
      "Epoch: 1600 | Loss: 0.00213 | Test Loss: 0.00211 | Valid Loss: 0.00213\n",
      "Epoch: 1800 | Loss: 0.00164 | Test Loss: 0.00163 | Valid Loss: 0.00163\n",
      "Epoch: 2000 | Loss: 0.00134 | Test Loss: 0.00135 | Valid Loss: 0.00134\n",
      "Epoch: 2200 | Loss: 0.00120 | Test Loss: 0.00122 | Valid Loss: 0.00119\n",
      "Epoch: 2400 | Loss: 0.00114 | Test Loss: 0.00117 | Valid Loss: 0.00113\n",
      "Epoch: 2600 | Loss: 0.00112 | Test Loss: 0.00115 | Valid Loss: 0.00111\n",
      "Epoch: 2800 | Loss: 0.00111 | Test Loss: 0.00115 | Valid Loss: 0.00110\n",
      "Epoch: 3000 | Loss: 0.00111 | Test Loss: 0.00115 | Valid Loss: 0.00110\n",
      "Epoch: 3200 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00110\n",
      "Epoch: 3400 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 3600 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 3800 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 4000 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 4200 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 4400 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00109\n",
      "Epoch: 4600 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00108\n",
      "Epoch: 4800 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00108\n",
      "Epoch: 5000 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00108\n",
      "Epoch: 5200 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00108\n",
      "Epoch: 5400 | Loss: 0.00109 | Test Loss: 0.00113 | Valid Loss: 0.00108\n",
      "Epoch: 5600 | Loss: 0.00109 | Test Loss: 0.00112 | Valid Loss: 0.00108\n",
      "Epoch: 5800 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00107\n",
      "Epoch: 6000 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00107\n",
      "Epoch: 6200 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00107\n",
      "Epoch: 6400 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00107\n",
      "Epoch: 6600 | Loss: 0.00108 | Test Loss: 0.00112 | Valid Loss: 0.00107\n",
      "Epoch: 6800 | Loss: 0.00107 | Test Loss: 0.00111 | Valid Loss: 0.00106\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 0.81237 | Test Loss: 0.72355 | Valid Loss: 0.72150\n",
      "Epoch: 200 | Loss: 0.00892 | Test Loss: 0.00920 | Valid Loss: 0.00898\n",
      "Epoch: 400 | Loss: 0.00724 | Test Loss: 0.00746 | Valid Loss: 0.00729\n",
      "Epoch: 600 | Loss: 0.00564 | Test Loss: 0.00581 | Valid Loss: 0.00567\n",
      "Epoch: 800 | Loss: 0.00428 | Test Loss: 0.00440 | Valid Loss: 0.00430\n",
      "Epoch: 1000 | Loss: 0.00319 | Test Loss: 0.00326 | Valid Loss: 0.00319\n",
      "Epoch: 1200 | Loss: 0.00233 | Test Loss: 0.00238 | Valid Loss: 0.00233\n",
      "Epoch: 1400 | Loss: 0.00173 | Test Loss: 0.00175 | Valid Loss: 0.00172\n",
      "Epoch: 1600 | Loss: 0.00135 | Test Loss: 0.00136 | Valid Loss: 0.00135\n",
      "Epoch: 1800 | Loss: 0.00117 | Test Loss: 0.00117 | Valid Loss: 0.00116\n",
      "Epoch: 2000 | Loss: 0.00109 | Test Loss: 0.00109 | Valid Loss: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00106 | Test Loss: 0.00107 | Valid Loss: 0.00107\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00106 | Valid Loss: 0.00106\n",
      "Epoch: 2600 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00106\n",
      "Epoch: 2800 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 3000 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 3200 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 3400 | Loss: 0.00105 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 3600 | Loss: 0.00104 | Test Loss: 0.00105 | Valid Loss: 0.00105\n",
      "Epoch: 3800 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00105\n",
      "Epoch: 4000 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00105\n",
      "Epoch: 4200 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00104\n",
      "Epoch: 4400 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00104\n",
      "Epoch: 4600 | Loss: 0.00104 | Test Loss: 0.00104 | Valid Loss: 0.00104\n",
      "Epoch: 4800 | Loss: 0.00103 | Test Loss: 0.00104 | Valid Loss: 0.00104\n",
      "Epoch: 5000 | Loss: 0.00103 | Test Loss: 0.00104 | Valid Loss: 0.00104\n",
      "Epoch: 5200 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00104\n",
      "Epoch: 5400 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 5600 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 5800 | Loss: 0.00103 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 6000 | Loss: 0.00102 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 6200 | Loss: 0.00102 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "Epoch: 6400 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 6600 | Loss: 0.00102 | Test Loss: 0.00102 | Valid Loss: 0.00103\n",
      "Epoch: 6800 | Loss: 0.00104 | Test Loss: 0.00103 | Valid Loss: 0.00103\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 1.43738 | Test Loss: 1.32736 | Valid Loss: 1.32949\n",
      "Epoch: 200 | Loss: 0.01048 | Test Loss: 0.01052 | Valid Loss: 0.01074\n",
      "Epoch: 400 | Loss: 0.00900 | Test Loss: 0.00903 | Valid Loss: 0.00923\n",
      "Epoch: 600 | Loss: 0.00745 | Test Loss: 0.00747 | Valid Loss: 0.00765\n",
      "Epoch: 800 | Loss: 0.00603 | Test Loss: 0.00604 | Valid Loss: 0.00619\n",
      "Epoch: 1000 | Loss: 0.00478 | Test Loss: 0.00477 | Valid Loss: 0.00491\n",
      "Epoch: 1200 | Loss: 0.00366 | Test Loss: 0.00365 | Valid Loss: 0.00377\n",
      "Epoch: 1400 | Loss: 0.00267 | Test Loss: 0.00265 | Valid Loss: 0.00275\n",
      "Epoch: 1600 | Loss: 0.00186 | Test Loss: 0.00184 | Valid Loss: 0.00192\n",
      "Epoch: 1800 | Loss: 0.00134 | Test Loss: 0.00131 | Valid Loss: 0.00138\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00107 | Valid Loss: 0.00113\n",
      "Epoch: 2200 | Loss: 0.00101 | Test Loss: 0.00099 | Valid Loss: 0.00104\n",
      "Epoch: 2400 | Loss: 0.00099 | Test Loss: 0.00096 | Valid Loss: 0.00102\n",
      "Epoch: 2600 | Loss: 0.00098 | Test Loss: 0.00095 | Valid Loss: 0.00101\n",
      "Epoch: 2800 | Loss: 0.00098 | Test Loss: 0.00095 | Valid Loss: 0.00100\n",
      "Epoch: 3000 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00100\n",
      "Epoch: 3200 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 3400 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 3600 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 3800 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 4000 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 4200 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00099\n",
      "Epoch: 4400 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00098\n",
      "Epoch: 4600 | Loss: 0.00096 | Test Loss: 0.00093 | Valid Loss: 0.00098\n",
      "Epoch: 4800 | Loss: 0.00096 | Test Loss: 0.00093 | Valid Loss: 0.00098\n",
      "Epoch: 5000 | Loss: 0.00096 | Test Loss: 0.00093 | Valid Loss: 0.00098\n",
      "Epoch: 5200 | Loss: 0.00096 | Test Loss: 0.00093 | Valid Loss: 0.00098\n",
      "Epoch: 5400 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00098\n",
      "Epoch: 5600 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00097\n",
      "Epoch: 5800 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00097\n",
      "Epoch: 6000 | Loss: 0.00095 | Test Loss: 0.00092 | Valid Loss: 0.00097\n",
      "Epoch: 6200 | Loss: 0.00095 | Test Loss: 0.00092 | Valid Loss: 0.00097\n",
      "Epoch: 6400 | Loss: 0.00095 | Test Loss: 0.00092 | Valid Loss: 0.00097\n",
      "Epoch: 6600 | Loss: 0.00094 | Test Loss: 0.00092 | Valid Loss: 0.00096\n",
      "Epoch: 6800 | Loss: 0.00094 | Test Loss: 0.00092 | Valid Loss: 0.00096\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 0.82619 | Test Loss: 0.74541 | Valid Loss: 0.74236\n",
      "Epoch: 200 | Loss: 0.01029 | Test Loss: 0.01056 | Valid Loss: 0.01041\n",
      "Epoch: 400 | Loss: 0.00839 | Test Loss: 0.00860 | Valid Loss: 0.00850\n",
      "Epoch: 600 | Loss: 0.00655 | Test Loss: 0.00672 | Valid Loss: 0.00665\n",
      "Epoch: 800 | Loss: 0.00492 | Test Loss: 0.00504 | Valid Loss: 0.00500\n",
      "Epoch: 1000 | Loss: 0.00351 | Test Loss: 0.00359 | Valid Loss: 0.00359\n",
      "Epoch: 1200 | Loss: 0.00238 | Test Loss: 0.00243 | Valid Loss: 0.00244\n",
      "Epoch: 1400 | Loss: 0.00160 | Test Loss: 0.00163 | Valid Loss: 0.00164\n",
      "Epoch: 1600 | Loss: 0.00116 | Test Loss: 0.00118 | Valid Loss: 0.00119\n",
      "Epoch: 1800 | Loss: 0.00098 | Test Loss: 0.00099 | Valid Loss: 0.00101\n",
      "Epoch: 2000 | Loss: 0.00093 | Test Loss: 0.00093 | Valid Loss: 0.00095\n",
      "Epoch: 2200 | Loss: 0.00092 | Test Loss: 0.00092 | Valid Loss: 0.00093\n",
      "Epoch: 2400 | Loss: 0.00091 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "Epoch: 2600 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00092\n",
      "Epoch: 2800 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00092\n",
      "Epoch: 3000 | Loss: 0.00091 | Test Loss: 0.00091 | Valid Loss: 0.00092\n",
      "Epoch: 3200 | Loss: 0.00090 | Test Loss: 0.00091 | Valid Loss: 0.00091\n",
      "Epoch: 3400 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00091\n",
      "Epoch: 3600 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00091\n",
      "Epoch: 3800 | Loss: 0.00090 | Test Loss: 0.00090 | Valid Loss: 0.00091\n",
      "Epoch: 4000 | Loss: 0.00089 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 4200 | Loss: 0.00089 | Test Loss: 0.00090 | Valid Loss: 0.00090\n",
      "Epoch: 4400 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 4600 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 4800 | Loss: 0.00089 | Test Loss: 0.00089 | Valid Loss: 0.00090\n",
      "Epoch: 5000 | Loss: 0.00088 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 5200 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 5400 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 5600 | Loss: 0.00088 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 5800 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00088\n",
      "Epoch: 6000 | Loss: 0.00087 | Test Loss: 0.00087 | Valid Loss: 0.00088\n",
      "Epoch: 6200 | Loss: 0.00087 | Test Loss: 0.00087 | Valid Loss: 0.00088\n",
      "Epoch: 6400 | Loss: 0.00090 | Test Loss: 0.00091 | Valid Loss: 0.00091\n",
      "Early stopping at epoch: 6520\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 0.96651 | Test Loss: 0.87982 | Valid Loss: 0.86915\n",
      "Epoch: 200 | Loss: 0.00820 | Test Loss: 0.00844 | Valid Loss: 0.00823\n",
      "Epoch: 400 | Loss: 0.00681 | Test Loss: 0.00700 | Valid Loss: 0.00683\n",
      "Epoch: 600 | Loss: 0.00542 | Test Loss: 0.00556 | Valid Loss: 0.00543\n",
      "Epoch: 800 | Loss: 0.00415 | Test Loss: 0.00425 | Valid Loss: 0.00416\n",
      "Epoch: 1000 | Loss: 0.00304 | Test Loss: 0.00311 | Valid Loss: 0.00304\n",
      "Epoch: 1200 | Loss: 0.00213 | Test Loss: 0.00217 | Valid Loss: 0.00213\n",
      "Epoch: 1400 | Loss: 0.00147 | Test Loss: 0.00151 | Valid Loss: 0.00148\n",
      "Epoch: 1600 | Loss: 0.00110 | Test Loss: 0.00112 | Valid Loss: 0.00110\n",
      "Epoch: 1800 | Loss: 0.00094 | Test Loss: 0.00096 | Valid Loss: 0.00094\n",
      "Epoch: 2000 | Loss: 0.00089 | Test Loss: 0.00091 | Valid Loss: 0.00089\n",
      "Epoch: 2200 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 2400 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 2600 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 2800 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 3000 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 3200 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 3400 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 3600 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 3800 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4000 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4200 | Loss: 0.00085 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 4400 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 4600 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 4800 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5000 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5200 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 5400 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 5600 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 5800 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 6000 | Loss: 0.00083 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 6400 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 6600 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Early stopping at epoch: 6738\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 2.74960 | Test Loss: 2.61381 | Valid Loss: 2.60600\n",
      "Epoch: 200 | Loss: 0.00799 | Test Loss: 0.00811 | Valid Loss: 0.00803\n",
      "Epoch: 400 | Loss: 0.00718 | Test Loss: 0.00730 | Valid Loss: 0.00722\n",
      "Epoch: 600 | Loss: 0.00627 | Test Loss: 0.00638 | Valid Loss: 0.00631\n",
      "Epoch: 800 | Loss: 0.00536 | Test Loss: 0.00546 | Valid Loss: 0.00540\n",
      "Epoch: 1000 | Loss: 0.00451 | Test Loss: 0.00460 | Valid Loss: 0.00455\n",
      "Epoch: 1200 | Loss: 0.00373 | Test Loss: 0.00382 | Valid Loss: 0.00377\n",
      "Epoch: 1400 | Loss: 0.00304 | Test Loss: 0.00313 | Valid Loss: 0.00308\n",
      "Epoch: 1600 | Loss: 0.00245 | Test Loss: 0.00253 | Valid Loss: 0.00249\n",
      "Epoch: 1800 | Loss: 0.00195 | Test Loss: 0.00203 | Valid Loss: 0.00199\n",
      "Epoch: 2000 | Loss: 0.00156 | Test Loss: 0.00163 | Valid Loss: 0.00159\n",
      "Epoch: 2200 | Loss: 0.00126 | Test Loss: 0.00133 | Valid Loss: 0.00129\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00112 | Valid Loss: 0.00109\n",
      "Epoch: 2600 | Loss: 0.00094 | Test Loss: 0.00099 | Valid Loss: 0.00096\n",
      "Epoch: 2800 | Loss: 0.00087 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 3000 | Loss: 0.00084 | Test Loss: 0.00089 | Valid Loss: 0.00085\n",
      "Epoch: 3200 | Loss: 0.00083 | Test Loss: 0.00087 | Valid Loss: 0.00084\n",
      "Epoch: 3400 | Loss: 0.00082 | Test Loss: 0.00087 | Valid Loss: 0.00083\n",
      "Epoch: 3600 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00083\n",
      "Epoch: 3800 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00083\n",
      "Epoch: 4000 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00083\n",
      "Epoch: 4200 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 4400 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 4600 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 4800 | Loss: 0.00082 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 5000 | Loss: 0.00081 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 5200 | Loss: 0.00081 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 5400 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00082\n",
      "Epoch: 5600 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00082\n",
      "Epoch: 5800 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00082\n",
      "Epoch: 6000 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 6200 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 6400 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 6600 | Loss: 0.00081 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 6800 | Loss: 0.00080 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 2.23160 | Test Loss: 2.10014 | Valid Loss: 2.09471\n",
      "Epoch: 200 | Loss: 0.00693 | Test Loss: 0.00689 | Valid Loss: 0.00704\n",
      "Epoch: 400 | Loss: 0.00630 | Test Loss: 0.00626 | Valid Loss: 0.00640\n",
      "Epoch: 600 | Loss: 0.00558 | Test Loss: 0.00557 | Valid Loss: 0.00568\n",
      "Epoch: 800 | Loss: 0.00486 | Test Loss: 0.00486 | Valid Loss: 0.00495\n",
      "Epoch: 1000 | Loss: 0.00416 | Test Loss: 0.00418 | Valid Loss: 0.00425\n",
      "Epoch: 1200 | Loss: 0.00350 | Test Loss: 0.00353 | Valid Loss: 0.00358\n",
      "Epoch: 1400 | Loss: 0.00289 | Test Loss: 0.00293 | Valid Loss: 0.00296\n",
      "Epoch: 1600 | Loss: 0.00234 | Test Loss: 0.00239 | Valid Loss: 0.00241\n",
      "Epoch: 1800 | Loss: 0.00188 | Test Loss: 0.00194 | Valid Loss: 0.00194\n",
      "Epoch: 2000 | Loss: 0.00152 | Test Loss: 0.00158 | Valid Loss: 0.00157\n",
      "Epoch: 2200 | Loss: 0.00125 | Test Loss: 0.00131 | Valid Loss: 0.00129\n",
      "Epoch: 2400 | Loss: 0.00108 | Test Loss: 0.00113 | Valid Loss: 0.00111\n",
      "Epoch: 2600 | Loss: 0.00098 | Test Loss: 0.00102 | Valid Loss: 0.00100\n",
      "Epoch: 2800 | Loss: 0.00092 | Test Loss: 0.00097 | Valid Loss: 0.00094\n",
      "Epoch: 3000 | Loss: 0.00090 | Test Loss: 0.00094 | Valid Loss: 0.00091\n",
      "Epoch: 3200 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00090\n",
      "Epoch: 3400 | Loss: 0.00088 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 3600 | Loss: 0.00088 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 3800 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00089\n",
      "Epoch: 4000 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00089\n",
      "Epoch: 4200 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 4400 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 4600 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 4800 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 5000 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 5200 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00088\n",
      "Epoch: 5400 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 5600 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 5800 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6000 | Loss: 0.00086 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6200 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 6400 | Loss: 0.00086 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 6600 | Loss: 0.00085 | Test Loss: 0.00089 | Valid Loss: 0.00086\n",
      "Epoch: 6800 | Loss: 0.00085 | Test Loss: 0.00089 | Valid Loss: 0.00086\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 1.31085 | Test Loss: 1.20197 | Valid Loss: 1.20812\n",
      "Epoch: 200 | Loss: 0.00941 | Test Loss: 0.00962 | Valid Loss: 0.00963\n",
      "Epoch: 400 | Loss: 0.00834 | Test Loss: 0.00853 | Valid Loss: 0.00854\n",
      "Epoch: 600 | Loss: 0.00719 | Test Loss: 0.00737 | Valid Loss: 0.00737\n",
      "Epoch: 800 | Loss: 0.00607 | Test Loss: 0.00623 | Valid Loss: 0.00622\n",
      "Epoch: 1000 | Loss: 0.00501 | Test Loss: 0.00514 | Valid Loss: 0.00514\n",
      "Epoch: 1200 | Loss: 0.00401 | Test Loss: 0.00412 | Valid Loss: 0.00412\n",
      "Epoch: 1400 | Loss: 0.00310 | Test Loss: 0.00319 | Valid Loss: 0.00318\n",
      "Epoch: 1600 | Loss: 0.00231 | Test Loss: 0.00238 | Valid Loss: 0.00237\n",
      "Epoch: 1800 | Loss: 0.00169 | Test Loss: 0.00175 | Valid Loss: 0.00174\n",
      "Epoch: 2000 | Loss: 0.00127 | Test Loss: 0.00132 | Valid Loss: 0.00131\n",
      "Epoch: 2200 | Loss: 0.00103 | Test Loss: 0.00107 | Valid Loss: 0.00106\n",
      "Epoch: 2400 | Loss: 0.00092 | Test Loss: 0.00095 | Valid Loss: 0.00094\n",
      "Epoch: 2600 | Loss: 0.00088 | Test Loss: 0.00090 | Valid Loss: 0.00089\n",
      "Epoch: 2800 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 3000 | Loss: 0.00086 | Test Loss: 0.00088 | Valid Loss: 0.00086\n",
      "Epoch: 3200 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 3400 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 3600 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 3800 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00086\n",
      "Epoch: 4000 | Loss: 0.00085 | Test Loss: 0.00087 | Valid Loss: 0.00085\n",
      "Epoch: 4200 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 4400 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 4600 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 4800 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 5000 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 5200 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 5400 | Loss: 0.00084 | Test Loss: 0.00086 | Valid Loss: 0.00084\n",
      "Epoch: 5600 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 5800 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 6000 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 6200 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00084\n",
      "Epoch: 6400 | Loss: 0.00083 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 6600 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "Epoch: 6800 | Loss: 0.00082 | Test Loss: 0.00084 | Valid Loss: 0.00083\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 0.86367 | Test Loss: 0.76021 | Valid Loss: 0.76772\n",
      "Epoch: 200 | Loss: 0.00975 | Test Loss: 0.00961 | Valid Loss: 0.00994\n",
      "Epoch: 400 | Loss: 0.00824 | Test Loss: 0.00812 | Valid Loss: 0.00841\n",
      "Epoch: 600 | Loss: 0.00678 | Test Loss: 0.00669 | Valid Loss: 0.00694\n",
      "Epoch: 800 | Loss: 0.00545 | Test Loss: 0.00537 | Valid Loss: 0.00558\n",
      "Epoch: 1000 | Loss: 0.00418 | Test Loss: 0.00413 | Valid Loss: 0.00429\n",
      "Epoch: 1200 | Loss: 0.00300 | Test Loss: 0.00297 | Valid Loss: 0.00308\n",
      "Epoch: 1400 | Loss: 0.00201 | Test Loss: 0.00201 | Valid Loss: 0.00207\n",
      "Epoch: 1600 | Loss: 0.00137 | Test Loss: 0.00138 | Valid Loss: 0.00141\n",
      "Epoch: 1800 | Loss: 0.00106 | Test Loss: 0.00109 | Valid Loss: 0.00109\n",
      "Epoch: 2000 | Loss: 0.00095 | Test Loss: 0.00098 | Valid Loss: 0.00097\n",
      "Epoch: 2200 | Loss: 0.00092 | Test Loss: 0.00095 | Valid Loss: 0.00093\n",
      "Epoch: 2400 | Loss: 0.00091 | Test Loss: 0.00095 | Valid Loss: 0.00092\n",
      "Epoch: 2600 | Loss: 0.00091 | Test Loss: 0.00094 | Valid Loss: 0.00091\n",
      "Epoch: 2800 | Loss: 0.00090 | Test Loss: 0.00094 | Valid Loss: 0.00091\n",
      "Epoch: 3000 | Loss: 0.00090 | Test Loss: 0.00094 | Valid Loss: 0.00090\n",
      "Epoch: 3200 | Loss: 0.00090 | Test Loss: 0.00093 | Valid Loss: 0.00090\n",
      "Epoch: 3400 | Loss: 0.00090 | Test Loss: 0.00093 | Valid Loss: 0.00090\n",
      "Epoch: 3600 | Loss: 0.00090 | Test Loss: 0.00093 | Valid Loss: 0.00090\n",
      "Epoch: 3800 | Loss: 0.00089 | Test Loss: 0.00093 | Valid Loss: 0.00089\n",
      "Epoch: 4000 | Loss: 0.00089 | Test Loss: 0.00093 | Valid Loss: 0.00089\n",
      "Epoch: 4200 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 4400 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 4600 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 4800 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00089\n",
      "Epoch: 5000 | Loss: 0.00088 | Test Loss: 0.00092 | Valid Loss: 0.00088\n",
      "Epoch: 5200 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 5400 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 5600 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 5800 | Loss: 0.00088 | Test Loss: 0.00091 | Valid Loss: 0.00088\n",
      "Epoch: 6000 | Loss: 0.00087 | Test Loss: 0.00091 | Valid Loss: 0.00087\n",
      "Epoch: 6200 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6400 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6600 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Epoch: 6800 | Loss: 0.00087 | Test Loss: 0.00090 | Valid Loss: 0.00087\n",
      "Early stopping at epoch: 6905\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 1.68760 | Test Loss: 1.55184 | Valid Loss: 1.55982\n",
      "Epoch: 200 | Loss: 0.00999 | Test Loss: 0.01000 | Valid Loss: 0.00983\n",
      "Epoch: 400 | Loss: 0.00882 | Test Loss: 0.00882 | Valid Loss: 0.00867\n",
      "Epoch: 600 | Loss: 0.00758 | Test Loss: 0.00758 | Valid Loss: 0.00744\n",
      "Epoch: 800 | Loss: 0.00639 | Test Loss: 0.00639 | Valid Loss: 0.00628\n",
      "Epoch: 1000 | Loss: 0.00531 | Test Loss: 0.00530 | Valid Loss: 0.00521\n",
      "Epoch: 1200 | Loss: 0.00430 | Test Loss: 0.00430 | Valid Loss: 0.00422\n",
      "Epoch: 1400 | Loss: 0.00340 | Test Loss: 0.00339 | Valid Loss: 0.00333\n",
      "Epoch: 1600 | Loss: 0.00262 | Test Loss: 0.00261 | Valid Loss: 0.00256\n",
      "Epoch: 1800 | Loss: 0.00199 | Test Loss: 0.00198 | Valid Loss: 0.00195\n",
      "Epoch: 2000 | Loss: 0.00154 | Test Loss: 0.00153 | Valid Loss: 0.00151\n",
      "Epoch: 2200 | Loss: 0.00125 | Test Loss: 0.00124 | Valid Loss: 0.00123\n",
      "Epoch: 2400 | Loss: 0.00110 | Test Loss: 0.00108 | Valid Loss: 0.00108\n",
      "Epoch: 2600 | Loss: 0.00103 | Test Loss: 0.00101 | Valid Loss: 0.00101\n",
      "Epoch: 2800 | Loss: 0.00099 | Test Loss: 0.00098 | Valid Loss: 0.00098\n",
      "Epoch: 3000 | Loss: 0.00098 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 3200 | Loss: 0.00098 | Test Loss: 0.00096 | Valid Loss: 0.00096\n",
      "Epoch: 3400 | Loss: 0.00097 | Test Loss: 0.00095 | Valid Loss: 0.00095\n",
      "Epoch: 3600 | Loss: 0.00097 | Test Loss: 0.00095 | Valid Loss: 0.00095\n",
      "Epoch: 3800 | Loss: 0.00097 | Test Loss: 0.00095 | Valid Loss: 0.00095\n",
      "Epoch: 4000 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00095\n",
      "Epoch: 4200 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00095\n",
      "Epoch: 4400 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 4600 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 4800 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 5000 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 5200 | Loss: 0.00096 | Test Loss: 0.00094 | Valid Loss: 0.00094\n",
      "Epoch: 5400 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00094\n",
      "Epoch: 5600 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 5800 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 6000 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 6200 | Loss: 0.00095 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 6400 | Loss: 0.00095 | Test Loss: 0.00092 | Valid Loss: 0.00093\n",
      "Epoch: 6600 | Loss: 0.00094 | Test Loss: 0.00092 | Valid Loss: 0.00093\n",
      "Epoch: 6800 | Loss: 0.00094 | Test Loss: 0.00092 | Valid Loss: 0.00092\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 1.01869 | Test Loss: 0.92505 | Valid Loss: 0.93252\n",
      "Epoch: 200 | Loss: 0.00972 | Test Loss: 0.00957 | Valid Loss: 0.00989\n",
      "Epoch: 400 | Loss: 0.00794 | Test Loss: 0.00782 | Valid Loss: 0.00809\n",
      "Epoch: 600 | Loss: 0.00624 | Test Loss: 0.00615 | Valid Loss: 0.00638\n",
      "Epoch: 800 | Loss: 0.00475 | Test Loss: 0.00468 | Valid Loss: 0.00487\n",
      "Epoch: 1000 | Loss: 0.00349 | Test Loss: 0.00343 | Valid Loss: 0.00359\n",
      "Epoch: 1200 | Loss: 0.00246 | Test Loss: 0.00241 | Valid Loss: 0.00254\n",
      "Epoch: 1400 | Loss: 0.00170 | Test Loss: 0.00166 | Valid Loss: 0.00178\n",
      "Epoch: 1600 | Loss: 0.00127 | Test Loss: 0.00123 | Valid Loss: 0.00133\n",
      "Epoch: 1800 | Loss: 0.00110 | Test Loss: 0.00106 | Valid Loss: 0.00115\n",
      "Epoch: 2000 | Loss: 0.00104 | Test Loss: 0.00100 | Valid Loss: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00103 | Test Loss: 0.00099 | Valid Loss: 0.00108\n",
      "Epoch: 2400 | Loss: 0.00103 | Test Loss: 0.00098 | Valid Loss: 0.00107\n",
      "Epoch: 2600 | Loss: 0.00102 | Test Loss: 0.00098 | Valid Loss: 0.00107\n",
      "Epoch: 2800 | Loss: 0.00102 | Test Loss: 0.00098 | Valid Loss: 0.00107\n",
      "Epoch: 3000 | Loss: 0.00102 | Test Loss: 0.00098 | Valid Loss: 0.00107\n",
      "Epoch: 3200 | Loss: 0.00102 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 3400 | Loss: 0.00102 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 3600 | Loss: 0.00102 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 3800 | Loss: 0.00101 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 4000 | Loss: 0.00101 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 4200 | Loss: 0.00101 | Test Loss: 0.00097 | Valid Loss: 0.00106\n",
      "Epoch: 4400 | Loss: 0.00101 | Test Loss: 0.00096 | Valid Loss: 0.00105\n",
      "Epoch: 4600 | Loss: 0.00101 | Test Loss: 0.00096 | Valid Loss: 0.00105\n",
      "Epoch: 4800 | Loss: 0.00100 | Test Loss: 0.00096 | Valid Loss: 0.00105\n",
      "Epoch: 5000 | Loss: 0.00100 | Test Loss: 0.00096 | Valid Loss: 0.00105\n",
      "Epoch: 5200 | Loss: 0.00100 | Test Loss: 0.00096 | Valid Loss: 0.00105\n",
      "Epoch: 5400 | Loss: 0.00100 | Test Loss: 0.00095 | Valid Loss: 0.00104\n",
      "Epoch: 5600 | Loss: 0.00100 | Test Loss: 0.00095 | Valid Loss: 0.00104\n",
      "Epoch: 5800 | Loss: 0.00099 | Test Loss: 0.00095 | Valid Loss: 0.00104\n",
      "Epoch: 6000 | Loss: 0.00099 | Test Loss: 0.00095 | Valid Loss: 0.00104\n",
      "Epoch: 6200 | Loss: 0.00099 | Test Loss: 0.00095 | Valid Loss: 0.00104\n",
      "Epoch: 6400 | Loss: 0.00099 | Test Loss: 0.00095 | Valid Loss: 0.00103\n",
      "Epoch: 6600 | Loss: 0.00099 | Test Loss: 0.00094 | Valid Loss: 0.00103\n",
      "Epoch: 6800 | Loss: 0.00099 | Test Loss: 0.00094 | Valid Loss: 0.00103\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 0.93160 | Test Loss: 0.82555 | Valid Loss: 0.82285\n",
      "Epoch: 200 | Loss: 0.00960 | Test Loss: 0.00950 | Valid Loss: 0.00986\n",
      "Epoch: 400 | Loss: 0.00821 | Test Loss: 0.00813 | Valid Loss: 0.00843\n",
      "Epoch: 600 | Loss: 0.00687 | Test Loss: 0.00682 | Valid Loss: 0.00705\n",
      "Epoch: 800 | Loss: 0.00569 | Test Loss: 0.00565 | Valid Loss: 0.00583\n",
      "Epoch: 1000 | Loss: 0.00464 | Test Loss: 0.00461 | Valid Loss: 0.00475\n",
      "Epoch: 1200 | Loss: 0.00372 | Test Loss: 0.00370 | Valid Loss: 0.00380\n",
      "Epoch: 1400 | Loss: 0.00291 | Test Loss: 0.00290 | Valid Loss: 0.00297\n",
      "Epoch: 1600 | Loss: 0.00225 | Test Loss: 0.00224 | Valid Loss: 0.00229\n",
      "Epoch: 1800 | Loss: 0.00176 | Test Loss: 0.00175 | Valid Loss: 0.00178\n",
      "Epoch: 2000 | Loss: 0.00143 | Test Loss: 0.00142 | Valid Loss: 0.00145\n",
      "Epoch: 2200 | Loss: 0.00124 | Test Loss: 0.00122 | Valid Loss: 0.00125\n",
      "Epoch: 2400 | Loss: 0.00115 | Test Loss: 0.00113 | Valid Loss: 0.00116\n",
      "Epoch: 2600 | Loss: 0.00111 | Test Loss: 0.00108 | Valid Loss: 0.00111\n",
      "Epoch: 2800 | Loss: 0.00109 | Test Loss: 0.00106 | Valid Loss: 0.00110\n",
      "Epoch: 3000 | Loss: 0.00109 | Test Loss: 0.00105 | Valid Loss: 0.00109\n",
      "Epoch: 3200 | Loss: 0.00108 | Test Loss: 0.00105 | Valid Loss: 0.00109\n",
      "Epoch: 3400 | Loss: 0.00108 | Test Loss: 0.00104 | Valid Loss: 0.00108\n",
      "Epoch: 3600 | Loss: 0.00108 | Test Loss: 0.00104 | Valid Loss: 0.00108\n",
      "Epoch: 3800 | Loss: 0.00108 | Test Loss: 0.00104 | Valid Loss: 0.00108\n",
      "Epoch: 4000 | Loss: 0.00107 | Test Loss: 0.00104 | Valid Loss: 0.00108\n",
      "Epoch: 4200 | Loss: 0.00107 | Test Loss: 0.00104 | Valid Loss: 0.00108\n",
      "Epoch: 4400 | Loss: 0.00107 | Test Loss: 0.00103 | Valid Loss: 0.00108\n",
      "Epoch: 4600 | Loss: 0.00107 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 4800 | Loss: 0.00107 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 5000 | Loss: 0.00107 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 5200 | Loss: 0.00106 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 5400 | Loss: 0.00106 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 5600 | Loss: 0.00106 | Test Loss: 0.00103 | Valid Loss: 0.00107\n",
      "Epoch: 5800 | Loss: 0.00106 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "Epoch: 6000 | Loss: 0.00106 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "Epoch: 6200 | Loss: 0.00106 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "Epoch: 6400 | Loss: 0.00105 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "Epoch: 6600 | Loss: 0.00105 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "Epoch: 6800 | Loss: 0.00105 | Test Loss: 0.00102 | Valid Loss: 0.00106\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.12742 | Test Loss: 1.03118 | Valid Loss: 1.02032\n",
      "Epoch: 200 | Loss: 0.00939 | Test Loss: 0.00898 | Valid Loss: 0.00920\n",
      "Epoch: 400 | Loss: 0.00781 | Test Loss: 0.00749 | Valid Loss: 0.00765\n",
      "Epoch: 600 | Loss: 0.00625 | Test Loss: 0.00600 | Valid Loss: 0.00611\n",
      "Epoch: 800 | Loss: 0.00485 | Test Loss: 0.00467 | Valid Loss: 0.00474\n",
      "Epoch: 1000 | Loss: 0.00364 | Test Loss: 0.00352 | Valid Loss: 0.00355\n",
      "Epoch: 1200 | Loss: 0.00261 | Test Loss: 0.00254 | Valid Loss: 0.00255\n",
      "Epoch: 1400 | Loss: 0.00183 | Test Loss: 0.00181 | Valid Loss: 0.00179\n",
      "Epoch: 1600 | Loss: 0.00136 | Test Loss: 0.00136 | Valid Loss: 0.00133\n",
      "Epoch: 1800 | Loss: 0.00114 | Test Loss: 0.00115 | Valid Loss: 0.00112\n",
      "Epoch: 2000 | Loss: 0.00106 | Test Loss: 0.00108 | Valid Loss: 0.00105\n",
      "Epoch: 2200 | Loss: 0.00104 | Test Loss: 0.00106 | Valid Loss: 0.00103\n",
      "Epoch: 2400 | Loss: 0.00103 | Test Loss: 0.00105 | Valid Loss: 0.00102\n",
      "Epoch: 2600 | Loss: 0.00103 | Test Loss: 0.00105 | Valid Loss: 0.00102\n",
      "Epoch: 2800 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 3000 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 3200 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 3400 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 3600 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 3800 | Loss: 0.00102 | Test Loss: 0.00104 | Valid Loss: 0.00101\n",
      "Epoch: 4000 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 4200 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 4400 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 4600 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 4800 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 5000 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 5200 | Loss: 0.00101 | Test Loss: 0.00103 | Valid Loss: 0.00100\n",
      "Epoch: 5400 | Loss: 0.00101 | Test Loss: 0.00102 | Valid Loss: 0.00100\n",
      "Epoch: 5600 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 5800 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 6000 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 6200 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 6400 | Loss: 0.00100 | Test Loss: 0.00102 | Valid Loss: 0.00099\n",
      "Epoch: 6600 | Loss: 0.00099 | Test Loss: 0.00102 | Valid Loss: 0.00098\n",
      "Epoch: 6800 | Loss: 0.00099 | Test Loss: 0.00101 | Valid Loss: 0.00098\n",
      "Early stopping at epoch: 6993\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(67)\n",
    "\n",
    "epochs = 7000\n",
    "learning_rate = 0.002\n",
    "momentum = 1.3\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(M_MT_stacking_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_M_MT_stack_train[:, num, :]).squeeze() # беру результаты сетей для num столбца\n",
    "                                                                # по смыслу - результаты G M на num координате слоя\n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_M_MT_stack_valid[:, num, :]).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_M_MT_stack_test[:, num, :]).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c0c87c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 6.6863 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(M_MT_stacking_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_M_MT_stack_test[:, num, :]).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "M_MT_stack_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(M_MT_stack_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4e448c",
   "metadata": {},
   "source": [
    "# Stacking - G + M + MT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "22bce3a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "input_dim_1 = 31\n",
    "input_dim_2 = 31\n",
    "input_dim_3 = 62\n",
    "hidden_units = 32\n",
    "output_dim = 1\n",
    "num_methods = 2\n",
    "\n",
    "meta_input_dim = 15\n",
    "\n",
    "num_models = 15\n",
    "G_M_MT_stacking_models = []\n",
    "    \n",
    "for i in range(num_models):\n",
    "    G_M_MT_stacking_models.append(Initial_Model_V0(meta_input_dim, hidden_units, output_dim).to(device))\n",
    "    \n",
    "next(G_M_MT_stacking_models[1].parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "df2ddd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_G_train, y_train = X_G_train.to(device), y_train.to(device)\n",
    "X_G_test, y_test = X_G_test.to(device), y_test.to(device)\n",
    "X_G_valid, y_valid = X_G_valid.to(device), y_valid.to(device)\n",
    "\n",
    "X_M_train = X_M_train.to(device)\n",
    "X_M_test = X_M_test.to(device)\n",
    "X_M_valid = X_M_valid.to(device)\n",
    "\n",
    "X_MT_train = X_MT_train.to(device)\n",
    "X_MT_test = X_MT_test.to(device)\n",
    "X_MT_valid = X_MT_valid.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f267829f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21000, 15, 15])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_NN_in_set = 5\n",
    "\n",
    "# Training\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_train)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_train)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_train)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_M_MT_stack_train = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_M_MT_stack_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6cec61c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6000, 15, 15])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_valid)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_valid)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_valid)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_M_MT_stack_valid = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_M_MT_stack_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "195345dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3000, 15, 15])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "G_pre_preds = []\n",
    "M_pre_preds = []\n",
    "MT_pre_preds = []\n",
    "G_preds = []\n",
    "M_preds = []\n",
    "MT_preds = []\n",
    "\n",
    "for num_in_set in range(num_NN_in_set):\n",
    "    \n",
    "    for num, model in enumerate(G_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_G_preds = model(X_G_test)\n",
    "            G_pre_preds.append(y_G_preds)\n",
    "\n",
    "    G_temp = torch.cat(G_pre_preds, 1).unsqueeze(2)\n",
    "    G_preds.append(G_temp)\n",
    "    G_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(M_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_M_preds = model(X_M_test)\n",
    "            M_pre_preds.append(y_M_preds)\n",
    "\n",
    "    M_temp = torch.cat(M_pre_preds, 1).unsqueeze(2)\n",
    "    M_preds.append(M_temp)\n",
    "    M_pre_preds = []\n",
    "    \n",
    "    for num, model in enumerate(MT_models, 0):\n",
    "    \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "        \n",
    "            y_MT_preds = model(X_MT_test)\n",
    "            MT_pre_preds.append(y_MT_preds)\n",
    "\n",
    "    MT_temp = torch.cat(MT_pre_preds, 1).unsqueeze(2)\n",
    "    MT_preds.append(MT_temp)\n",
    "    MT_pre_preds = []\n",
    "\n",
    "X_G_M_MT_stack_test = torch.cat((torch.cat(G_preds, 2), torch.cat(M_preds, 2), torch.cat(MT_preds, 2)), 2)\n",
    "X_G_M_MT_stack_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b66c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model num: 0\n",
      "Epoch: 0 | Loss: 1.13495 | Test Loss: 1.03278 | Valid Loss: 1.01492\n",
      "Epoch: 200 | Loss: 0.01022 | Test Loss: 0.01035 | Valid Loss: 0.01019\n",
      "Epoch: 400 | Loss: 0.00750 | Test Loss: 0.00760 | Valid Loss: 0.00746\n",
      "Epoch: 600 | Loss: 0.00533 | Test Loss: 0.00540 | Valid Loss: 0.00529\n",
      "Epoch: 800 | Loss: 0.00365 | Test Loss: 0.00370 | Valid Loss: 0.00360\n",
      "Epoch: 1000 | Loss: 0.00232 | Test Loss: 0.00236 | Valid Loss: 0.00228\n",
      "Epoch: 1200 | Loss: 0.00145 | Test Loss: 0.00148 | Valid Loss: 0.00142\n",
      "Epoch: 1400 | Loss: 0.00104 | Test Loss: 0.00107 | Valid Loss: 0.00101\n",
      "Epoch: 1600 | Loss: 0.00089 | Test Loss: 0.00092 | Valid Loss: 0.00087\n",
      "Epoch: 1800 | Loss: 0.00084 | Test Loss: 0.00087 | Valid Loss: 0.00083\n",
      "Epoch: 2000 | Loss: 0.00083 | Test Loss: 0.00086 | Valid Loss: 0.00082\n",
      "Epoch: 2200 | Loss: 0.00082 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 2400 | Loss: 0.00082 | Test Loss: 0.00085 | Valid Loss: 0.00081\n",
      "Epoch: 2600 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00081\n",
      "Epoch: 2800 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00080\n",
      "Epoch: 3000 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00080\n",
      "Epoch: 3200 | Loss: 0.00081 | Test Loss: 0.00084 | Valid Loss: 0.00080\n",
      "Epoch: 3400 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00080\n",
      "Epoch: 3600 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00080\n",
      "Epoch: 3800 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00079\n",
      "Epoch: 4000 | Loss: 0.00080 | Test Loss: 0.00083 | Valid Loss: 0.00079\n",
      "Epoch: 4200 | Loss: 0.00079 | Test Loss: 0.00082 | Valid Loss: 0.00079\n",
      "Epoch: 4400 | Loss: 0.00079 | Test Loss: 0.00082 | Valid Loss: 0.00079\n",
      "Epoch: 4600 | Loss: 0.00079 | Test Loss: 0.00082 | Valid Loss: 0.00078\n",
      "Epoch: 4800 | Loss: 0.00079 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Epoch: 5000 | Loss: 0.00078 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Epoch: 5200 | Loss: 0.00078 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Epoch: 5400 | Loss: 0.00078 | Test Loss: 0.00081 | Valid Loss: 0.00078\n",
      "Early stopping at epoch: 5584\n",
      "=====================\n",
      "Model num: 1\n",
      "Epoch: 0 | Loss: 1.73799 | Test Loss: 1.59024 | Valid Loss: 1.56896\n",
      "Epoch: 200 | Loss: 0.00777 | Test Loss: 0.00804 | Valid Loss: 0.00770\n",
      "Epoch: 400 | Loss: 0.00644 | Test Loss: 0.00666 | Valid Loss: 0.00638\n",
      "Epoch: 600 | Loss: 0.00510 | Test Loss: 0.00526 | Valid Loss: 0.00504\n",
      "Epoch: 800 | Loss: 0.00394 | Test Loss: 0.00406 | Valid Loss: 0.00389\n",
      "Epoch: 1000 | Loss: 0.00301 | Test Loss: 0.00310 | Valid Loss: 0.00298\n",
      "Epoch: 1200 | Loss: 0.00229 | Test Loss: 0.00235 | Valid Loss: 0.00226\n",
      "Epoch: 1400 | Loss: 0.00173 | Test Loss: 0.00177 | Valid Loss: 0.00171\n",
      "Epoch: 1600 | Loss: 0.00133 | Test Loss: 0.00135 | Valid Loss: 0.00131\n",
      "Epoch: 1800 | Loss: 0.00107 | Test Loss: 0.00109 | Valid Loss: 0.00106\n",
      "Epoch: 2000 | Loss: 0.00092 | Test Loss: 0.00094 | Valid Loss: 0.00092\n",
      "Epoch: 2200 | Loss: 0.00085 | Test Loss: 0.00086 | Valid Loss: 0.00085\n",
      "Epoch: 2400 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 2600 | Loss: 0.00080 | Test Loss: 0.00081 | Valid Loss: 0.00080\n",
      "Epoch: 2800 | Loss: 0.00079 | Test Loss: 0.00080 | Valid Loss: 0.00079\n",
      "Epoch: 3000 | Loss: 0.00078 | Test Loss: 0.00079 | Valid Loss: 0.00078\n",
      "Epoch: 3200 | Loss: 0.00077 | Test Loss: 0.00078 | Valid Loss: 0.00078\n",
      "Epoch: 3400 | Loss: 0.00076 | Test Loss: 0.00077 | Valid Loss: 0.00077\n",
      "Epoch: 3600 | Loss: 0.00076 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 3800 | Loss: 0.00075 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 4000 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 4200 | Loss: 0.00074 | Test Loss: 0.00075 | Valid Loss: 0.00075\n",
      "Epoch: 4400 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 4600 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 4800 | Loss: 0.00073 | Test Loss: 0.00074 | Valid Loss: 0.00074\n",
      "Epoch: 5000 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00074\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 5400 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 5600 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 5800 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6000 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6200 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6400 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6600 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 6800 | Loss: 0.00071 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "=====================\n",
      "Model num: 2\n",
      "Epoch: 0 | Loss: 1.83034 | Test Loss: 1.67498 | Valid Loss: 1.66441\n",
      "Epoch: 200 | Loss: 0.00820 | Test Loss: 0.00815 | Valid Loss: 0.00820\n",
      "Epoch: 400 | Loss: 0.00711 | Test Loss: 0.00706 | Valid Loss: 0.00710\n",
      "Epoch: 600 | Loss: 0.00604 | Test Loss: 0.00599 | Valid Loss: 0.00603\n",
      "Epoch: 800 | Loss: 0.00510 | Test Loss: 0.00505 | Valid Loss: 0.00508\n",
      "Epoch: 1000 | Loss: 0.00426 | Test Loss: 0.00422 | Valid Loss: 0.00425\n",
      "Epoch: 1200 | Loss: 0.00350 | Test Loss: 0.00346 | Valid Loss: 0.00348\n",
      "Epoch: 1400 | Loss: 0.00279 | Test Loss: 0.00276 | Valid Loss: 0.00277\n",
      "Epoch: 1600 | Loss: 0.00217 | Test Loss: 0.00214 | Valid Loss: 0.00215\n",
      "Epoch: 1800 | Loss: 0.00167 | Test Loss: 0.00165 | Valid Loss: 0.00164\n",
      "Epoch: 2000 | Loss: 0.00131 | Test Loss: 0.00130 | Valid Loss: 0.00129\n",
      "Epoch: 2200 | Loss: 0.00108 | Test Loss: 0.00108 | Valid Loss: 0.00106\n",
      "Epoch: 2400 | Loss: 0.00094 | Test Loss: 0.00095 | Valid Loss: 0.00093\n",
      "Epoch: 2600 | Loss: 0.00087 | Test Loss: 0.00088 | Valid Loss: 0.00085\n",
      "Epoch: 2800 | Loss: 0.00082 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3000 | Loss: 0.00079 | Test Loss: 0.00080 | Valid Loss: 0.00078\n",
      "Epoch: 3200 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00076\n",
      "Epoch: 3400 | Loss: 0.00075 | Test Loss: 0.00077 | Valid Loss: 0.00074\n",
      "Epoch: 3600 | Loss: 0.00074 | Test Loss: 0.00076 | Valid Loss: 0.00073\n",
      "Epoch: 3800 | Loss: 0.00073 | Test Loss: 0.00075 | Valid Loss: 0.00072\n",
      "Epoch: 4000 | Loss: 0.00072 | Test Loss: 0.00074 | Valid Loss: 0.00071\n",
      "Epoch: 4200 | Loss: 0.00071 | Test Loss: 0.00073 | Valid Loss: 0.00071\n",
      "Epoch: 4400 | Loss: 0.00070 | Test Loss: 0.00072 | Valid Loss: 0.00070\n",
      "Epoch: 4600 | Loss: 0.00070 | Test Loss: 0.00071 | Valid Loss: 0.00069\n",
      "Epoch: 4800 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 5000 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 5200 | Loss: 0.00069 | Test Loss: 0.00070 | Valid Loss: 0.00068\n",
      "Epoch: 5400 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 5600 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 5800 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6000 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 6200 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00068\n",
      "Epoch: 6400 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00068\n",
      "Epoch: 6600 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 6800 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "=====================\n",
      "Model num: 3\n",
      "Epoch: 0 | Loss: 1.03129 | Test Loss: 0.91457 | Valid Loss: 0.91230\n",
      "Epoch: 200 | Loss: 0.00789 | Test Loss: 0.00813 | Valid Loss: 0.00793\n",
      "Epoch: 400 | Loss: 0.00626 | Test Loss: 0.00645 | Valid Loss: 0.00629\n",
      "Epoch: 600 | Loss: 0.00481 | Test Loss: 0.00494 | Valid Loss: 0.00482\n",
      "Epoch: 800 | Loss: 0.00362 | Test Loss: 0.00372 | Valid Loss: 0.00362\n",
      "Epoch: 1000 | Loss: 0.00266 | Test Loss: 0.00273 | Valid Loss: 0.00265\n",
      "Epoch: 1200 | Loss: 0.00190 | Test Loss: 0.00194 | Valid Loss: 0.00189\n",
      "Epoch: 1400 | Loss: 0.00135 | Test Loss: 0.00137 | Valid Loss: 0.00134\n",
      "Epoch: 1600 | Loss: 0.00101 | Test Loss: 0.00102 | Valid Loss: 0.00100\n",
      "Epoch: 1800 | Loss: 0.00083 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 2000 | Loss: 0.00075 | Test Loss: 0.00075 | Valid Loss: 0.00074\n",
      "Epoch: 2200 | Loss: 0.00070 | Test Loss: 0.00070 | Valid Loss: 0.00069\n",
      "Epoch: 2400 | Loss: 0.00067 | Test Loss: 0.00068 | Valid Loss: 0.00067\n",
      "Epoch: 2600 | Loss: 0.00066 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 2800 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00063\n",
      "Epoch: 3000 | Loss: 0.00063 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 3200 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 3400 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 3600 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 3800 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 4000 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 5000 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5200 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5600 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5800 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6000 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6200 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6400 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Early stopping at epoch: 6473\n",
      "=====================\n",
      "Model num: 4\n",
      "Epoch: 0 | Loss: 0.63167 | Test Loss: 0.54711 | Valid Loss: 0.54829\n",
      "Epoch: 200 | Loss: 0.00812 | Test Loss: 0.00815 | Valid Loss: 0.00831\n",
      "Epoch: 400 | Loss: 0.00600 | Test Loss: 0.00602 | Valid Loss: 0.00615\n",
      "Epoch: 600 | Loss: 0.00434 | Test Loss: 0.00434 | Valid Loss: 0.00444\n",
      "Epoch: 800 | Loss: 0.00303 | Test Loss: 0.00303 | Valid Loss: 0.00311\n",
      "Epoch: 1000 | Loss: 0.00204 | Test Loss: 0.00203 | Valid Loss: 0.00209\n",
      "Epoch: 1200 | Loss: 0.00136 | Test Loss: 0.00136 | Valid Loss: 0.00140\n",
      "Epoch: 1400 | Loss: 0.00098 | Test Loss: 0.00097 | Valid Loss: 0.00100\n",
      "Epoch: 1600 | Loss: 0.00080 | Test Loss: 0.00079 | Valid Loss: 0.00081\n",
      "Epoch: 1800 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00073\n",
      "Epoch: 2000 | Loss: 0.00068 | Test Loss: 0.00068 | Valid Loss: 0.00069\n",
      "Epoch: 2200 | Loss: 0.00066 | Test Loss: 0.00065 | Valid Loss: 0.00067\n",
      "Epoch: 2400 | Loss: 0.00064 | Test Loss: 0.00064 | Valid Loss: 0.00065\n",
      "Epoch: 2600 | Loss: 0.00063 | Test Loss: 0.00063 | Valid Loss: 0.00064\n",
      "Epoch: 2800 | Loss: 0.00062 | Test Loss: 0.00062 | Valid Loss: 0.00063\n",
      "Epoch: 3000 | Loss: 0.00061 | Test Loss: 0.00061 | Valid Loss: 0.00062\n",
      "Epoch: 3200 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 3400 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 3600 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 3800 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 4000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4200 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 4400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 4600 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 5000 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 5200 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 5800 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6000 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6200 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6400 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00056\n",
      "Early stopping at epoch: 6762\n",
      "=====================\n",
      "Model num: 5\n",
      "Epoch: 0 | Loss: 1.99472 | Test Loss: 1.83412 | Valid Loss: 1.82942\n",
      "Epoch: 200 | Loss: 0.00742 | Test Loss: 0.00762 | Valid Loss: 0.00752\n",
      "Epoch: 400 | Loss: 0.00658 | Test Loss: 0.00675 | Valid Loss: 0.00667\n",
      "Epoch: 600 | Loss: 0.00565 | Test Loss: 0.00580 | Valid Loss: 0.00573\n",
      "Epoch: 800 | Loss: 0.00474 | Test Loss: 0.00487 | Valid Loss: 0.00482\n",
      "Epoch: 1000 | Loss: 0.00389 | Test Loss: 0.00399 | Valid Loss: 0.00396\n",
      "Epoch: 1200 | Loss: 0.00312 | Test Loss: 0.00320 | Valid Loss: 0.00318\n",
      "Epoch: 1400 | Loss: 0.00245 | Test Loss: 0.00251 | Valid Loss: 0.00250\n",
      "Epoch: 1600 | Loss: 0.00189 | Test Loss: 0.00193 | Valid Loss: 0.00193\n",
      "Epoch: 1800 | Loss: 0.00144 | Test Loss: 0.00148 | Valid Loss: 0.00148\n",
      "Epoch: 2000 | Loss: 0.00112 | Test Loss: 0.00115 | Valid Loss: 0.00115\n",
      "Epoch: 2200 | Loss: 0.00091 | Test Loss: 0.00093 | Valid Loss: 0.00093\n",
      "Epoch: 2400 | Loss: 0.00078 | Test Loss: 0.00080 | Valid Loss: 0.00080\n",
      "Epoch: 2600 | Loss: 0.00071 | Test Loss: 0.00073 | Valid Loss: 0.00072\n",
      "Epoch: 2800 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 3000 | Loss: 0.00066 | Test Loss: 0.00067 | Valid Loss: 0.00066\n",
      "Epoch: 3200 | Loss: 0.00065 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00064\n",
      "Epoch: 3600 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 3800 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 4000 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 4200 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 4400 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 4600 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 4800 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 5000 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 5200 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 5400 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 5600 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 5800 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 6000 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6200 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 6400 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 6600 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 6800 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "=====================\n",
      "Model num: 6\n",
      "Epoch: 0 | Loss: 2.59818 | Test Loss: 2.42108 | Valid Loss: 2.40156\n",
      "Epoch: 200 | Loss: 0.00650 | Test Loss: 0.00669 | Valid Loss: 0.00651\n",
      "Epoch: 400 | Loss: 0.00566 | Test Loss: 0.00582 | Valid Loss: 0.00567\n",
      "Epoch: 600 | Loss: 0.00480 | Test Loss: 0.00494 | Valid Loss: 0.00480\n",
      "Epoch: 800 | Loss: 0.00401 | Test Loss: 0.00413 | Valid Loss: 0.00401\n",
      "Epoch: 1000 | Loss: 0.00332 | Test Loss: 0.00342 | Valid Loss: 0.00332\n",
      "Epoch: 1200 | Loss: 0.00272 | Test Loss: 0.00280 | Valid Loss: 0.00271\n",
      "Epoch: 1400 | Loss: 0.00220 | Test Loss: 0.00227 | Valid Loss: 0.00219\n",
      "Epoch: 1600 | Loss: 0.00175 | Test Loss: 0.00181 | Valid Loss: 0.00174\n",
      "Epoch: 1800 | Loss: 0.00139 | Test Loss: 0.00144 | Valid Loss: 0.00137\n",
      "Epoch: 2000 | Loss: 0.00110 | Test Loss: 0.00114 | Valid Loss: 0.00109\n",
      "Epoch: 2200 | Loss: 0.00090 | Test Loss: 0.00093 | Valid Loss: 0.00088\n",
      "Epoch: 2400 | Loss: 0.00076 | Test Loss: 0.00079 | Valid Loss: 0.00074\n",
      "Epoch: 2600 | Loss: 0.00068 | Test Loss: 0.00070 | Valid Loss: 0.00066\n",
      "Epoch: 2800 | Loss: 0.00063 | Test Loss: 0.00066 | Valid Loss: 0.00061\n",
      "Epoch: 3000 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00059\n",
      "Epoch: 3200 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00058\n",
      "Epoch: 3400 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00057\n",
      "Epoch: 3600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00057\n",
      "Epoch: 3800 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00056\n",
      "Epoch: 4000 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00056\n",
      "Epoch: 4200 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00055\n",
      "Epoch: 4400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00055\n",
      "Epoch: 4600 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00055\n",
      "Epoch: 4800 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00054\n",
      "Epoch: 5000 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00054\n",
      "Epoch: 5200 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 5400 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 5600 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00053\n",
      "Epoch: 5800 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00053\n",
      "Epoch: 6000 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00052\n",
      "Epoch: 6200 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00052\n",
      "Epoch: 6400 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00052\n",
      "Epoch: 6600 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00052\n",
      "Epoch: 6800 | Loss: 0.00054 | Test Loss: 0.00057 | Valid Loss: 0.00052\n",
      "=====================\n",
      "Model num: 7\n",
      "Epoch: 0 | Loss: 1.46640 | Test Loss: 1.33208 | Valid Loss: 1.32640\n",
      "Epoch: 200 | Loss: 0.00674 | Test Loss: 0.00685 | Valid Loss: 0.00678\n",
      "Epoch: 400 | Loss: 0.00581 | Test Loss: 0.00591 | Valid Loss: 0.00585\n",
      "Epoch: 600 | Loss: 0.00485 | Test Loss: 0.00494 | Valid Loss: 0.00489\n",
      "Epoch: 800 | Loss: 0.00393 | Test Loss: 0.00402 | Valid Loss: 0.00397\n",
      "Epoch: 1000 | Loss: 0.00311 | Test Loss: 0.00319 | Valid Loss: 0.00315\n",
      "Epoch: 1200 | Loss: 0.00239 | Test Loss: 0.00247 | Valid Loss: 0.00243\n",
      "Epoch: 1400 | Loss: 0.00180 | Test Loss: 0.00187 | Valid Loss: 0.00183\n",
      "Epoch: 1600 | Loss: 0.00134 | Test Loss: 0.00140 | Valid Loss: 0.00137\n",
      "Epoch: 1800 | Loss: 0.00102 | Test Loss: 0.00107 | Valid Loss: 0.00104\n",
      "Epoch: 2000 | Loss: 0.00082 | Test Loss: 0.00087 | Valid Loss: 0.00084\n",
      "Epoch: 2200 | Loss: 0.00072 | Test Loss: 0.00076 | Valid Loss: 0.00073\n",
      "Epoch: 2400 | Loss: 0.00067 | Test Loss: 0.00070 | Valid Loss: 0.00067\n",
      "Epoch: 2600 | Loss: 0.00064 | Test Loss: 0.00067 | Valid Loss: 0.00064\n",
      "Epoch: 2800 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00062\n",
      "Epoch: 3000 | Loss: 0.00061 | Test Loss: 0.00064 | Valid Loss: 0.00061\n",
      "Epoch: 3200 | Loss: 0.00060 | Test Loss: 0.00063 | Valid Loss: 0.00060\n",
      "Epoch: 3400 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00059\n",
      "Epoch: 3600 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 3800 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Epoch: 4000 | Loss: 0.00058 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 4200 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00057\n",
      "Epoch: 4400 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00057 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5000 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5200 | Loss: 0.00056 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 5400 | Loss: 0.00056 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 5600 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 6000 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00057 | Valid Loss: 0.00055\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "Epoch: 6800 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00054\n",
      "=====================\n",
      "Model num: 8\n",
      "Epoch: 0 | Loss: 1.93670 | Test Loss: 1.80394 | Valid Loss: 1.79851\n",
      "Epoch: 200 | Loss: 0.00883 | Test Loss: 0.00872 | Valid Loss: 0.00893\n",
      "Epoch: 400 | Loss: 0.00765 | Test Loss: 0.00757 | Valid Loss: 0.00775\n",
      "Epoch: 600 | Loss: 0.00646 | Test Loss: 0.00641 | Valid Loss: 0.00655\n",
      "Epoch: 800 | Loss: 0.00539 | Test Loss: 0.00537 | Valid Loss: 0.00547\n",
      "Epoch: 1000 | Loss: 0.00439 | Test Loss: 0.00439 | Valid Loss: 0.00447\n",
      "Epoch: 1200 | Loss: 0.00343 | Test Loss: 0.00345 | Valid Loss: 0.00350\n",
      "Epoch: 1400 | Loss: 0.00251 | Test Loss: 0.00254 | Valid Loss: 0.00257\n",
      "Epoch: 1600 | Loss: 0.00172 | Test Loss: 0.00177 | Valid Loss: 0.00177\n",
      "Epoch: 1800 | Loss: 0.00116 | Test Loss: 0.00121 | Valid Loss: 0.00119\n",
      "Epoch: 2000 | Loss: 0.00085 | Test Loss: 0.00089 | Valid Loss: 0.00087\n",
      "Epoch: 2200 | Loss: 0.00071 | Test Loss: 0.00075 | Valid Loss: 0.00073\n",
      "Epoch: 2400 | Loss: 0.00065 | Test Loss: 0.00069 | Valid Loss: 0.00066\n",
      "Epoch: 2600 | Loss: 0.00062 | Test Loss: 0.00065 | Valid Loss: 0.00063\n",
      "Epoch: 2800 | Loss: 0.00060 | Test Loss: 0.00064 | Valid Loss: 0.00061\n",
      "Epoch: 3000 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00060\n",
      "Epoch: 3200 | Loss: 0.00059 | Test Loss: 0.00062 | Valid Loss: 0.00059\n",
      "Epoch: 3400 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00059\n",
      "Epoch: 3600 | Loss: 0.00058 | Test Loss: 0.00061 | Valid Loss: 0.00058\n",
      "Epoch: 3800 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 4000 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 4200 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00058\n",
      "Epoch: 4400 | Loss: 0.00057 | Test Loss: 0.00060 | Valid Loss: 0.00057\n",
      "Epoch: 4600 | Loss: 0.00056 | Test Loss: 0.00060 | Valid Loss: 0.00057\n",
      "Epoch: 4800 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5000 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5200 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00056 | Test Loss: 0.00059 | Valid Loss: 0.00057\n",
      "Epoch: 5800 | Loss: 0.00055 | Test Loss: 0.00059 | Valid Loss: 0.00056\n",
      "Epoch: 6000 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "Epoch: 6800 | Loss: 0.00055 | Test Loss: 0.00058 | Valid Loss: 0.00056\n",
      "=====================\n",
      "Model num: 9\n",
      "Epoch: 0 | Loss: 3.10636 | Test Loss: 2.88292 | Valid Loss: 2.89319\n",
      "Epoch: 200 | Loss: 0.00685 | Test Loss: 0.00701 | Valid Loss: 0.00702\n",
      "Epoch: 400 | Loss: 0.00628 | Test Loss: 0.00643 | Valid Loss: 0.00644\n",
      "Epoch: 600 | Loss: 0.00564 | Test Loss: 0.00578 | Valid Loss: 0.00579\n",
      "Epoch: 800 | Loss: 0.00500 | Test Loss: 0.00512 | Valid Loss: 0.00513\n",
      "Epoch: 1000 | Loss: 0.00437 | Test Loss: 0.00448 | Valid Loss: 0.00449\n",
      "Epoch: 1200 | Loss: 0.00377 | Test Loss: 0.00386 | Valid Loss: 0.00387\n",
      "Epoch: 1400 | Loss: 0.00319 | Test Loss: 0.00327 | Valid Loss: 0.00328\n",
      "Epoch: 1600 | Loss: 0.00264 | Test Loss: 0.00271 | Valid Loss: 0.00272\n",
      "Epoch: 1800 | Loss: 0.00213 | Test Loss: 0.00219 | Valid Loss: 0.00220\n",
      "Epoch: 2000 | Loss: 0.00169 | Test Loss: 0.00174 | Valid Loss: 0.00175\n",
      "Epoch: 2200 | Loss: 0.00133 | Test Loss: 0.00137 | Valid Loss: 0.00137\n",
      "Epoch: 2400 | Loss: 0.00106 | Test Loss: 0.00109 | Valid Loss: 0.00109\n",
      "Epoch: 2600 | Loss: 0.00087 | Test Loss: 0.00089 | Valid Loss: 0.00089\n",
      "Epoch: 2800 | Loss: 0.00075 | Test Loss: 0.00076 | Valid Loss: 0.00076\n",
      "Epoch: 3000 | Loss: 0.00068 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 3200 | Loss: 0.00064 | Test Loss: 0.00065 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00063\n",
      "Epoch: 3600 | Loss: 0.00061 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 3800 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00060\n",
      "Epoch: 4000 | Loss: 0.00060 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 4200 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00059\n",
      "Epoch: 4400 | Loss: 0.00059 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 4600 | Loss: 0.00058 | Test Loss: 0.00059 | Valid Loss: 0.00058\n",
      "Epoch: 4800 | Loss: 0.00058 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 5000 | Loss: 0.00057 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 5200 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5400 | Loss: 0.00057 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 5600 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 5800 | Loss: 0.00056 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 6000 | Loss: 0.00055 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 6200 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 6400 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 6600 | Loss: 0.00055 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "Epoch: 6800 | Loss: 0.00054 | Test Loss: 0.00055 | Valid Loss: 0.00055\n",
      "=====================\n",
      "Model num: 10\n",
      "Epoch: 0 | Loss: 2.00588 | Test Loss: 1.82146 | Valid Loss: 1.83167\n",
      "Epoch: 200 | Loss: 0.00645 | Test Loss: 0.00637 | Valid Loss: 0.00661\n",
      "Epoch: 400 | Loss: 0.00561 | Test Loss: 0.00555 | Valid Loss: 0.00576\n",
      "Epoch: 600 | Loss: 0.00476 | Test Loss: 0.00471 | Valid Loss: 0.00489\n",
      "Epoch: 800 | Loss: 0.00394 | Test Loss: 0.00391 | Valid Loss: 0.00406\n",
      "Epoch: 1000 | Loss: 0.00318 | Test Loss: 0.00316 | Valid Loss: 0.00329\n",
      "Epoch: 1200 | Loss: 0.00250 | Test Loss: 0.00248 | Valid Loss: 0.00258\n",
      "Epoch: 1400 | Loss: 0.00190 | Test Loss: 0.00190 | Valid Loss: 0.00197\n",
      "Epoch: 1600 | Loss: 0.00143 | Test Loss: 0.00143 | Valid Loss: 0.00148\n",
      "Epoch: 1800 | Loss: 0.00109 | Test Loss: 0.00110 | Valid Loss: 0.00113\n",
      "Epoch: 2000 | Loss: 0.00088 | Test Loss: 0.00090 | Valid Loss: 0.00091\n",
      "Epoch: 2200 | Loss: 0.00077 | Test Loss: 0.00079 | Valid Loss: 0.00079\n",
      "Epoch: 2400 | Loss: 0.00072 | Test Loss: 0.00073 | Valid Loss: 0.00073\n",
      "Epoch: 2600 | Loss: 0.00069 | Test Loss: 0.00071 | Valid Loss: 0.00070\n",
      "Epoch: 2800 | Loss: 0.00067 | Test Loss: 0.00069 | Valid Loss: 0.00068\n",
      "Epoch: 3000 | Loss: 0.00066 | Test Loss: 0.00068 | Valid Loss: 0.00066\n",
      "Epoch: 3200 | Loss: 0.00065 | Test Loss: 0.00067 | Valid Loss: 0.00065\n",
      "Epoch: 3400 | Loss: 0.00064 | Test Loss: 0.00066 | Valid Loss: 0.00065\n",
      "Epoch: 3600 | Loss: 0.00063 | Test Loss: 0.00065 | Valid Loss: 0.00064\n",
      "Epoch: 3800 | Loss: 0.00063 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 4000 | Loss: 0.00062 | Test Loss: 0.00064 | Valid Loss: 0.00063\n",
      "Epoch: 4200 | Loss: 0.00062 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 4400 | Loss: 0.00061 | Test Loss: 0.00063 | Valid Loss: 0.00062\n",
      "Epoch: 4600 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 4800 | Loss: 0.00061 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 5000 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 5200 | Loss: 0.00060 | Test Loss: 0.00062 | Valid Loss: 0.00061\n",
      "Epoch: 5400 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 5600 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 5800 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 6000 | Loss: 0.00060 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 6400 | Loss: 0.00059 | Test Loss: 0.00061 | Valid Loss: 0.00061\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00061\n",
      "Epoch: 6800 | Loss: 0.00059 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "=====================\n",
      "Model num: 11\n",
      "Epoch: 0 | Loss: 0.40111 | Test Loss: 0.33271 | Valid Loss: 0.33590\n",
      "Epoch: 200 | Loss: 0.00888 | Test Loss: 0.00887 | Valid Loss: 0.00871\n",
      "Epoch: 400 | Loss: 0.00591 | Test Loss: 0.00590 | Valid Loss: 0.00579\n",
      "Epoch: 600 | Loss: 0.00357 | Test Loss: 0.00356 | Valid Loss: 0.00349\n",
      "Epoch: 800 | Loss: 0.00192 | Test Loss: 0.00192 | Valid Loss: 0.00188\n",
      "Epoch: 1000 | Loss: 0.00110 | Test Loss: 0.00109 | Valid Loss: 0.00108\n",
      "Epoch: 1200 | Loss: 0.00081 | Test Loss: 0.00080 | Valid Loss: 0.00080\n",
      "Epoch: 1400 | Loss: 0.00070 | Test Loss: 0.00069 | Valid Loss: 0.00069\n",
      "Epoch: 1600 | Loss: 0.00066 | Test Loss: 0.00064 | Valid Loss: 0.00064\n",
      "Epoch: 1800 | Loss: 0.00063 | Test Loss: 0.00062 | Valid Loss: 0.00062\n",
      "Epoch: 2000 | Loss: 0.00062 | Test Loss: 0.00060 | Valid Loss: 0.00060\n",
      "Epoch: 2200 | Loss: 0.00060 | Test Loss: 0.00059 | Valid Loss: 0.00059\n",
      "Epoch: 2400 | Loss: 0.00060 | Test Loss: 0.00058 | Valid Loss: 0.00058\n",
      "Epoch: 2600 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00058\n",
      "Epoch: 2800 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 3000 | Loss: 0.00058 | Test Loss: 0.00057 | Valid Loss: 0.00057\n",
      "Epoch: 3200 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 3400 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00057\n",
      "Epoch: 3600 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 3800 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 4000 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 4200 | Loss: 0.00057 | Test Loss: 0.00056 | Valid Loss: 0.00056\n",
      "Epoch: 4400 | Loss: 0.00057 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 4600 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Epoch: 4800 | Loss: 0.00056 | Test Loss: 0.00055 | Valid Loss: 0.00056\n",
      "Early stopping at epoch: 4986\n",
      "=====================\n",
      "Model num: 12\n",
      "Epoch: 0 | Loss: 2.20752 | Test Loss: 2.02747 | Valid Loss: 2.03912\n",
      "Epoch: 200 | Loss: 0.00740 | Test Loss: 0.00728 | Valid Loss: 0.00754\n",
      "Epoch: 400 | Loss: 0.00675 | Test Loss: 0.00665 | Valid Loss: 0.00689\n",
      "Epoch: 600 | Loss: 0.00605 | Test Loss: 0.00595 | Valid Loss: 0.00617\n",
      "Epoch: 800 | Loss: 0.00535 | Test Loss: 0.00526 | Valid Loss: 0.00546\n",
      "Epoch: 1000 | Loss: 0.00467 | Test Loss: 0.00459 | Valid Loss: 0.00477\n",
      "Epoch: 1200 | Loss: 0.00400 | Test Loss: 0.00393 | Valid Loss: 0.00410\n",
      "Epoch: 1400 | Loss: 0.00337 | Test Loss: 0.00331 | Valid Loss: 0.00345\n",
      "Epoch: 1600 | Loss: 0.00277 | Test Loss: 0.00272 | Valid Loss: 0.00285\n",
      "Epoch: 1800 | Loss: 0.00224 | Test Loss: 0.00219 | Valid Loss: 0.00230\n",
      "Epoch: 2000 | Loss: 0.00178 | Test Loss: 0.00174 | Valid Loss: 0.00184\n",
      "Epoch: 2200 | Loss: 0.00142 | Test Loss: 0.00138 | Valid Loss: 0.00147\n",
      "Epoch: 2400 | Loss: 0.00115 | Test Loss: 0.00112 | Valid Loss: 0.00120\n",
      "Epoch: 2600 | Loss: 0.00097 | Test Loss: 0.00094 | Valid Loss: 0.00101\n",
      "Epoch: 2800 | Loss: 0.00086 | Test Loss: 0.00083 | Valid Loss: 0.00089\n",
      "Epoch: 3000 | Loss: 0.00078 | Test Loss: 0.00075 | Valid Loss: 0.00081\n",
      "Epoch: 3200 | Loss: 0.00074 | Test Loss: 0.00071 | Valid Loss: 0.00076\n",
      "Epoch: 3400 | Loss: 0.00071 | Test Loss: 0.00068 | Valid Loss: 0.00073\n",
      "Epoch: 3600 | Loss: 0.00069 | Test Loss: 0.00066 | Valid Loss: 0.00071\n",
      "Epoch: 3800 | Loss: 0.00067 | Test Loss: 0.00064 | Valid Loss: 0.00069\n",
      "Epoch: 4000 | Loss: 0.00066 | Test Loss: 0.00063 | Valid Loss: 0.00068\n",
      "Epoch: 4200 | Loss: 0.00065 | Test Loss: 0.00062 | Valid Loss: 0.00067\n",
      "Epoch: 4400 | Loss: 0.00064 | Test Loss: 0.00061 | Valid Loss: 0.00066\n",
      "Epoch: 4600 | Loss: 0.00064 | Test Loss: 0.00061 | Valid Loss: 0.00065\n",
      "Epoch: 4800 | Loss: 0.00063 | Test Loss: 0.00060 | Valid Loss: 0.00064\n",
      "Epoch: 5000 | Loss: 0.00062 | Test Loss: 0.00059 | Valid Loss: 0.00064\n",
      "Epoch: 5200 | Loss: 0.00061 | Test Loss: 0.00058 | Valid Loss: 0.00063\n",
      "Epoch: 5400 | Loss: 0.00061 | Test Loss: 0.00058 | Valid Loss: 0.00062\n",
      "Epoch: 5600 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 5800 | Loss: 0.00060 | Test Loss: 0.00057 | Valid Loss: 0.00061\n",
      "Epoch: 6000 | Loss: 0.00059 | Test Loss: 0.00057 | Valid Loss: 0.00060\n",
      "Epoch: 6200 | Loss: 0.00059 | Test Loss: 0.00056 | Valid Loss: 0.00060\n",
      "Epoch: 6400 | Loss: 0.00059 | Test Loss: 0.00056 | Valid Loss: 0.00060\n",
      "Epoch: 6600 | Loss: 0.00059 | Test Loss: 0.00056 | Valid Loss: 0.00060\n",
      "Epoch: 6800 | Loss: 0.00059 | Test Loss: 0.00056 | Valid Loss: 0.00059\n",
      "=====================\n",
      "Model num: 13\n",
      "Epoch: 0 | Loss: 2.71046 | Test Loss: 2.50423 | Valid Loss: 2.49859\n",
      "Epoch: 200 | Loss: 0.00683 | Test Loss: 0.00675 | Valid Loss: 0.00701\n",
      "Epoch: 400 | Loss: 0.00608 | Test Loss: 0.00602 | Valid Loss: 0.00624\n",
      "Epoch: 600 | Loss: 0.00532 | Test Loss: 0.00526 | Valid Loss: 0.00545\n",
      "Epoch: 800 | Loss: 0.00461 | Test Loss: 0.00456 | Valid Loss: 0.00472\n",
      "Epoch: 1000 | Loss: 0.00396 | Test Loss: 0.00392 | Valid Loss: 0.00406\n",
      "Epoch: 1200 | Loss: 0.00338 | Test Loss: 0.00334 | Valid Loss: 0.00345\n",
      "Epoch: 1400 | Loss: 0.00285 | Test Loss: 0.00282 | Valid Loss: 0.00291\n",
      "Epoch: 1600 | Loss: 0.00238 | Test Loss: 0.00235 | Valid Loss: 0.00242\n",
      "Epoch: 1800 | Loss: 0.00196 | Test Loss: 0.00194 | Valid Loss: 0.00199\n",
      "Epoch: 2000 | Loss: 0.00162 | Test Loss: 0.00160 | Valid Loss: 0.00164\n",
      "Epoch: 2200 | Loss: 0.00134 | Test Loss: 0.00132 | Valid Loss: 0.00135\n",
      "Epoch: 2400 | Loss: 0.00113 | Test Loss: 0.00112 | Valid Loss: 0.00114\n",
      "Epoch: 2600 | Loss: 0.00099 | Test Loss: 0.00097 | Valid Loss: 0.00099\n",
      "Epoch: 2800 | Loss: 0.00090 | Test Loss: 0.00088 | Valid Loss: 0.00089\n",
      "Epoch: 3000 | Loss: 0.00084 | Test Loss: 0.00082 | Valid Loss: 0.00084\n",
      "Epoch: 3200 | Loss: 0.00081 | Test Loss: 0.00079 | Valid Loss: 0.00080\n",
      "Epoch: 3400 | Loss: 0.00079 | Test Loss: 0.00077 | Valid Loss: 0.00078\n",
      "Epoch: 3600 | Loss: 0.00077 | Test Loss: 0.00075 | Valid Loss: 0.00077\n",
      "Epoch: 3800 | Loss: 0.00076 | Test Loss: 0.00074 | Valid Loss: 0.00076\n",
      "Epoch: 4000 | Loss: 0.00075 | Test Loss: 0.00073 | Valid Loss: 0.00075\n",
      "Epoch: 4200 | Loss: 0.00075 | Test Loss: 0.00073 | Valid Loss: 0.00074\n",
      "Epoch: 4400 | Loss: 0.00074 | Test Loss: 0.00072 | Valid Loss: 0.00074\n",
      "Epoch: 4600 | Loss: 0.00074 | Test Loss: 0.00072 | Valid Loss: 0.00073\n",
      "Epoch: 4800 | Loss: 0.00073 | Test Loss: 0.00071 | Valid Loss: 0.00073\n",
      "Epoch: 5000 | Loss: 0.00073 | Test Loss: 0.00071 | Valid Loss: 0.00072\n",
      "Epoch: 5200 | Loss: 0.00072 | Test Loss: 0.00071 | Valid Loss: 0.00072\n",
      "Epoch: 5400 | Loss: 0.00072 | Test Loss: 0.00070 | Valid Loss: 0.00071\n",
      "Epoch: 5600 | Loss: 0.00072 | Test Loss: 0.00070 | Valid Loss: 0.00071\n",
      "Epoch: 5800 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00071\n",
      "Epoch: 6000 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 6200 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 6400 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 6600 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "Epoch: 6800 | Loss: 0.00071 | Test Loss: 0.00070 | Valid Loss: 0.00070\n",
      "=====================\n",
      "Model num: 14\n",
      "Epoch: 0 | Loss: 1.33692 | Test Loss: 1.21716 | Valid Loss: 1.20592\n",
      "Epoch: 200 | Loss: 0.00751 | Test Loss: 0.00717 | Valid Loss: 0.00735\n",
      "Epoch: 400 | Loss: 0.00621 | Test Loss: 0.00593 | Valid Loss: 0.00608\n",
      "Epoch: 600 | Loss: 0.00500 | Test Loss: 0.00478 | Valid Loss: 0.00489\n",
      "Epoch: 800 | Loss: 0.00397 | Test Loss: 0.00380 | Valid Loss: 0.00388\n",
      "Epoch: 1000 | Loss: 0.00310 | Test Loss: 0.00297 | Valid Loss: 0.00302\n",
      "Epoch: 1200 | Loss: 0.00238 | Test Loss: 0.00228 | Valid Loss: 0.00232\n",
      "Epoch: 1400 | Loss: 0.00181 | Test Loss: 0.00175 | Valid Loss: 0.00176\n",
      "Epoch: 1600 | Loss: 0.00140 | Test Loss: 0.00135 | Valid Loss: 0.00136\n",
      "Epoch: 1800 | Loss: 0.00112 | Test Loss: 0.00110 | Valid Loss: 0.00109\n",
      "Epoch: 2000 | Loss: 0.00097 | Test Loss: 0.00095 | Valid Loss: 0.00094\n",
      "Epoch: 2200 | Loss: 0.00089 | Test Loss: 0.00088 | Valid Loss: 0.00087\n",
      "Epoch: 2400 | Loss: 0.00086 | Test Loss: 0.00085 | Valid Loss: 0.00083\n",
      "Epoch: 2600 | Loss: 0.00085 | Test Loss: 0.00084 | Valid Loss: 0.00082\n",
      "Epoch: 2800 | Loss: 0.00084 | Test Loss: 0.00083 | Valid Loss: 0.00082\n",
      "Epoch: 3000 | Loss: 0.00084 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3200 | Loss: 0.00084 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3400 | Loss: 0.00084 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3600 | Loss: 0.00083 | Test Loss: 0.00083 | Valid Loss: 0.00081\n",
      "Epoch: 3800 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 4000 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00081\n",
      "Epoch: 4200 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 4400 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 4600 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 4800 | Loss: 0.00083 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 5000 | Loss: 0.00082 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 5200 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00080\n",
      "Epoch: 5400 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 5600 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 5800 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 6000 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 6200 | Loss: 0.00082 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 6400 | Loss: 0.00081 | Test Loss: 0.00081 | Valid Loss: 0.00079\n",
      "Epoch: 6600 | Loss: 0.00081 | Test Loss: 0.00082 | Valid Loss: 0.00080\n",
      "Epoch: 6800 | Loss: 0.00081 | Test Loss: 0.00080 | Valid Loss: 0.00079\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "epochs = 7000\n",
    "learning_rate = 0.002\n",
    "momentum = 1.3\n",
    "patience = 500\n",
    "min_delta = 0\n",
    "prev_valid_loss = torch.tensor([0]).to(device)\n",
    "\n",
    "epochs_list=[]\n",
    "train_losses=[]\n",
    "test_losses=[]\n",
    "valid_losses=[]\n",
    "rel_errors = []\n",
    "\n",
    "for num, model in enumerate(G_M_MT_stacking_models, 0):\n",
    "    \n",
    "    print(\"Model num:\", num)\n",
    "    \n",
    "    #optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "    #                            lr=learning_rate,\n",
    "    #                            momentum=momentum)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                                 lr=learning_rate)\n",
    "    \n",
    "    early_stopping = EarlyStopping_V1(patience=patience, min_delta=min_delta)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        model.train()\n",
    "    \n",
    "        y_preds = model(X_G_M_MT_stack_train[:, num, :]).squeeze() # беру результаты сетей для num столбца\n",
    "                                                                # по смыслу - результаты G M на num координате слоя\n",
    "        train_loss = loss_fn(y_preds, y_train[:, num]) # вырезаю num-тый столбец\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        train_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.inference_mode():\n",
    "            \n",
    "            valid_preds = model(X_G_M_MT_stack_valid[:, num, :]).squeeze()\n",
    "            \n",
    "            valid_loss = loss_fn(valid_preds, y_valid[:, num])\n",
    "            \n",
    "            early_stopping(prev_loss=prev_valid_loss,\n",
    "                           next_loss=valid_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping at epoch:\", epoch)\n",
    "                break\n",
    "            prev_valid_loss = valid_loss\n",
    "            \n",
    "        \n",
    "            test_preds = model(X_G_M_MT_stack_test[:, num, :]).squeeze()\n",
    "        \n",
    "            test_loss = loss_fn(test_preds, y_test[:, num])\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            epochs_list.append(epoch)\n",
    "            train_losses.append(train_loss.cpu().item())\n",
    "            test_losses.append(test_loss.cpu().item())\n",
    "            valid_losses.append(valid_loss.cpu().item())\n",
    "            print(f\"Epoch: {epoch} | Loss: {train_loss.item():.5f} | Test Loss: {test_loss.item():.5f} | Valid Loss: {(valid_loss.item()):.5f}\")\n",
    "            \n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2e5f951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related error: 5.4205 %\n"
     ]
    }
   ],
   "source": [
    "mse = []\n",
    "rmse = []\n",
    "ranges = []\n",
    "\n",
    "for num, model in enumerate(G_M_MT_stacking_models, 0):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        \n",
    "        y_preds = model(X_G_M_MT_stack_test[:, num, :]).squeeze()\n",
    "    \n",
    "        error = loss_fn(y_preds, y_test[:, num])\n",
    "    \n",
    "        mse.append(error.item())\n",
    "        rmse.append(torch.sqrt(error).item())\n",
    "        ranges.append(torch.max(y_test[:,num]).item() - torch.min(y_test[:,num]).item())\n",
    "    \n",
    "rel_errors = [i / j for i, j in zip(rmse, ranges)]\n",
    "\n",
    "G_M_MT_stack_rel_error = statistics.mean(rel_errors)\n",
    "    \n",
    "print(f\"Related error: {(G_M_MT_stack_rel_error*100):.4f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8583f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84668ab8",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a0714a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-ae71017126be>:21: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  ax = plt.axes()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-118-ae71017126be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m         zorder = 2)\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautofmt_xdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xs' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 512x384 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# vim: set ai et ts=4 sw=4:\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "data_names = ['G', 'M', 'MT', 'G+M', 'G+MT', 'M+MT', 'G_M_MT']\n",
    "data_values = [G_rel_error*100, M_rel_error*100, MT_rel_error*100, \n",
    "               G_M_rel_error*100, G_MT_rel_error*100, M_MT_rel_error*100, G_M_MT_rel_error*100,\n",
    "               G_M_stack_rel_error*100, G_MT_stack_rel_error*100, M_MT_stack_rel_error*100, G_M_MT_stack_rel_error*100]\n",
    "\n",
    "dpi = 80\n",
    "fig = plt.figure(dpi = dpi, figsize = (512 / dpi, 384 / dpi) )\n",
    "mpl.rcParams.update({'font.size': 10})\n",
    "\n",
    "plt.title('Relation error, % (Layer 1)')\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.yaxis.grid(True, zorder = 1)\n",
    "\n",
    "#xs_1 = range(3)\n",
    "xs_1_G = range(1)\n",
    "xs_1 = range(2)\n",
    "xs_2 = range(4)\n",
    "xs_3 = range(4)\n",
    "\n",
    "\n",
    "plt.bar([x + 0.01 for x in xs_1_G], [ d * 0.9 for d in data_values[:1]],\n",
    "        width = 0.2, color = 'dimgray', alpha = 0.7, label = 'Initial solution',\n",
    "        yerr = MT_st_dev * 100 ,zorder = 2)\n",
    "plt.bar([x + 1.0 for x in xs_1], [ d * 0.9 for d in data_values[1:3]],\n",
    "        width = 0.2, color = 'dimgray', alpha = 0.7, label = 'Initial solution',\n",
    "        zorder = 2)\n",
    "plt.bar([x + 2.85 for x in xs_2], data_values[3:7],\n",
    "        width = 0.2, color = 'darkred', alpha = 0.7, label = 'Data integration',\n",
    "        zorder = 2)\n",
    "plt.bar([x + 3.15 for x in xs_3], data_values[7:11],\n",
    "        width = 0.2, color = 'darkblue', alpha = 0.7, label = 'Stacking',\n",
    "        zorder = 2)\n",
    "\n",
    "plt.xticks(xs, data_names)\n",
    "\n",
    "fig.autofmt_xdate(rotation = 25)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "fig.savefig('bars.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566e234",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
